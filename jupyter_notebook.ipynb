{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = 'C:\\\\Users\\\\MdeCL\\\\Google Drive\\\\Work, productivity and interests_\\\\computer science\\\\coding skills (technical)\\\\VS Code project files\\\\NLP-powered Vocab Learning Strategy'\n",
    "\n",
    "support_files_dir = 'C:\\\\Users\\\\MdeCL\\\\Google Drive\\\\Work, productivity and interests \\\\computer science (technical)\\\\coding skills (technical)\\\\VS Code project files\\\\NLP-powered Vocab Learning Strategy\\\\General support files'\n",
    "\n",
    "desktop_dir = 'C:\\\\Users\\\\MdeCL\\\\Desktop\\\\Vocab Project Desktop Files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define source and target languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'ru'\n",
    "target_lang = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MdeCL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MdeCL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MdeCL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import math\n",
    "\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "eng_stop_words = set(stopwords.words('english'))\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from pyinflect import getAllInflections, getInflection\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "ru_stemmer = SnowballStemmer(\"russian\") \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "english_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from inspect import currentframe, getframeinfo\n",
    "from pandas.core.common import SettingWithCopyError\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import genanki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Russian National Corpus frequency list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poss_in_full = {\n",
    "    '(v)' : 'verb',\n",
    "    '(pr)' : 'preposition',\n",
    "    '(conj)' : 'conjunction',\n",
    "    '(apro)' : 'pronoun',\n",
    "    '(spro)' : 'pronoun',\n",
    "    '(s)' : 'noun',\n",
    "    '(part)' : 'particle',\n",
    "    '(a)' : 'adjective',\n",
    "    '(adv)' : 'adverb',\n",
    "    '(advpro)' : 'adverb',\n",
    "    '(anum)' : 'numeral',\n",
    "    '(num)' : 'numeral',\n",
    "    '(intj)' : 'interjection',\n",
    "    '(v)f' : 'verb'\n",
    "}\n",
    "\n",
    "freq_ranks = []\n",
    "words = []\n",
    "pos_tags = []\n",
    "\n",
    "with open(\"C:\\\\Users\\\\MdeCL\\\\Desktop\\\\Vocab-Project\\\\supporting-files\\\\ru_nat_corpus_freq_list.txt\", 'r', encoding='utf8') as file:\n",
    "    file_read = file.read()\n",
    "    lines = file_read.split('\\n')\n",
    "    idx = 1\n",
    "    for line in lines:\n",
    "        freq_rank = idx\n",
    "        freq_ranks.append(freq_rank)\n",
    "        bare_word = line.split(' ')[0]\n",
    "        words.append(bare_word)\n",
    "        pos_tag = line.split(' ')[1]\n",
    "        pos_tags.append(poss_in_full[pos_tag])\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "est_freqs_df = pd.read_csv(desktop_dir + '\\\\freqs.csv')\n",
    "freqs_list = est_freqs_df.head(len(freq_ranks))['Frequency'].to_list()\n",
    "freqs_list = [round(item) for item in freqs_list]\n",
    "        \n",
    "rnc_freq_list = pd.DataFrame({'Frequency rank' : freq_ranks, 'Estimated frequency' : freqs_list, 'Word' : words, 'PoS tag' : pos_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency rank</th>\n",
       "      <th>Estimated frequency</th>\n",
       "      <th>Word</th>\n",
       "      <th>PoS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>4001</td>\n",
       "      <td>2854</td>\n",
       "      <td>сбросить</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>4002</td>\n",
       "      <td>2853</td>\n",
       "      <td>уснуть</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>4003</td>\n",
       "      <td>2851</td>\n",
       "      <td>близость</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>4004</td>\n",
       "      <td>2850</td>\n",
       "      <td>возрождение</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>4005</td>\n",
       "      <td>2849</td>\n",
       "      <td>заключенный</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>4006</td>\n",
       "      <td>2849</td>\n",
       "      <td>кружка</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>4007</td>\n",
       "      <td>2847</td>\n",
       "      <td>мяч</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>4008</td>\n",
       "      <td>2847</td>\n",
       "      <td>неведомый</td>\n",
       "      <td>adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>4009</td>\n",
       "      <td>2846</td>\n",
       "      <td>референдум</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>4010</td>\n",
       "      <td>2845</td>\n",
       "      <td>сниться</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Frequency rank  Estimated frequency         Word    PoS tag\n",
       "4000            4001                 2854     сбросить       verb\n",
       "4001            4002                 2853       уснуть       verb\n",
       "4002            4003                 2851     близость       noun\n",
       "4003            4004                 2850  возрождение       noun\n",
       "4004            4005                 2849  заключенный       noun\n",
       "4005            4006                 2849       кружка       noun\n",
       "4006            4007                 2847          мяч       noun\n",
       "4007            4008                 2847    неведомый  adjective\n",
       "4008            4009                 2846   референдум       noun\n",
       "4009            4010                 2845      сниться       verb"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnc_freq_list.iloc[4000:4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = rnc_freq_list['Estimated frequency'].sum()\n",
    "\n",
    "grouped_df = pd.DataFrame()\n",
    "idx = 0\n",
    "while idx <= 10000:\n",
    "    sect_df = rnc_freq_list.iloc[idx:idx + 100]\n",
    "    sect_est_freq = sect_df['Estimated frequency'].sum()\n",
    "    grouped_df = grouped_df.append(\n",
    "                                {'Frequency ranks' : str(idx) + '-' + str(idx + 100), \n",
    "                                   'Estimated frequency' : sect_est_freq,\n",
    "                                   'Percentage of corpus' : sect_est_freq / corpus_size * 100},\n",
    "                                  ignore_index = True\n",
    "                                  )\n",
    "    idx += 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Frequency ranks', ylabel='Estimated frequency'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAG3CAYAAAA95HVlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABSD0lEQVR4nO3dd7gkZZX48e+BAUWJyghKcNwV80oQwbD+xAwoGRUTggEFFXNad81Z14AIyIqoa0CCCCoI5oyEAQmCiqKCIqAiGFfC+f1RNdLTU3W76t5bfatvfz/PU890V58+fd5+u++dOVP1VmQmkiRJkiRJUpXVFroASZIkSZIk9ZfNI0mSJEmSJNWyeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUq2JbB5FxEci4uqIuLBh/BMi4kcRcVFEfKrr+iRJkiRJkhaLyMyFrqG1iPh/wJ+Bj2fmfUbEbgEcCzw8M6+NiDtk5tXjqFOSJEmSJGnSTeSRR5n5LeAPg/si4l8j4ksRcU5EfDsi7lE+9Gzgg5l5bflcG0eSJEmSJEkNTWTzqMaRwAsy837Ay4DDyv13A+4WEd+NiDMiYscFq1CSJEmSJGnCLFnoAuZDRKwNPAg4LiJW7L5V+ecSYAtgB2BT4NsRcZ/M/OOYy5QkSZIkSZo4i6J5RHEE1R8zc6uKx64AzsjMG4DLIuLHFM2ks8ZYnyRJkiRJ0kRaFKetZeb1FI2hxwNEYcvy4c8BDyv3b0hxGtvPF6JOSZIkSZKkSTORzaOI+DTwfeDuEXFFRDwTeArwzIj4IXARsFsZfhrw+4j4EfB14OWZ+fuFqFuSJEmSJGnSRGYudA2SJEmSJEnqqYk88kiSJEmSJEnjMXELZm+44Ya5bNmyhS5DkiRJkiRp0TjnnHN+l5lLqx6buObRsmXLOPvssxe6DEmSJEmSpEUjIn5Z95inrUmSJEmSJKmWzSNJkiRJkiTVsnkkSZIkSZKkWjaPJEmSJEmSVMvmkSRJkiRJkmrZPJIkSZIkSVItm0eSJEmSJEmqZfNIkiRJkiRJtWweSZIkSZIkqZbNI0mSJEmSJNWyeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUq0lC13AbFxz+CcaxS098KkdVyJJkiRJkrS4eeSRJEmSJEmSatk8kiRJkiRJUi2bR5IkSZIkSapl80iSJEmSJEm1bB5JkiRJkiSpls0jSZIkSZIk1bJ5JEmSJEmSpFo2jyRJkiRJklTL5pEkSZIkSZJq2TySJEmSJElSLZtHkiRJkiRJqmXzSJIkSZIkSbVsHkmSJEmSJKlWZ82jiLh1RJwZET+MiIsi4g0VMRERh0TEpRFxfkRs01U9kiRJkiRJam9Jh7n/D3h4Zv45ItYAvhMRp2bmGQMxOwFblNv2wOHln5IkSZIkSeqBzo48ysKfy7trlFsOhe0GfLyMPQNYPyLu2FVNkiRJkiRJaqfTNY8iYvWIOA+4GvhyZv5gKGQT4PKB+1eU+4bzHBARZ0fE2ddcc01n9UqSJEmSJGllnTaPMvOmzNwK2BTYLiLuMxQSVU+ryHNkZm6bmdsuXbq0g0olSZIkSZJUZSxXW8vMPwLfAHYceugKYLOB+5sCvxlHTZIkSZIkSRqty6utLY2I9cvbawGPBC4ZCjsZ2Le86toDgOsy88quapIkSZIkSVI7XV5t7Y7AxyJidYom1bGZ+YWIeC5AZh4BnALsDFwK/BXYv8N6JEmSJEmS1FJnzaPMPB/YumL/EQO3E3heVzVIkiRJkiRpbsay5pEkSZIkSZImk80jSZIkSZIk1bJ5JEmSJEmSpFo2jyRJkiRJklTL5pEkSZIkSZJq2TySJEmSJElSLZtHkiRJkiRJqmXzSJIkSZIkSbVsHkmSJEmSJKmWzSNJkiRJkiTVsnkkSZIkSZKkWjaPJEmSJEmSVMvmkSRJkiRJkmrZPJIkSZIkSVItm0eSJEmSJEmqZfNIkiRJkiRJtWweSZIkSZIkqZbNI0mSJEmSJNWyeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSai1Z6ALG4ZrDjx4Zs/TA/cdQiSRJkiRJ0mTxyCNJkiRJkiTVsnkkSZIkSZKkWjaPJEmSJEmSVMvmkSRJkiRJkmrZPJIkSZIkSVItm0eSJEmSJEmqZfNIkiRJkiRJtWweSZIkSZIkqZbNI0mSJEmSJNWyeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUi2bR5IkSZIkSapl80iSJEmSJEm1bB5JkiRJkiSpls0jSZIkSZIk1bJ5JEmSJEmSpFqdNY8iYrOI+HpEXBwRF0XECytidoiI6yLivHJ7bVf1SJIkSZIkqb0lHea+EXhpZi6PiHWAcyLiy5n5o6G4b2fm4zqsQ5IkSZIkSbPU2ZFHmXllZi4vb/8JuBjYpKvXkyRJkiRJ0vwby5pHEbEM2Br4QcXDD4yIH0bEqRFx75rnHxARZ0fE2ddcc02XpUqSJEmSJGlA582jiFgbOAF4UWZeP/TwcuDOmbkl8AHgc1U5MvPIzNw2M7ddunRpp/VKkiRJkiTpFp02jyJiDYrG0Scz87PDj2fm9Zn55/L2KcAaEbFhlzVJkiRJkiSpuS6vthbAUcDFmfmempiNyzgiYruynt93VZMkSZIkSZLa6fJqaw8GngZcEBHnlfv+A9gcIDOPAPYGDoyIG4G/AftkZnZYkyRJkiRJklrorHmUmd8BYkTMocChXdUgSZIkSZKkuRnL1dYkSZIkSZI0mWweSZIkSZIkqZbNI0mSJEmSJNWyeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUi2bR5IkSZIkSapl80iSJEmSJEm1bB5JkiRJkiSp1pKFLqCPrjniyJExS597wBgqkSRJkiRJWlgeeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUi2bR5IkSZIkSapl80iSJEmSJEm1bB5JkiRJkiSpls0jSZIkSZIk1bJ5JEmSJEmSpFo2jyRJkiRJklTL5pEkSZIkSZJq2TySJEmSJElSLZtHkiRJkiRJqmXzSJIkSZIkSbVGNo8i4nERYZNJkiRJkiRpCjVpCu0D/DQi3hkR9+y6IEmSJEmSJPXHyOZRZj4V2Br4GXB0RHw/Ig6IiHU6r06SJEmSJEkLqtHpaJl5PXACcAxwR2APYHlEvKDD2iRJkiRJkrTAmqx5tEtEnAh8DVgD2C4zdwK2BF7WcX2SJEmSJElaQEsaxDweeG9mfmtwZ2b+NSKe0U1ZkiRJkiRJ6oMmzaPXAVeuuBMRawEbZeYvMvOrnVUmSZIkSZKkBddkzaPjgJsH7t9U7pMkSZIkSdIi16R5tCQz/7HiTnl7ze5KkiRJkiRJUl80aR5dExG7rrgTEbsBv+uuJEmSJEmSJPVFkzWPngt8MiIOBQK4HNi306okSZIkSZLUCyObR5n5M+ABEbE2EJn5p+7LkiRJkiRJUh+MbB5FxK2AvYBlwJKIACAz39hpZZIkSZIkSVpwTU5bOwm4DjgH+L9uy5k8Vx/xgUZxd3juCzquRJIkSZIkaf41aR5tmpk7dl6JJEmSJEmSeqfJ1da+FxH/1jZxRGwWEV+PiIsj4qKIeGFFTETEIRFxaUScHxHbtH0dSZIkSZIkdafJkUf/DuwXEZdRnLYWQGbmfUc870bgpZm5PCLWAc6JiC9n5o8GYnYCtii37YHDyz8lSZIkSZLUA02aRzvNJnFmXglcWd7+U0RcDGwCDDaPdgM+npkJnBER60fEHcvnSpIkSZIkaYGNPG0tM38JbAY8vLz91ybPGxQRy4CtgR8MPbQJcPnA/SvKfcPPPyAizo6Is6+55po2Ly1JkiRJkqQ5GNkEiojXAa8EXl3uWgP4RNMXiIi1gROAF2Xm9cMPVzwlV9mReWRmbpuZ2y5durTpS0uSJEmSJGmOmhxBtAewK/AXgMz8DbBOk+QRsQZF4+iTmfnZipArKI5qWmFT4DdNckuSJEmSJKl7TZpH/yjXJEqAiLhtk8QREcBRwMWZ+Z6asJOBfcurrj0AuM71jiRJkiRJkvqjyYLZx0bEh4D1I+LZwDOA/2nwvAcDTwMuiIjzyn3/AWwOkJlHAKcAOwOXUqyltH+r6iVJkiRJktSpkc2jzHx3RDwKuB64O/DazPxyg+d9h+o1jQZjEnhew1olSZIkSZI0Zk2OPKJsFo1sGEmSJEmSJGlxGdk8iog/ccsV0NakuNraXzJz3S4LkyRJkiRJ0sJrctraSldWi4jdge26KkiSJEmSJEn90eRqayvJzM8BD5//UiRJkiRJktQ3TU5b23Pg7mrAttxyGpskSZIkSZIWsSYLZu8ycPtG4BfAbp1UI0mSJEmSpF5psubR/uMoRJIkSZIkSf3T5LS1Q2Z6PDMPnr9yJEmSJEmS1CdNFsy+NbAN8NNy2wq4CTin3CRJkiRJkrRINVnzaAvgYZl5A0BEHAGcnpkv7rQySZIkSZIkLbgmzaM7AesAfyjvr13u0yxcdfi7RsZsdODLx1CJJEmSJEnSaE2aR28Hzo2Ir5f3Hwq8vrOKJEmSJEmS1BtNrrZ2dEScCmxf7npVZv6227IkSZIkSZLUByMXzI6IAB4JbJmZJwFrRsR2nVcmSZIkSZKkBdfkamuHAQ8EnlTe/xPwwc4qkiRJkiRJUm80WfNo+8zcJiLOBcjMayNizY7rkiRJkiRJUg80OfLohohYHUiAiFgK3NxpVZIkSZIkSeqFJs2jQ4ATgTtExFuA7wBv7bQqSZIkSZIk9cKMp61FxGrAZcArgEcAAeyemRePoTZJkiRJkiQtsBmbR5l5c0T8d2Y+ELhkTDVJkiRJkiSpJ5qctnZ6ROwVEdF5NZIkSZIkSeqVJldbewlwW+DGiPg7xalrmZnrdlqZJEmSJEmSFlztkUcR8eDy5tLMXC0z18zMdTNzHRtHkiRJkiRJ02Gm09YOKf/83jgKkSRJkiRJUv/MdNraDRFxNLBpRBwy/GBmHtxdWZIkSZIkSeqDmZpHjwMeCTwcOGc85UiSJEmSJKlPaptHmfk74JiIuDgzfzjGmiRJkiRJktQTM615BICNI0mSJEmSpOk1snkkSZIkSZKk6TXTmkdaYL89/I2N4jY+8LUdVyJJkiRJkqZVbfMoIl4y0xMz8z3zX44kSZIkSZL6ZKYjj9Yp/7w7cH/g5PL+LsC3uixKkiRJkiRJ/TDT1dbeABARpwPbZOafyvuvB44bS3WSJEmSJElaUE0WzN4c+MfA/X8AyzqpRpIkSZIkSb3SZMHs/wXOjIgTgQT2AD7eaVWSJEmSJEnqhZHNo8x8S0ScCjyk3LV/Zp7bbVmSJEmSJEnqgyanrQHcBrg+M98PXBERd+mwJkmSJEmSJPXEyOZRRLwOeCXw6nLXGsAnuixKkiRJkiRJ/dDkyKM9gF2BvwBk5m+AdbosSpIkSZIkSf3QpHn0j8xMisWyiYjbdluSJEmSJEmS+qJJ8+jYiPgQsH5EPBv4CvDhUU+KiI9ExNURcWHN4ztExHURcV65vbZd6ZIkSZIkSepak6utvTsiHgVcD9wdeG1mfrlB7o8ChwIfnyHm25n5uCaFSpIkSZIkafxGNo8i4h2Z+UrgyxX7amXmtyJi2dxLlCRJkiRJ0kJpctraoyr27TRPr//AiPhhRJwaEfeuC4qIAyLi7Ig4+5prrpmnl5YkSZIkSdIotc2jiDgwIi4A7h4R5w9slwHnz8NrLwfunJlbAh8APlcXmJlHZua2mbnt0qVL5+GlJUmSJEmS1MRMp619CjgVeBvwqoH9f8rMP8z1hTPz+oHbp0TEYRGxYWb+bq65JUmSJEmSND9qm0eZeR1wHfAkgIi4A3BrYO2IWDszfzWXF46IjYGrMjMjYjuKo6B+P5ec0+7Kw2ZchgqAOx70jjFUIkmSJEmSFosmC2bvArwHuBNwNXBn4GKgdo2i8nmfBnYANoyIK4DXAWsAZOYRwN7AgRFxI/A3YJ/MzFmPRJIkSZIkSfNuZPMIeDPwAOArmbl1RDyM8mikmWTmjDGZeShwaKMqJUmSJEmStCCaXG3thsz8PbBaRKyWmV8Htuq2LEmSJEmSJPVBkyOP/hgRawPfAj4ZEVcDN3ZbliRJkiRJkvqgyZFHu1GsSfRi4EvAz4BduixKkiRJkiRJ/TDyyKPM/AtARKwLfL7ziiRJkiRJktQbTa629hzgjRRHH90MBJDAv3RbmiRJkiRJkhZakzWPXgbcOzN/13UxkiRJkiRJ6pcmax79DPhr14VIkiRJkiSpf5ocefRq4HsR8QPg/1bszMyDO6tKkiRJkiRJvdCkefQh4GvABRRrHkmSJEmSJGlKNGke3ZiZL+m8Eo3drw997siYTZ5/xBgqkSRJkiRJfdVkzaOvR8QBEXHHiLjdiq3zyiRJkiRJkrTgmhx59OTyz1cP7EvgX+a/HEmSJEmSJPXJyOZRZt5lHIVIkiRJkiSpf2qbRxHx8Mz8WkTsWfV4Zn62u7IkSZIkSZLUBzMdefRQiqus7VLxWAI2jyRJkiRJkha52uZRZr6uvPnGzLxs8LGI8FQ2SZIkSZKkKdDkamsnVOw7fr4LkSRJkiRJUv/MtObRPYB7A+sNrXu0LnDrrguTJEmSJEnSwptpzaO7A48D1mfldY/+BDy7w5rUQ786ZO9GcZsf7EFpkiRJkiQtJjOteXQScFJEPDAzvz/GmiRJkiRJktQTTdY82iMi1o2INSLiqxHxu4h4aueVSZIkSZIkacE1aR49OjOvpziF7QrgbsDLO61KkiRJkiRJvdCkebRG+efOwKcz8w8d1iNJkiRJkqQemWnB7BU+HxGXAH8DDoqIpcDfuy1LkiRJkiRJfTDyyKPMfBXwQGDbzLwB+CuwW9eFSZIkSZIkaeHVNo8i4hUDdx+ZmTcBZOZfgIO7LkySJEmSJEkLb6Yjj/YZuP3qocd27KAWSZIkSZIk9cxMax5Fze2q+9JKLj109JmNd33+SWOoRJIkSZIkzcVMRx5lze2q+5IkSZIkSVqEZjryaMuIuJ7iKKO1ytuU92/deWWSJEmSJElacLXNo8xcfZyFSJIkSZIkqX9mOm1NkiRJkiRJU87mkSRJkiRJkmrZPJIkSZIkSVItm0eSJEmSJEmqZfNIkiRJkiRJtWqvtiaN048O23VkzL0OOnkMlUiSJEmSpEEeeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSanXWPIqIj0TE1RFxYc3jERGHRMSlEXF+RGzTVS2SJEmSJEmanS6PPPoosOMMj+8EbFFuBwCHd1iLJEmSJEmSZqGz5lFmfgv4wwwhuwEfz8IZwPoRcceu6pEkSZIkSVJ7SxbwtTcBLh+4f0W578rhwIg4gOLoJDbffPOxFKf+Ou/wXRvFbXXgyR1XIkmSJEnS4reQC2ZHxb6sCszMIzNz28zcdunSpR2XJUmSJEmSpBUWsnl0BbDZwP1Ngd8sUC2SJEmSJEmqsJDNo5OBfcurrj0AuC4zVzllTZIkSZIkSQunszWPIuLTwA7AhhFxBfA6YA2AzDwCOAXYGbgU+Cuwf1e1SJIkSZIkaXY6ax5l5pNGPJ7A87p6fUmSJEmSJM3dQp62JkmSJEmSpJ7r7MgjqS/O+tAuI2Pu/5zPj6ESSZIkSZImj0ceSZIkSZIkqZbNI0mSJEmSJNWyeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUq0lC12A1CffP/JxjeIeeMAXOq5EkiRJkqR+8MgjSZIkSZIk1bJ5JEmSJEmSpFo2jyRJkiRJklTL5pEkSZIkSZJq2TySJEmSJElSLa+2Js3Bt/7nsSNj/t+zvziGSiRJkiRJ6oZHHkmSJEmSJKmWzSNJkiRJkiTVsnkkSZIkSZKkWjaPJEmSJEmSVMsFs6Ux+tqHRy+w/fBnucC2JEmSJKk/PPJIkiRJkiRJtWweSZIkSZIkqZbNI0mSJEmSJNVyzSOpp047audGcY955ikdVyJJkiRJmmYeeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSarnmkbRIfPGonUbGPPaZp46hEkmSJEnSYuKRR5IkSZIkSapl80iSJEmSJEm1PG1NmlInfWT0aW67PcPT3CRJkiRp2nnkkSRJkiRJkmrZPJIkSZIkSVItm0eSJEmSJEmq5ZpHkkY64egdG8Xttf+XOq5EkiRJkjRuHnkkSZIkSZKkWjaPJEmSJEmSVMvT1iTNu880OM3tiZ7iJkmSJEkTwSOPJEmSJEmSVMsjjyQtqE9+9DGN4p6y32kdVyJJkiRJqtLpkUcRsWNE/DgiLo2IV1U8vkNEXBcR55Xba7usR5IkSZIkSe10duRRRKwOfBB4FHAFcFZEnJyZPxoK/XZmPq6rOiRJkiRJkjR7XZ62th1waWb+HCAijgF2A4abR5LU2Ec/+uiRMfvtd/oYKpEkSZKk6dDlaWubAJcP3L+i3DfsgRHxw4g4NSLuXZUoIg6IiLMj4uxrrrmmi1olSZIkSZJUocsjj6JiXw7dXw7cOTP/HBE7A58DtljlSZlHAkcCbLvttsM5JKnWUR8ffaTSM/f1SCVJkiRJqtPlkUdXAJsN3N8U+M1gQGZen5l/Lm+fAqwRERt2WJMkSZIkSZJa6PLIo7OALSLiLsCvgX2AJw8GRMTGwFWZmRGxHUUz6/cd1iRJtT70v49pFPecp53WcSWSJEmS1B+dNY8y88aIeD5wGrA68JHMvCginls+fgSwN3BgRNwI/A3YJzM9LU2SJEmSJKknujzyaMWpaKcM7Tti4PahwKFd1iBJkiRJkqTZ67R5JEmL2Qc/Mfo0t+c91VPcJEmSJE22LhfMliRJkiRJ0oTzyCNJGpP3fWr0kUoverJHKkmSJEnqF488kiRJkiRJUi2PPJKkHnrXp0cfpQTw8id5pJIkSZKkbtk8kqRF4C2fGd1ses0TbTRJkiRJas/T1iRJkiRJklTLI48kacq87tgdG8W94Qlf6rgSSZIkSZPA5pEkaUavPH50s+kde9tokiRJkhYrT1uTJEmSJElSLY88kiTNq4NPGH2k0iF7eaSSJEmSNClsHkmSFsz+JzZbf+noPWw2SZIkSQvF09YkSZIkSZJUyyOPJEkTY9eTRh+pdPJuHqUkSZIkzSebR5KkRWunk54yMubU3T45hkokSZKkyWXzSJIkYKeTDmwUd+puh3dciSRJktQvNo8kSZqFnT/30pExp+z+32OoRJIkSeqWzSNJkjq28+de0yjulN3f0nElkiRJUns2jyRJ6pmdT3zDyJhT9njdGCqRJEmSbB5JkjTxdj7xbSNjTtnj1WOoRJIkSYuRzSNJkqbIY098V6O4L+7x8o4rkSRJ0qSweSRJkmo99rPvGxnzxT1f1HkdkiRJWjg2jyRJ0rx57Gc/MDLmi3u+oIw9vFHOL+554JxqkiRJ0tzYPJIkSRPhsSccOTLmi3sdMIZKJEmSpovNI0mStOg89oSjGsV9ca9nAvC4Ez46MvYLe+03h4okSZIml80jSZKklh53/P+OjPnC3k8rYz/ZKOcX9n7KnGqSJEnqis0jSZKkHnnc8ceMjPnC3vsMxB/bIP4Jc6pJkiRNN5tHkiRJU2KX409oFPf5vfcq4z/XIHb3OVQkSZImgc0jSZIkzdmux3++UdzJe+8CwG7Hnzoy9qS9d/rn7d2OP71B/KMb1SBJktqxeSRJkqRFZffjv9oo7nN7PwKAPU745sjYE/d66JxqkiRpktk8kiRJklrY84Tvjoz57F4PBmCvE37QKOcJe20PwN4nLB8Ze/xe2zTKKUnSfLF5JEmSJE2gx59wYaO44/a6DwBPOOEnI2OP3etu/7z9pM/+YmT8p/dcBsDBJ17eqJZD9tisUZwkqV9sHkmSJEkaq/868TcjY960x53+efsdJ145Mv6Ve9wRgA+ceFWjGl6wx0YAfPizV4+Mfdaed2iUU5IWK5tHkiRJktTAJz57TaO4p+65FIDjT/jdyNi999rwn7dPPm50/K6PL+JP/czoWICdnrjh6CBJGsHmkSRJkiQtcl/51OjG1yOfvPSft7/5idHxD31qEf+9jzVrqj3o6UX8WUePPtrr/vsXR3ud9+HRsQBbPauIv+iI0Uee3fu5GzXKKekWNo8kSZIkSVPpJ4eObjbd7flFs+kX7/tto5zLXrQxAL9+1+jTLTd5+R3/efvKd45eO+yOryjWDfvtu3/eqJaNX/YvRfx/XzI69qX3KGLf02w9tY1fcp9GcVocbB5JkiRJkqRZueq9546M2ejFW98S/76zRse/6P5F7Pu/36iGjV74wCL+kG+Pjj34If+8ffUHvj4y/g4veFgZ++VGtdzhBY8q4g89dXTs83cqYj/4+Wa5n7dLo7gu2DySJEmSJEmaEFd/8LMjY+7wvD1viT/s2NHxBz1hxsdXG12WJEmSJEmSplWnzaOI2DEifhwRl0bEqyoej4g4pHz8/IjYpst6JEmSJEmS1E5nzaOIWB34ILATcC/gSRFxr6GwnYAtyu0A4PCu6pEkSZIkSVJ7XR55tB1waWb+PDP/ARwD7DYUsxvw8SycAawfEXccTiRJkiRJkqSFEZnZTeKIvYEdM/NZ5f2nAdtn5vMHYr4AvD0zv1Pe/yrwysw8eyjXARRHJgHcHfhxxUtuCPyuRYlt4rvM3adapmWcfaplWsbZp1qmZZx9qmVaxtmnWqZlnH2qZVrG2adapmWcfaplWsbZp1qmZZx9qmVaxtmnWqZlnH2qpS72zpm5tPIZmdnJBjwe+PDA/acBHxiK+SLw7wP3vwrcb5avd3ZX8V3m7lMt0zLOPtUyLePsUy3TMs4+1TIt4+xTLdMyzj7VMi3j7FMt0zLOPtUyLePsUy3TMs4+1TIt4+xTLdMyzj7V0jZ3ZnZ62toVwGYD9zcFfjOLGEmSJEmSJC2QLptHZwFbRMRdImJNYB/g5KGYk4F9y6uuPQC4LjOv7LAmSZIkSZIktbCkq8SZeWNEPB84DVgd+EhmXhQRzy0fPwI4BdgZuBT4K7D/HF7yyA7ju8zdNn5Sc7eNn5ZapmWcbeMnNXfb+GmpZVrG2TZ+UnO3jZ+WWqZlnG3jJzV32/hpqWVaxtk2flJzt42fllqmZZxt4yc1d9v4aamlbe7uFsyWJEmSJEnS5OvytDVJkiRJkiRNOJtHkiRJkiRJqmXzSJIkSZIkSbVsHkmSJEmSJKlWZ1dbG5eIuB2QmXntfMaOI16SJEmSJKmtiNgI2ARI4DeZeVWnrzeJV1uLiM2BdwKPAP4IBLAu8DXgVZn5i9nEjil+PWBHBiYZOC0z/zib90KTIyKWAM8E9gDuxC3zfxJwVGbeMNv4LnNbi+N0nI5zsdfSp9qnZZx9qmVaxtmnWqZlnH2qxXEurnH2qZZpGWefaomIrYAjgPWAX5e7N6XoRxyUmcvpwKQ2j74PvA84PjNvKvetDjweeFFmPmA2sV3HR8S+wOuA01l5kh8FvCEzPz6Ue1o+/NMyzk9TfKE/BlxR7t4UeDpwu8x84lDuxvFd5rYWx+k4Hedir6VPtU/LOPtUy7SMs0+1TMs4+1SL41xc4+xTLdMyzj7VEhHnAc/JzB8M5XgA8KHM3JIKEXEPYDdWPpDl5My8uCp+FZk5cRvw06aPtYntOh74MbB+RdwGwE8q9n8aOBx4QPnB2bS8fTjwmXHGT2ruPtUC/HiGz0rV/DeO7zK3tThOx+k4F3stfap9WsbZp1qmZZx9qmVaxtmnWhzn4hpnn2qZlnH2qRZm7kFcWrP/lcB5wKuAp5bbq1bsq8s3uE3qgtnnRMRhEbF9RNyp3LaPiMOAc+cQ23V8UHT4ht1cPjZsm8w8MDPPyMwryu2MzDwQ2HrM8ZOau0+1XBsRj4+If37vImK1iHgicG1F7jbxXea2FsfpOB3nYq+lT7VPyzj7VMu0jLNPtUzLOPtUi+NcXOPsUy3TMs4+1XJqRHwxIp4YEQ8qtydGxBeBL1XkhuJsmftn5tsz8xPl9nZgu/Kx0Zp0mPq2AWsCB5ZvzAXAheXtg4BbzTa263iKQ85+RnFUyn+U2xHlvv0qcp9BcfrbagP7VgOeCPxgnPGTmrtPtQDLgM8A1wA/AX4KXF3uu0tF7sbxXea2FsfpOB3nYq+lT7VPyzj7VMu0jLNPtUzLOPtUi+NcXOPsUy3TMs4e1rITRS/h88AXyts7D8cNxF8C3Lli/52Z4ainwW0i1zyaZBGxAfAYivMMg+J8xtOy4gptEbEMeAfwcIpuY1AsivV1ikPLLhtX/KTm7lstA8+7PRCZ+buqx+cS32Vuaxl/7j7V4jgXVy3TMs6u4yc1t7WMP7e1jD+3tYw/d59qmZZx9qmWaRln32ppmHNH4FCKptTl5e7NgbsCz8/MuiOWbskxqc2jiHgMsDsrL/Z0UtWg28SOI758zu2ArGoa1cRPxYd/sY8zWl5tr018l7mtxXE6Tse52GvpU+3TMs4+1TIt4+xTLdMyzj7V4jgX1zj7VMu0jLMvtcQtF23afSi28iJPA89bjeI0tcEDWc7K8sJfo0zkmkcR8T7ghcA3gXcC7ypvHxwR759tbNfxEbF5RBwTEVcDPwDOioiry33Lasa6XhTnOT4deFoU5zKuP8N701n8pObuSy1RXG1vObADcBvgtsDDKNbN2ncu8V3mthbH6Tgd52KvpU+1T8s4+1TLtIyzT7VMyzj7VIvjXFzj7FMt0zLOntXyv8BWwBuAnYHHlre3BD4xnHuFzLwZuKzcfgZc1rRxtCLBxG1UrE5e7g9WvcJZ49iu44HvU6yFs/rAvtWBfYAzKnLsyy1rJP1nua1YI2nfccZPau4+1UL7q+01ju8yt7U4TsfpOBd7LX2qfVrG2adapmWcfaplWsbZp1oc5+IaZ59qmZZx9qkWWl7Jrdy/FcV6vRcDXwa+QrEO0hkUF4GqzDe4LWEy/T0itsvMM4f23x/4+xxiu47fMDM/M7gji07fMRHxporcrwHul6seprYBxZFLHx9j/KTm7lMtQbur7bWJ7zK3tThOxzm33H2qZVrG2XX8pOa2lukeZ59qmZZx9qkWx7m4xtmnWqZlnH2q5dqIeDxwQhZHExHFKWmPp/pKbgAfBZ6TmT9Y6UUjHgAcTXHU0owmtXm0H3B4RKxDcZ4ewGbA9eVjg/YHDmsY2zZ32/hzIuIw4GPcskjVZhSnO51bkXtaPvzTMs63AMsj4nRWXqTsUUBV87BNfJe5rcVxOk7Hudhr6VPt0zLOPtUyLePsUy3TMs4+1eI4F9c4+1TLtIyzT7XsQ3HRpsMi4lqKf3euR3HRpn0qcgPcdrhxBJCZZ0TEbWues5KJXTAbICI2ZmCxp8z87XzEdhUfEWtSLGy1GysvUnUyxcJW/zcU/3TgtUDlBygzPzqu+EnN3cNaNqDh1fbaxneZu4e13A549ELXMi3vufPpfLaM78V8dh0/qblnUUvj+RxDLROZu4e19OI7OmXveZe19GI+xzDOXuQeQy2d/czt2TinpZZOf4eWz7k9jL7IU0QcAvwrxZkxK/7duhnFMiyXZebzZ3o+5YuMiumdsglzQ5bFR8TDgG2Ai3LoCmcRcd/MPL9l/s2B6zPzj1EsZL0tcHFmXjTDc7alePNvpFjr6JI2rzlD3j59+Ccydxnfm1+sfRUR22Tm8g7yrgtsAfy8q/ckIjZs8ANzA+DGzPxTw5ytrojYN87nKvHOZ3Ve53OBTOqcNpnPMm6q5nRS57N8Db+jQ7qazzL3gn9Hnc95zb3g81nGNZ7TSZ9PmNyfufM5n9HySm7lc3ai4kCWzDyl0QCywcJIfduAHwIblLdfDnyPYqHiLwNvG4q9CbiU4lCvezXI/SqK1ccvAZ5V/nkUcBHwkor4hwJnUyw4dS3wBeC7wDeAzSriH0OxuPLJFJfSOxzYcaHf01nOQ6OFtWaRd13gfivmuKPX2LBBzAbAOi1y3m62NQMXVOzbDDgG+DbwH8AaA499bij2HsCpwBcpOsofBf4InAncs2ruKrYrgK2r5hV4xsDtTYCvlp/37wF3G4r9xIr3t/y8X15+P34JPL4i9x+ADwOPoGxoj3ivdiq/o98p672IYoHyK4BHDMXeiaK7fl35s+BX5fb6wfdzIH7z8j2/Gvhp+bPj6nLfsnHNZ9s5dT6dT+ez+XzOx5y2mc+2c9pmPrue0zbzOcnf0S7n0+/odP/MbTuni3E+q+bU+Rz/z9xJnc+2c9rlfLad047nc19aXORpvrZOkna9ARcO3D4bWKu8vQQ4fyj2XOA+FOcQXkrReHpV3ZeknNS1gNsDfwKWlvtvO/i6Q/lXxNwFOLG8/Sjg9KHY9wGnUJyH+O/ltk+57/0t34NF84u1yy9tGd+LX6zAnjXbXsA1Fbm/DDyXYmX8D5Tv3e1XfO6GYr8F7AI8qXzf9qHoJu8CfLUi981lvq8PbH8r//xaRfzygdvHAs8BVgP2GM4/+NksX2NZeXtD4IcVuX8MPJ+i6fpr4P3AA2aYz/OAewIPBH6/Irbct3wo9mvADgPv/3spvstvBo6syN34iohdzmfbOXU+nU/ns/KKpRP5M7fNfHY9p23mc5K/o13Op9/R6f6Z23ZOJ3U+286p8zn3+Ww7p5M6n23ntMv5bDunHc9nqyu5jdqG89fGtU3ch62c2PuUt7/ELUch3ZqhBs/wxADbAe+haFB8ryL3+eWfq1M0AVYbeKyqeXT+wO3Vhz6wFw3F1l02LyhOdZv1l7aMn8gfxF1+acv4xl/cNl/aMqbNX5RuoGjQHV2x/amq7qH7T6VofP1rRd3nDty+dKbvQLlvb+CbwM4D+y6b4T1cPkNd5w7dvwhYt7z9HVb+Dl00IvfmwCuA5cDPgbeOiL98xHs2/Ev8nIHbl1TkXuV7WPdYl/PZdk6dT+fT+az8HTqRP3PbzGfXc9pmPrue00mdz7Zz2uV8tp3TSZ3PtnPa5Xy2ndNJnc+2c+p8zn0+287ppM5n2zntcj7bzmnH8/kTYL2KHOvVzTXFWTJV2+0p1myu/HwMbkuYTM8FPhkRP6Ro8JwdEd8E7gu8dSg2Bu9k5pnAmRHxUuD/VeReHhGfomgYfBX4WER8CXg48KOK+LMj4qgydjeK09WIiNtQNBEG/T0ititrGHR/4O8VuT8DfJLqq3ndumLf0sw8orz9goh4KvCtiNi1Jsc6mfn5st43ZeYx5f7PR8QbhmKfALwAeFeW50RGxGWZ+bCKvMPulplPKG+fGBGvHXp8tYhYNzOvp2hS/QogM38XEVWf0b9k5qHAoeX6VPtQrDS/PnBMZv7HUPzNmXlxWfNfM/OMMv/FUVzScNDtM/Mb5eOfjYjXZOZfgP+MiKp1rDbMzM8M7sjMm4BjImJ4VfzzgXdn5oXDSSLikRW514iIW2fm38u8n4iI3wKnUXw+Bw1+1t4z9Niaw4kz8/jyc/2miNgfeCnVn5EVNi0XWQtgaUSskZk3rKhzKPYNwNcj4oMUDb7jIuIkiu/Ql1jVP7+jmfkr4J3AOyPi7lRfLeCPEfEcitMbr42IF1M0KB8J/Hko9prye/A1iqbrLwAiIiiamcPaXBGxy/mEFnPqfDqfzmflFUsn9Wdum/mEbue0zXzC5H5H/R3qz9yu5hPazemkzie0m1Pnc+7zCe3mdCLns8zn79BV57PtldwArqE4YGSwP5Ll/TvUPGdlTTpMfdwoPnQ7AS+k+AA9kepDt57cMu8SiiNx9ilvPwg4lKKreNuK+DWAg8qYZ1MegUJx6tudh2K3AX5A0YQ6vdwuLvfdryL3OZRHWFU8dnnFvouAWw/teyTFqVRXVsQPHjV10NBjVUdZrU1xJM5xFB/On8/wPl4NHEJxBNSvWfkUuuGjw55QjvUZFJccPIHiPM6PAv9dkfvcmte8O/C6iv1fozjy6eUUPwhfTHEq3dOB7wzFfoWiW34nimbZCeX+oOLIMYrT0w4Dti+fc6fy9mHAsUOxDwE2r6l924p9LwYeWrF/a+DLQ/ueA6xdEXtX4H0jPvNbURxBdvUMMU8f2lYc7bcx1f9zctdyLk8EPk9xPu5janK/p8l3cyB+M+BDZc6Ny/fpQorTL4dPt9yc4of0hRSnR96x3H97YK+K3GsCB1L8srigfN6XKL7jtxrXfM5lTgfmc5WjE51P53Na5rPrOZ3tfDaZ07bz2eWctpnPrud0UudzNnPa1Xy2ndN5ms9Txz2fc5lTOvg7UZs5nYD5nJefuQswn/P2/Swf22Kh57PtnC6G+RzIOWm/Q4+Y7/ks929A0bN4KfCy8nbt+rsUS6zUzekqvYWqbSKvtlYlWqy43ia2i/iI2JiBFc4z87c1cQ8BfplFV3P4sW0z8+yhfS+mODzum0P7twbemZmPGtr/HOCTmfnnof13BZ6fmS+qqWsriibSvTOzsksZxSXsB52cmdeWYz84h44OKl/z2cDdKJp2V1Cs03RaRe73ZOZLql63ppbNKBYRu5miu/wk4JkUndeXZXlUUhm7OfBu4F4Up7u9PDOvjOISiDtk5glDudcsc+3GLXN6OcUPnqMy8/+a1rmQyo72Olkc/aUJ53wuLs7n4uOcLi7lfK6dDa9cpX7z+7m4OJ+Lj3O6smh49byIeB7FgRM/rHjsBZn5gZEv1rRz1veNivMj5yN2HPHlc+6x0O9hy3qD8vxQt1bv22Momk3LhvY/Y67xXeaeQy13nqRays/1E4DHl7cfQXEE3UEMnAPdNrbr+EnNPSL+wLnWUvPZWWWNtvmI7Tp+UnPPZy0MXSWT4ijRQ4ADqLh4Qpfxk5p7Hmt59jzVsgdwu/L2UorTKS6gOG1/0xGxH6+LbRs/D7lr656HWuYt9xjel/cAD274XW8c23X8tNQyi9y3A15L8XeooLgYzxeAd1FxdMNA/LPK+NfUxbeJnYfc81Z32/xd5q6Zo1H5H0ZxtsxJFGd6vB246wyfgcbxXeZehLX861xzM49Xz2uzLaYjj87NzK3nO3Yc8eVzfpWZm7eIf21mvrEP8ZOae75qiYjHAJsCX8nMXw7sf0ZmfmTg/lsprrC3nGJR8vdl2eGNiOWZuc1Q3rcBD24S3yZ2DLV0lrtt/lnUchjFOb9rAtcDt6I4imxn4KrMfOFsYruOn9TcY6jlfFYWFEc3/hggM+87m9iu4yc19xhq+ed3NiL+k+KQ+k8Bj6M4ivfF44qf1Nw9rOVHmXmv8vZngDMoTo1/JPCUHDhiuk1sn3JPWS0r1tNYStFg+nRmnkuFNrFdx09LLbPIfQpFs3BdiovMXEBxSs2jgC0zc7fZxk9q7gmv5e3ARhTr9O5OcSXqn1D8B9xbM/O4odyN47vMbS21ub9PcSX347NYb5eIWJ3iP1hflJkPoEJErAfsSHHGTAK/AU7LzD9Wxa8iO+pKjXsDdu8idj7jKf73rWr7AHB9y9f4VV/iJzX3fNRCsUD7t8ov78+AFww8Nnz1hwuAJeXt9YFTgPeW98+tyN04vsvc01ZL+ecaFFfmW7O8v2TFY7OJ7Tp+UnOPoZaTKc4ZvwdwZ2AZxWmld2bVI9Eax3YdP6m5x1DLuQO3l1OuQ1h+Hqrmv7P4Sc3dw1p+PHD7nKHHzpttbJ9yT1kt55Z/bgH8F8VanJcAr6O4eMqsYruOn5ZaZpH7vPLPAH7dYP4bx09q7gmvZfDq1kuA75a3N6B6rdvG8V3mtpba3K2unlfu35fi36uHUyzp8p8U6zH9DNi3Lt/gVrWy/kSIwvYRsWdE7AFcGREx19iO4/enWADrnKHtbOAfFXmvr9n+RLEw89jiJzV317VQHM3y8CzWh7ofsFNEvHdFuqHYJZl5I0AW3d1dgHUj4jgqrizQMr7L3NNUy4rYG4CzMvMf5f0bgZvmENt1/KTm7rSWzNyV4pDfIyn+B+4XwA2Z+cscOEqwbWzX8ZOau+tagLUiYuuIuB/FxSn+Uua5gerPVpfxk5q7b7V8IyLeGBFrlbd3B4iIhwHXzSG2T7mnqZYEyMyfZuabMvPeFKca35riP29mG9t1/LTU0jb3ahGxAcWiv2tHxDKAKNYBrfo7VJv4Sc09ybXcHMXaOFD8m2Z1gCzWyan6d2ub+C5zW0t1/DkRcVgUPYg7ldv2URylf25FbihOa7xfZh6YmW8ut+cC21I0kkbLBh2mvm3AoynO6zsV+HC5fanc9+jZxnYdT3HVrwfVjOmyin2/Ajaqia+62lpn8ZOaewy1XDx0f3XgKIrDui8aeuwLVF+J4M3AzRX7G8d3mXvKajmV6qs/bAycOdvYruMnNXfXtQw8fluKtR5Opjh1pjKubWzX8ZOau6taKK6uMrgNXoXk7HHGT2ruHtayBvB6it+9v6K4sMWfKE5123y2sX3KPWW1nDvqZ8NsYruOn5ZaZpH7ScBV5bYXxRWJv0JxFeUD5hI/qbknvJYnUpy2eHr5fX5suX8p8KmK3I3ju8xtLbW516Th1S0HnvMTYL2K/esxw5FMK8W2+SHSl43i8vbLKvbfhVX/Md84tut4ikXNbtNinG8Gtqt57B3jjJ/U3GOopU0jYy1grZrcm1TsaxzfZe5pqqVuo/jH7R3mO7br+EnN3VUtwJbAcxvmbBzbdfyk5u66loHnrU67362dxU9q7j7UQvGX19s3zNc4tk+5F3stVDT35yO26/hpqaVt7vI5q3PL6f9LKI5QuON8xE9q7gmv5Xbl4+s3nP/G8V3mtpb52YCnc8tpa/9RbitOW9uvUY75LmocG8WK4ksq9q8JXDrb2HHEV8Rts9Dvp9vsN+bYnABe3/L1Gsd3mdtaHKfjXDy1TMs4J7n2aRlnn2qZlnH2qZZpGWefanGc1jKJua1l5HNeO8NjGwD7AC8FXlbeXuXKfHXbpK559BHgrIh4ZUQ8udxeCfyA4pSh2caOI37Yh5sNuRARr+9L/KTmns9aMvNvmfm3qvjM/HWD9Ls2rWMW8V3mbhs/LbU4zvHHT2rutvGTmrttfJ9qaRs/qbnbxk9LLdMyzrbxk5q7bfy01OI4xx8/LbVMyzjbxnddC8Cz6h7IzGsz85jM/O/MfHd5+9qmiSeyeZSZbwOeQrFw1AOBB5W3n1I+NqvYccRXqF2Iu8a0fPinZZxt579NfJe528ZPSy2Oc/zxk5q7bfyk5m4b36da2sZPau628dNSy7SMs238pOZuGz8ttTjO8cdPSy3TMs628fOSO9pf5GnmF4m4oFFcefiSFkhE7J6Zn2sRf25mbt2H+EnN3adaImK1zLy5Re7G8V3mtpbx5+5TLY5zcdUyLePsOn5Sc1vL+HNby/hzW8v4c/eplmkZZ59qmZZxLlQtEfEr4P6ZeVXFY5dn5mYV+/esexngiMxcOrKeSWweRcR6wKuB3SlWHwe4GjgJeHsWl+RuHTum+AC2AzahuGTmbyiuEtRoIhbjh3/cuReyloh4DMVnZXD+T8rML801vsvc1uI4HafjXOy19Kn2aRlnn2qZlnH2qZZpGWefanGci2ucfaplWsbZl1oi4s3AyZl5ZkWOd2TmKyv23wB8ssw7bO/MXKeqppVyTGjz6DSKy95/LDN/W+7bGNgPeERmPmo2sV3HR8SjgcMoFtlesR7OpsBdgYMy8/SKsS76D/+0jDMi3gfcDfg4cEW5e1NgX4rLI75wtvFd5rYWx+k4Hedir6VPtU/LOPtUy7SMs0+1TMs4+1SL41xc4+xTLdMyzr7V0lZEnAM8PTMvrHis8milVWTL1bv7sAE/bvpYm9iu44GLgWUVcXcBLq7Y/z7gFIpV0P+93PYp971/nPGTmrtPtQA/qfmcBMUPhOH9jeO7zG0tjtNxOs7FXkufap+WcfaplmkZZ59qmZZx9qkWx7m4xtmnWqZlnH2rpSLu9SMefwiwec1j247Knzm5V1v7ZUS8IiI2WrEjIjaK4ipnl88htuv4JdzSRRz0a2CNiv07Z+bOWayC/p1yOwZ4LLDzmOMnNXefavl7RGxXkeP+wN8r9reJ7zK3tThOxzm33H2qZVrG2XX8pOa2lukeZ59qmZZx9qkWx7m4xtmnWqZlnH2rZdiuMz2Ymd/OzF/VPHZ2g/wsaRLUQ08EXgV8s2zaJHAVcDLwhDnEdh3/EeCsiDiGWxpLm1EcqXJURe6/R8R2ueq5jDN+4DqKn9TcfaplP+DwiFiHW5qImwHXl48NaxPfZW5rcZyOc265+1RLl7knuZY+1d5lbmuZ7nH2qZYuc1uL45yGcfapli5zW0t9/LC2V3IjIpZn5jaN48vDlCZaRDyEYhHqC3Jo3aCI2B64JDOvi4jbUDR6tgEuAt6amdcNxR8MnJiZVUcZVb32msCTgF9n5lci4inAg4AfAUdm5g1D8fei6ApuQjHBV1AsdvWjitzbAIcDVR+ggzLznHHFT2ruvtVSPmdjBuY/y7Wy6rSJ7zK3tYw/d59qcZyLq5ZpGeck1z4t4+xTLdMyzj7VMi3j7FMtjtNaJjG3tYwWLS/yVD7n3GxxVfGJPPIoIs7MzO3K288Cngd8DnhdRGyTmW8fCP8IsGV5+33AX4C3A48Ajgb2HEr/JuBVEfEz4FPAcZn5uxnKOZrifVwrIp4O3BY4scy/HfD0weCySbRKo6hKZi4Htm/6AeoyflJz962WiAjgztyyuPbqEXFV1nRx28R3mdtaHKfjdJyLvZY+1T4t4+xTLdMyzj7VMi3j7FMtjnNxjbNPtUzLOPtUS1RctCkiai/yVOOLLWIns3nEyusDPQd4dGZeExHvBs6gaA6tsFpm3lje3jZvOSzrOxFxXkXunwP3Ax5JcUraG6NYmfzTwGcz809D8f+WmfeNiCUUaxfdKTNviohPAD8cDIyI9YBXU0zy0nL31cBJwNsz84/DxUzRh3/RjzNmuNpeRKxytb028V3mthbH6Tgd52KvpU+1T8s4+1TLtIyzT7VMyzj7VIvjXFzj7FMt0zLOPtUS9VdmOzgidsoRV2aLiNsBmZn/OVPcKrLBqtp92yiaMhsAtwfOHnrs3KH7xwH7l7ePplxJvHyzz6rIvXzo/hoUp5l9GrimIv5CYM2ynj8Btyv335qhK6gBpwGvBDYe2Lcxxal0X67I/WjgUuBU4MPl9qVy36PHGT+puftUC+2vttc4vsvc1uI4HafjXOy19Kn2aRlnn2qZlnH2qZZpGWefanGci2ucfaplWsbZp1qYxZXZgM2BY4BrKBpUl1IcyHJM1etWbZN65NF6wDkUb05GxMaZ+duIWLvcN+hZwPsj4j+B3wHfj4jLKRasflZF7pWen8WaRScDJ0fEWhXxRwGXAKsDrwGOi4ifAw+gmIhByzLzHUP5fwu8PSL2r8j9fuCRmfmLlQqMuAvFpeDvOcb4Sc3dp1raXm2vTXyXua3FcTrOueXuUy3TMs6u4yc1t7VM9zj7VMu0jLNPtTjOxTXOPtUyLePsUy1tL/IE8BmKZXyekpk3AUTE6sDjKfoWD6h53koFTpzMXFbz0M3AHkOx1wH7RbFq+b9QTkpmXlWT44kzvO7fKva9NyI+U97+TUR8nOKUt/+pmMxfRsQrgI+teP0ortC2H7dcfW3QtHz4p2Wcba+21ya+y9x18ZtTfF+6qKUud9v8i+0977IW59P5XEzz2XX8uHPP13veNv80v+f+Dl187/lC/Mydj1omYT77VMu0zOd81DIJueejlsX4O3Q/2l+ZbcPM/MzgjrKJdExEvKnmOStZFFdbmxQRsQHFKWq7AhuVu6+iOLLpHZn5h6H4VwNPoOgEDn+Ajs3Mt40rfp5yr/hijS33Ao1zpvjGV9sr4+8J7NYkfha5u6ylcWzX8V2+L12+h32qxfl0PruKH8PPrYn8mduzOersc942flLncxa19Gn+J3I+e1hLL+a/Tz9znc+J+Jz3Yj7HUEuf5r/t+7LxYGzOfNW3Y4A/AB9j5X+3Pp2isfSEuuf+M4fNo/GKiLtSHB21GXAj8BPg0+URUlXxU/GLtWdf8k5/sS4WEXGHzLy6o9y3z8zfd5Fb1ZzPxcX5XFy6nM8yv3M6Zn5HFxfnc3FxPheXxfo7NCKC4urum1BebQ04M2saPBGxJvBMKv5NDByVmf838kWzwcJIbvOzAQcDpwP/CXyPYjX1twA/AnZY6PrGMP47dJj79gs9vgY1rkdxJcBLgN+X28XlvvVb5jp16P66wNuA/wWeNPTYYRXP3xg4HPggxcLzrwfOB44F7lgRf7uK7RcUC8Xfbih2x6Exf7jM/Slgo4rcb6fodkNxpcOfUyzi9kvgoRXxy8vv0L80eJ+2Bb4OfIKiYftl4I/AWcDWFfFrA28ELgKuo1hQ7gxgv3HOZ9s5dT6dT+dz8fzMbTOfXc9pm/mc5O9ol/Ppd3S6f+a2ndNJnc/5nFPns5ufuZM6n23ntMv5bDunHc9nq4s8zdfWSVK32g/EBcDq5e3bAN8ob2/O0FXiyv1T8Yu1yy9tGdOLX6y0v9reNjXb/YArh2JPKN+X3Sm6xycAt1ox/orcXwJeUL72+WVdm5f7TqqIvxm4bGi7ofzz58Pv98DtDwNvBu4MvBj4XNX3YuD214H7l7fvxtDVFMv9lwHvBn4FnFnmvVPNfJ4J7AQ8ieLwzL3L/Y8Avl8RfxLFecKbAi8B/gvYguLwzreOaz7bzqnz6Xw6nyvPZ9dz2mY+285pm/nsek7bzOckf0e7nE+/o9P9M7ftnE7qfLadU+dz7vPZdk4ndT7bzmmX89l2Tjuez1ZXcht4/DEU/6Y/ufxMHM7Av8VHbY2C3OZno2gerfhibACcM/DYhRXxU/GLtcsvbbm/F79YgR/P8NlY5THgJuBr5RiHt78NxZ43dP81wHcpGn1VP4TPHbj9q5lylfteVn4G/m3wfa0Zy/IZ6qrKfQmwpLx9Rt1c1+R/CMURfL8t35cDWozz3IrcPxy6f1b552rAJeOaz7Zz6nw6n87nyvPZ9Zy2mc+2c9pmPrue0zbz2fWcTup8tp3TLuez7ZxO6ny2ndMu57PtnE7qfLadU+dz7vPZdk4ndT7bzmmX89l2Tjuez5+uqGNo/5rApTVjfR/F1cD3Af693PYp972/7j1aKUeTILf52YAXUjRejiw/ePuX+5cC36qIn4pfrF1+aSvyL+RflE4HXsHA0VQUC6e/EvhKRe4LgS1q3rPLh+5fDKw2tO/pFEdE/XKmuoE3j3oPy/2bAscB7wHWoaJ7X8ZdQdFIeynFUWEx8Nj5FfEvKN+bh1Mc0fY+4P8BbwD+d6b5HNi3OrAjcPTQ/u9THNb5eIoj03Yv9z+U6kbj94B/L2/vApw28NjwL8rO5rPtnI6Yz1Xec+fT+Vzs89n1nLaZz9nMadP57HpO28xn13M6yfPZZk67nM8y/tymczqp89l2Trucz7ZzOqnz2XZOnc+5/w5tO6eTOp+zmdOB+XzvfM5n2znteD5fDZxbzseTy+2V5b5X14z1JzX7A/hp3Xu0UmyTILf524B7A3sD92gQOym/WG02ZKO/KG0AvIOi+XUtxWr3F5f7qs7B3Ru4e817tvvQ/XcCj6yI27HqhwHFqXZrV+y/K3D8iM/lLhSn5v225vHXDW1Ly/0bAx+vec4OwGfKH3gXUHTADwDWqIg9psl3rYzdkuIIvlOBewDvpzgN8SLgQTXxZ5Yx31nx/lM0eA8e13y2ndMpn89ry/l8cIP5vFuL+by2nM93Tth87trBfD6sYj6fMw/zudUs5/M6Rnw/a+Z04n7mMuL7OYc53aFiTlf5jraZzznO6R9p/x2duPlsMqddzmfbOaX9z9z79mE+285pl/M5mzml+mfuQvydaMV8rviZWzufbee0x/M54+/QWc7nDl3M5wxzWvkdpcXP26H5vLicywWZz7nMKe2/n68dNZ9Dc7qcGf5e1OV8lvH3ojgr6APAoeXte82Q/3xgu4r921Hz7/lVYtsMyG28Gyv/EB7+0m5QEd/XH8Tj/ItS1eF74/jF+kea/SC+B/DI4feSmnNNy/hHNImfIXanueYejgfWAu4zi1rmPM5ZvC/3bJn7nk3niOKH7YrTJu9N0QDdeYbP12D8vSiapvMSP8fc/0axLlgXuef1fQG2b5l7+za1Dz13lWb0iPjKn1fzEd8ydi3guD7UPYvaO3vPKY48fSkNF5KkOJz7JU3i28TOIvdDyu/nvOdu+750mXtU/vK7vF55+zYUfy/5AsXfidariV+3vL1WGf/5qvih3DPG1uR+Q4v421D8/esrDWq5TctaxvG+jMo9WHvt+0JxEZnNmnwuuo6fa24G/k600LUsVO4yfk2K/4R+ZHn/KRRrpT6PVf9RfStg34HYJ1P843eV2Lbxs8i95lD80yjO5Jiplqc3rGU4d+17Mofam77nw7Ez5i5j7gq8HDiE4mCA51Lx3S9j/5XizJP3A/89U2zX8UOx76E48GCm3CvG2aaWrt6X4VoOnCm+zUaxnM0PKC7WdXq5XVzuu1+THFEm0oSJiP0z8+g+xDeJjYi1gH/NzAvnO/e44ueaOyIOpvgBfTHF/86+MDNPKh9bnpnbDD2/cXxEvAB4fovcbeO7rKXt+9I4f5n7IIoGbNNaGsVHxOso1sdaQrGw+nbANykaT6dl5luGcg/Hbw98Yz7i5yF3be1d5p6Hcc5n7pNZ1cMp/gJJZu46lHs4Pij+t3jO8fOQu7b2eRhn2/g245zvWs7MzO3K28+i+DnzOYqjSz+fmW+fIf7ZZfyJVfFtYuch90Et6n4Wxc/HprXM+L7MwzjnrfaIuAjYMjNvjIgjgb9QrNX4iHL/nkO5h+P/ChxfFd8mdp7ia2vvMvc8vC/zmfu6Mt/PKC6Qclxm/o4aQ/GfLuOvmY/4ech97HzV3uX7MsdxforiP4tniv8kxe/btSiOVrotxff5ERRnFTy9IvY2FP8Buzbw2arYBvFk5n6zia2Jr617HmqZS+4m8U3e8xXzMyr3wcDjgG8BOwPnURwFswdwUGZ+YzaxXceXsbtQ/P2wi1peCDx2oWuJiLUpzlDai+Jsn39QfFePyMyPMoOI2BjYhOLvaFdk5m9nil9JzkMXy238G0Pr8Sxk/KTmHnctFEdIrV3eXgacTdGcgOpz1hvHd5nbWmbMvTrFL+zrWfl/cqtOt+wsflJz96kWikOPP0FxVONDyz+vLG8/tCL3uV3FzyJ349q7zD2GcbaOH7h9Frcc1XpbqtfHaxw/qbknvJaLB24Pr8t4XkXuxvFd5raW2tznUqwP+WjgKIqr1X6J4siIdao+K13FT0stYxjn+eWfS4CruOUK08Gqv3Mbx/Yp95TVcgENrxLeJrbr+GmphZZXzxt4XlD8Z+qeFE2p7eGWJWNGbUtQb0XE+XUPUax9NLb4Sc3ds1pWz8w/A2TmLyJiB+D4iLhzGT+sTXyXua2lOv7GzLwJ+GtE/Cwzry+f97eIuLkid5fxk5q7T7VsS3FRg9cAL8/M8yLib5n5zYq8UFzFsqv4trnb1N5l7q7H2TZ+tYjYgOIfP5Hl/5hn5l8i4sY5xk9q7kmuZfDI5R9GxLaZeXZE3I3iaq7D2sR3mdtaquMzM2+mPHUiItbglqvdvpvi9P9xxU9LLV2Pc7WIWJOi+XsbYD2KJTduBawxh9g+5Z6mWqBoNN1UxqwDkJm/Kj8Lc4ntOn4aalmWtxxh9J6IOCsz3xQR+1OclvYfw4kj4tEUF476KfDrcvemwF0j4qDMPL2mpltkwy6T2/g3iq7wVhSXrR/clgG/GWf8pObuUy0Up1ZsNbRvCfBx4KaK3I3ju8xtLbW5fwDcpry92sD+9ahepL2z+EnN3bdaysdWLPR/KA2ONuwyflJz96UW4BcUF2G4rPxz43L/2lQfNdE4flJzT3gt6wEfpTgs/wcUzYifU5wOsGVF7sbxXea2ltrc587w3V2rYl9n8dNSyxjG+eJyvn9JsV7SV4H/oTia4nWzje1T7imr5YU0vEp4m9iu46elFla+aNOujLh6Xrn/Yoqm0/D+uzBw5OhM28gAt4XbKA4R/feaxz41zvhJzd2nWij+wbNxTWzVCvqN47vMbS21uW9VE7ch8G8V+zuLn9TcfatlKOaxzHDY7zjjJzV332oZeN5tgLt0ET+puSepFor/id2S4ki0jRrkahzfZW5rWSXmbk0/D13HT0stXY+zfM6dgDuVt9enuJjPKld3ahvbp9xTVsu9aX6V8MaxXcdPQy20vHpe+dhPqb641JrApU3qc8FsSZIkSZKkCRERd6VYt2gziiM9fwp8OjOvq4l/NfAE4Bjg8nL3ZsA+FAv/v23ka9o8kiRJkiRJ6r9oeZW4gefdi+I0t39ebQ04OTN/1Oh1bR5JkiRJkiT1X0RcQLFO600RcRvglMzcISI2B07KzK27eN3VukgqSZIkSZKkTiwp/1zpymzUXD0vItaLiLdHxCUR8ftyu7jct36TF7R5JEmSJEmSNBk+DJwVEUcC36e4Ai0RsRT4Q81zjqU4tW2HzLx9Zt4eeBjFotvHNXlRT1uTJEmSJEmaEBFxb+CewIWZeUmD+B9n5t3bPjZoyagASZIkSZIk9UNmXgRc1OIpv4yIVwAfy8yrACJiI2A/brn62ow8bU2SJEmSJGnxeiJwe+CbEfGHiPgD8A3gdsDjmyTwtDVJkiRJkqQpFBH7Z+bRI+NsHkmSJEmSJE2fiPhVZm4+Ks41jyRJkiRJkhapiDi/7iFgoyY5bB5JkiRJkiQtXhsBjwGuHdofwPeaJLB5JEmSJEmStHh9AVg7M88bfiAivtEkgWseSZIkSZIkqdZqC12AJEmSJEmS+svmkSRJkiRJkmrZPJIkSRMrIm6KiPMGtmULXVNfRMQ3ImLbha5DkiRNPhfMliRJk+xvmblV1QMRERTrO9483pLmV0QsycwbF7oOSZI0vTzySJIkLRoRsSwiLo6Iw4DlwGYR8fKIOCsizo+INwzEviYifhwRX4mIT0fEy8r9/zxiJyI2jIhflLdXj4h3DeR6Trl/h/I5x0fEJRHxybJxRUTcPyK+FxE/jIgzI2KdiPh2RGw1UMd3I+K+Q+PYLyKOi4jPA6dHxNoR8dWIWB4RF0TEbkPj/Z+IuCgiTo+ItYZyrRYRH4uIN5dj+GhEXFjmefG8T4IkSVp0PPJIkiRNsrUi4rzy9mXAi4G7A/tn5kER8WhgC2A7IICTI+L/AX8B9gG2pvj70HLgnBGv9Uzgusy8f0TcCvhuRJxePrY1cG/gN8B3gQdHxJnAZ4AnZuZZEbEu8Dfgw8B+wIsi4m7ArTLz/IrXeyBw38z8Q0QsAfbIzOsjYkPgjIg4uYzbAnhSZj47Io4F9gI+UT62BPgkcGFmviUi7gdskpn3AYiI9UeMWZIkyeaRJEmaaCudtlauefTLzDyj3PXocju3vL82RbNlHeDEzPxr+bwVjZiZPBq4b0TsXd5fr8z1D+DMzLyizHUesAy4DrgyM88CyMzry8ePA/4rIl4OPAP4aM3rfTkz/7BiaMBby8bXzcAmwEblY5dl5nnl7XPK117hQ8CxmfmW8v7PgX+JiA8AXwROR5IkaQRPW5MkSYvNXwZuB/C2zNyq3O6amUeVj2XN82/klr8j3Xoo1wsGct0lM1c0X/5vIO4miv+gi6rXKBtWXwZ2A54AfKrBOJ4CLAXuVzbLrhqoreq1V/ge8LCIuHX52tcCWwLfAJ5HcRSUJEnSjGweSZKkxew04BkRsTZARGwSEXcAvgXsERFrRcQ6wC4Dz/kFcL/y9t5DuQ6MiDXKXHeLiNvO8NqXAHeKiPuX8euUp59B0bQ5BDhr4OiimawHXJ2ZN0TEw4A7N3gOwFHAKcBxEbGkPOVttcw8AfgvYJuGeSRJ0hTztDVJkrRoZebpEXFP4PvlGtZ/Bp6amcsj4jPAecAvgW8PPO3dwLER8TTgawP7P0xxStjyckHsa4DdZ3jtf0TEE4EPlItY/w14JPDnzDwnIq4Hjm44lE8Cn4+Is8uaL2n4PDLzPRGxHvC/wNuBoyNixX8gvrppHkmSNL0is+6IbUmSpOkQEa+naOq8e0yvdyeKU8fukZk3j+M1JUmSZsvT1iRJksYoIvYFfgC8xsaRJEmaBB55JEmSJEmSpFoeeSRJkiRJkqRaNo8kSZIkSZJUy+aRJEmSJEmSatk8kiRJkiRJUi2bR5IkSZIkSar1/wGn6RxopYlZcwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x='Frequency ranks', y='Estimated frequency', data=grouped_df.iloc[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile a corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tmx files using translate toolkit module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsplit.filesplit import FileSplit\n",
    "\n",
    "fs = FileSplit(file='C:\\\\Users\\\\MdeCL\\\\Desktop\\\\opus tmx files\\\\Russian\\\\UNPC (need to split).tmx', splitsize=700000000, output_dir='C:\\\\Users\\\\MdeCL\\\\Desktop\\\\UNPC split files')\n",
    "\n",
    "fs.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corp_folder = 'C:\\\\Users\\\\MdeCL\\\\Desktop\\\\opus tmx files\\\\Russian\\\\For para_texts_df'\n",
    "\n",
    "num_sources = 0\n",
    "for tmx_folder in os.listdir(parallel_corp_folder):\n",
    "    num_sources += 1\n",
    "    \n",
    "para_texts_df = pd.DataFrame()\n",
    "\n",
    "ru_sents = []\n",
    "en_sents = []\n",
    "chunk_idx = 1\n",
    "\n",
    "for tmx_folder in os.listdir(parallel_corp_folder):\n",
    "    sent_idx = 1\n",
    "    \n",
    "    print(tmx_folder)\n",
    "    \n",
    "    with open(\"C:\\\\Users\\\\MdeCL\\\\Desktop\\\\Vocab-Project\\\\supporting-files\\\\opus tmx files\\\\Russian\\\\For para_texts_df\\\\\" + tmx_folder, 'rb') as fin:\n",
    "        tmx_file = tmxfile(fin, 'en', 'ru')\n",
    "\n",
    "        for node in tmx_file.unit_iter():\n",
    "            try:\n",
    "                node_target_sent = node.gettarget()\n",
    "                if 16 < len(node_target_sent) < 75:\n",
    "                    en_sents.append(node.getsource())\n",
    "                    ru_sents.append(node_target_sent)\n",
    "                    sent_idx += 1\n",
    "                    chunk_idx += 1\n",
    "            except Exception:\n",
    "                print(node)\n",
    "\n",
    "            if chunk_idx > 20000:\n",
    "                para_text_df = pd.DataFrame({'Source sentence' : ru_sents,\n",
    "                                            'Target sentence' : en_sents})\n",
    "                para_text_df['Source'] = tmx_folder\n",
    "                para_texts_df = para_texts_df.append(para_text_df, ignore_index = True)\n",
    "                print('20,000 sentences appended to para_texts_df.')\n",
    "                ru_sents = []\n",
    "                en_sents = []\n",
    "                chunk_idx = 1\n",
    "\n",
    "        print(sent_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly order DataFrame rows\n",
    "para_texts_df = para_texts_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, standardise and pre-process parallel corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates in source_sentence column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df = para_texts_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in source and target sentences\n",
    "para_texts_df.drop_duplicates('Source sentence', inplace=True)\n",
    "para_texts_df.drop_duplicates('Target sentence', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sent_quality(df_row):\n",
    "    return_bool = True\n",
    "    src_sent = df_row['Source sentence']\n",
    "    src_sent_tokens = src_sent.split(' ')\n",
    "    target_sent = df_row['Target sentence']\n",
    "    target_sent_tokens = target_sent.split(' ')\n",
    "    if len(src_sent_tokens) > 2.2 * len(target_sent_tokens) or len(src_sent_tokens) * 2 < len(target_sent_tokens):\n",
    "        return_bool = False\n",
    "    if sum(c.isdigit() for c in src_sent) >= 7:\n",
    "        return_bool = False\n",
    "    if len(src_sent) < 22:\n",
    "        return_bool = False\n",
    "    \n",
    "    return return_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df = para_texts_df[para_texts_df.apply(filter_sent_quality, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bare_source_sent(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = [char for char in sent if char.isalpha() or char.isspace()]\n",
    "    sent = ''.join(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows whose first 20 characters are the same as those of another row\n",
    "para_texts_df['Bare source sentence'] = para_texts_df['Source sentence'].apply(bare_source_sent)\n",
    "para_texts_df['Bare source sentence first chars'] = para_texts_df['Bare source sentence'].str[:20]\n",
    "para_texts_df.drop_duplicates('Bare source sentence first chars', inplace=True)\n",
    "para_texts_df = para_texts_df.drop(columns = ['Bare source sentence', 'Bare source sentence first chars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise the source and target sentences so they can be uniformly processed at later stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_sent(sent):\n",
    "    sent = ''.join([char for char in sent if (char.isalpha() or char.isdigit() or char.isspace() or char in string.punctuation)])\n",
    "    sent = sent.strip().lower()\n",
    "    sent = ' ' + sent + ' '\n",
    "    puncts = [',', '.', ';', ':', '!', '?', '(', ')', '{', '}', '\\\"', '\\'', '[', ']']\n",
    "    for punct in puncts:\n",
    "        sent = sent.replace(punct, ' ' + punct + ' ')\n",
    "    sent = sent.replace('  ', ' ').replace('   ', ' ').strip()\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_src_sents = para_texts_df['Source sentence'].apply(stand_sent)\n",
    "para_texts_df['Standardised source sentence'] = stand_src_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stand_en_sent(sent):\n",
    "    \n",
    "    sent = ''.join([char for char in sent if (char.isalpha() or char.isdigit() or char.isspace() or char in string.punctuation)])\n",
    "    if type(sent) == str:\n",
    "        sent = sent.strip().lower()\n",
    "        sent = ' ' + sent + ' '\n",
    "        elisions = {'that\\'s' : 'that is',\n",
    "                    'it\\'s' : 'it is',\n",
    "                    'what\\'s' : 'what is',\n",
    "                    'who\\'s' : 'who is',\n",
    "                    'I\\'m' : 'I am',\n",
    "                    'I\\'ve' : 'I have',\n",
    "                    'he\\'s' : 'he is',\n",
    "                    'she\\'s' : 'she is',\n",
    "                    'isn\\'t' : 'is not',\n",
    "                    'won\\'t' : 'will not',\n",
    "                    'gonna' : 'going to',\n",
    "                    '\\'ve' : ' have',\n",
    "                    '\\'re' : ' are'\n",
    "                   }\n",
    "        for elision_key in elisions.keys():\n",
    "            sent = sent.replace(elision_key, elisions[elision_key])\n",
    "        puncts = [',', '.', ';', ':', '!', '?', '(', ')', '{', '}', '\\\"', '\\'', '[', ']']\n",
    "        for punct in puncts:\n",
    "            sent = sent.replace(punct, ' ' + punct + ' ')\n",
    "        sent = sent.replace('  ', ' ').replace('   ', ' ').strip()\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_target_sents = para_texts_df['Target sentence'].apply(stand_en_sent)\n",
    "para_texts_df['Standardised target sentence'] = stand_target_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the source sentences to make subsequent processing faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sent(sent_df_row):\n",
    "    \n",
    "    sent = sent_df_row['Standardised source sentence'].replace('  ', ' ').replace('   ', ' ').strip()\n",
    "    \n",
    "    lemmed_sent = []\n",
    "    coll_lemmed_sent = []\n",
    "    pos_tag_list = []\n",
    "    cases_list = []\n",
    "    \n",
    "    sent_tokens = sent.split(' ')\n",
    "    lem = True\n",
    "    for token in sent_tokens:\n",
    "        p = morph.parse(token)[0]\n",
    "        lemmed_sent.append(p.normal_form)\n",
    "        pos = str(p.tag.POS)\n",
    "        if pos != None:\n",
    "            pos_tag_list.append(pos)\n",
    "        else:\n",
    "            pos_tag_list.append('None')\n",
    "        case = p.tag.case\n",
    "        if case != None:\n",
    "            cases_list.append(case)\n",
    "        else:\n",
    "            cases_list.append('None')\n",
    "        if lem == True:\n",
    "            coll_lemmed_sent.append(p.normal_form)\n",
    "        else:\n",
    "            coll_lemmed_sent.append(token)\n",
    "        if pos == 'PREP':\n",
    "            lem = False\n",
    "        elif pos in ['INFN', 'VERB', 'NOUN']:\n",
    "            lem = True\n",
    "    \n",
    "    lemmed_sent = ' '.join(lemmed_sent)\n",
    "    coll_lemmed_sent = ' '.join(coll_lemmed_sent)\n",
    "    pos_tag_list = ' '.join(pos_tag_list)\n",
    "    cases_list = ' '.join(cases_list)\n",
    "    \n",
    "    return lemmed_sent, coll_lemmed_sent, pos_tag_list, cases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "l_df = para_texts_df\n",
    "lemmed_sents, coll_lemmed_sents, pos_tag_list, case_list = zip(*l_df.apply(preprocess_sent, axis=1))\n",
    "l_df.insert(1, 'Lemmed source sentence', list(lemmed_sents))\n",
    "l_df.insert(2, 'Coll lemmed source sentence', coll_lemmed_sents)\n",
    "l_df.insert(3, 'Source sentence PoS tags', pos_tag_list)\n",
    "l_df.insert(3, 'Source sentence cases', case_list)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df = para_texts_df[~para_texts_df['Lemmed source sentence'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data to find disparities between estimated frequency and frequency in parallel corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for index, row in para_texts_df.iloc[100:200].iterrows():\n",
    "    print(row['Standardised source sentence'])\n",
    "    print(row['Standardised target sentence'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "anomalies_df = pd.DataFrame()\n",
    "\n",
    "for index, row in rnc_freq_list.iloc[2000:2010].iterrows():\n",
    "\n",
    "    estimated_freq = row['Estimated frequency']\n",
    "    query = row['Word']\n",
    "    query_pos_full = row['PoS tag']\n",
    "    print(index)\n",
    "    print(query)\n",
    "    \n",
    "    query_forms = []\n",
    "    query_parse = morph.parse(query)[0]\n",
    "    for word_form in query_parse.lexeme:\n",
    "        form = word_form[0]\n",
    "        query_forms.append(form)\n",
    "        if 'ё' in form:\n",
    "            query_forms.append(form.replace('ё', 'е'))\n",
    "    query_forms = list(set(query_forms))\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    for query_form in query_forms:\n",
    "        query_form_results_df = para_texts_df[para_texts_df['Standardised source sentence'].str.contains(' ' + query_form + ' ', regex = False)]\n",
    "        results_df = results_df.append(query_form_results_df)\n",
    "#     p = morph.parse(query)[0]\n",
    "#     lemmed_query = p.normal_form\n",
    "#     results_df = reduced_df[reduced_df['Lemmed source sentence'].str.contains(' ' + lemmed_query + ' ', regex = False)]\n",
    "    results_df = results_df.drop_duplicates('Source sentence')\n",
    "    print('NUMBER OF CONCORDANCE RESULTS AFTER DELETING DUPLICATES:', len(results_df.index))\n",
    "    concordance_freq = len(results_df.iloc[:])\n",
    "    \n",
    "    open_subtitles_freq = len(results_df[results_df['Source'].str.contains('OpenSubtitles')])\n",
    "    un_freq = len(results_df[results_df['Source'].str.contains('MultiUN')])\n",
    "    qed_freq = len(results_df[results_df['Source'].str.contains('QED')])\n",
    "    wiki_freq = len(results_df[results_df['Source'].str.contains('Wikipedia')])\n",
    "    \n",
    "    anomalies_df = anomalies_df.append({\n",
    "                                        'Term' : query,\n",
    "                                        'Estimated frequency' : estimated_freq,\n",
    "                                        'Actual frequency' : concordance_freq,\n",
    "                                        'Open Subtitles frequency' : open_subtitles_freq,\n",
    "                                        'Multi UN frequency' : un_freq,\n",
    "                                        'QED frequency' : qed_freq,\n",
    "                                        'Wikipedia frequency' : wiki_freq\n",
    "                                       },\n",
    "                                        ignore_index = True)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "anomalies_df['Anomaly score'] = anomalies_df['Estimated frequency'] / anomalies_df['Actual frequency']\n",
    "anomalies_df.sort_values('Anomaly score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for group_name, group_df in para_texts_df.groupby('Source'):\n",
    "    print(group_name)\n",
    "    for index, row in group_df.iloc[0:30].iterrows():\n",
    "        print(row['Standardised source sentence'])\n",
    "        print(row['Standardised target sentence'])\n",
    "        print('\\n')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column for the sentences' genre/register and create a function to return a register tag from a list of registers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_register(source):\n",
    "    register_dict = {\n",
    "        'GlobalVoices' : \n",
    "        'MultiUN_1' : 'political',\n",
    "        'MultiUN_2' : 'political',\n",
    "        'MultiUN_3' : 'political',\n",
    "        'MultiUN_4' : 'political',\n",
    "        'MultiUN_5' : 'political',\n",
    "        'MultiUN_6' : 'political',\n",
    "        'MultiUN_7' : 'political',\n",
    "        'MultiUN_8' : 'political',\n",
    "        'OpenSubtitles_1' : 'informal',\n",
    "        'OpenSubtitles_2' : 'informal',\n",
    "        'OpenSubtitles_4' : 'informal',\n",
    "        'QED' : 'journalism'\n",
    "        'TED2013' : 'journalism',\n",
    "        'Tanzil.tmx' : \n",
    "        'Tatoeba' : 'general'\n",
    "        'Wikipedia' : 'general'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, group_df in para_texts_df.groupby('Source'):\n",
    "    print(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save para_texts_df to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df.to_csv(desktop_dir + '\\\\para_texts_df.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load para_texts_df from csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_texts_df = pd.read_csv(desktop_dir + '\\\\para_texts_df.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary webscraping functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\MdeCL\\\\Google Drive\\\\Work, productivity and interests_\\\\computer science\\\\coding skills (technical)\\\\VS Code project files\\\\NLP-powered Vocab Learning Strategy\\\\Russian Vocab Project\\\\ru_prepositions.txt\", 'r', encoding='utf8') as file:\n",
    "    file_read = file.read()\n",
    "    preps = file_read.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def abbyy_scrape(word):\n",
    "    \"\"\"\"Web-scrape the ABBYY Lingvo Live website\n",
    "    for English definitions of a given Russian word\"\"\"\n",
    "    \n",
    "    url = \"https://www.lingvolive.com/en-us/translate/ru-en/\" + word\n",
    "    \n",
    "    def web_scrape_this_url(url):\n",
    "        \n",
    "        return_dict = {\n",
    "        'PoS' : '',\n",
    "        'Term with accent' : '',\n",
    "        'Case taken' : '',\n",
    "        'Grammatical info' : '',\n",
    "        'Distinguishing gram info' : '',\n",
    "        'English translations' : [],\n",
    "        }\n",
    "        \n",
    "        reroute_link_found = False\n",
    "        eng_def_found = False\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            print('RESPONSE FROM ABBYY LINGVO LIVE DICTIONARY:', response)\n",
    "            #try:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            dict_div = soup.find('div', {'name' : '#dictionary'})\n",
    "            first_def_div = dict_div.find('div', {'class' : '_1mexQ Zf_4w _3bSyz'})\n",
    "            term_div = first_def_div.find('h1', {'class' : '_2bepj _2lIoa _3bSyz sSWiV Zf_4w _3bSyz'})\n",
    "            print('TERM:', term_div.get_text())\n",
    "            \n",
    "            gram_info_div = first_def_div.find('p', {'data-reactid' : '.1a8ou94m96o.1.0.$content.0.1.1.1.1.0.0:$0.2:$1'})\n",
    "            \n",
    "            if first_def_div.find('ol', {'class' : '_2xKEq _1TaPP'}) != None:\n",
    "                defs_div = first_def_div.find('ol', {'class' : '_2xKEq _1TaPP'})\n",
    "            else:\n",
    "                defs_div = first_def_div\n",
    "                \n",
    "            case_taken_dict = {\n",
    "                'к' : ' к',\n",
    "                'с' : ' с',\n",
    "                'из' : ' из',\n",
    "                'от' : ' от',\n",
    "                'к' : ' к',\n",
    "                'у' : ' у',\n",
    "                'в' : ' в',\n",
    "                'на' : ' на',\n",
    "                'за' : ' за',\n",
    "                'от' : ' от',\n",
    "                'к' : ' к',\n",
    "                'у' : ' у',\n",
    "                'что-л.' : ' + acc',\n",
    "                'кого-л./что-л.' : '+ acc',\n",
    "                'кого-л.' : ' + <font color=\"green\">gen</font>',\n",
    "                'чего-л.' : ' + <font color=\"green\">gen</font>',\n",
    "                'кого-л./чего-л.' : ' + <font color=\"green\">gen</font>',\n",
    "                'кому-л.' : ' + <font color=\"purple\">dat</font>',\n",
    "                'чему-л.' : ' + <font color=\"purple\">dat</font>',\n",
    "                'кому-л./чему-л.' : ' + <font color=\"purple\">dat</font>',\n",
    "                'кем-л.' : ' + <font color=\"blue\">instr</font>',\n",
    "                'чем-л.' : ' + <font color=\"blue\">instr</font>',\n",
    "                'кем-л./чем-л.' : ' + <font color=\"blue\">instr</font>',\n",
    "            }\n",
    "            \n",
    "            def_texts = {}\n",
    "            try:\n",
    "                case_found = False\n",
    "                \n",
    "                # Iterate over definition divs\n",
    "                for p_div in defs_div.find_all('p'):\n",
    "                    \n",
    "                    s = p_div.get_text()\n",
    "                    \n",
    "                    # Find the case that the word takes\n",
    "                    if case_found == False:\n",
    "                        case_tag = s[s.find(\"(\")+1:s.find(\")\")]\n",
    "                        case_tag = case_tag.replace(' / ', '/').strip()\n",
    "                        case_units = case_tag.split(';')\n",
    "                        case_unit = case_units[0]\n",
    "                        case_unit_cases = case_unit.split(' ')\n",
    "                        cases_taken = []\n",
    "                        for case in case_unit_cases:\n",
    "                            if case in preps:\n",
    "                                cases_taken.append(' ' + case)\n",
    "                            elif case in case_taken_dict:\n",
    "                                value = case_taken_dict[case]\n",
    "                                cases_taken.append(value)\n",
    "                                case_found = True\n",
    "                        return_dict['Case taken'] = ' '.join(cases_taken)\n",
    "\n",
    "                    # Get filtered definition text\n",
    "                    def_items = []\n",
    "                    for span in p_div.find_all('span', {'class' : '_3zJig'}):\n",
    "                        word = span.get_text()\n",
    "                        if word != 'I':\n",
    "                            def_items.append(word)\n",
    "                    def_text = ' '.join(def_items)\n",
    "                    \n",
    "                    # Add div text and filtered div text to def_texts dictionary\n",
    "                    def_texts[s] = def_text\n",
    "                    \n",
    "                for s, def_text in def_texts.items():\n",
    "                    \n",
    "                    # Remove the long text in brackets that interfers with the later text splitting\n",
    "                    def_text = re.sub(r\" ?\\([^)]+\\|\\|[^)]+\\)\", \"\", def_text).strip()\n",
    "                    # Remove the text in brackets that contains a comma, as it interfers with the later text splitting:\n",
    "                    def_text = re.sub(r\" ?\\([^)]+\\,[^)]+\\)\", \"\", def_text).strip()\n",
    "                    \n",
    "                    final_defs = []\n",
    "\n",
    "                    def_text = def_text.replace('; ', ', ')\n",
    "                    def_texts = def_text.split(', ')\n",
    "                    \n",
    "                    for defin in def_texts:\n",
    "                        \n",
    "                        defins = []\n",
    "                        \n",
    "                        if bool(re.search('[a-zA-Z]', defin)) == True:\n",
    "                            \n",
    "                            eng_def_found = True\n",
    "                            if ' / ' in defin:\n",
    "                                if defin.count(' / ') >= 2:\n",
    "                                    defins += defin.split(' / ')\n",
    "                                else:\n",
    "                                    word_tokens = defin.split(' ')\n",
    "                                    word_tokens = ['', '', ''] + word_tokens + ['', '', '']\n",
    "                                    slash_idx = word_tokens.index('/')\n",
    "                                    var_one = ' '.join(word_tokens[:slash_idx] + word_tokens[slash_idx+2:]).strip()\n",
    "                                    var_two = ' '.join(word_tokens[:slash_idx-2] + word_tokens[slash_idx+1:]).strip()\n",
    "                                    defins.append(var_one)\n",
    "                                    defins.append(var_two)\n",
    "                            else:\n",
    "                                defins.append(defin)\n",
    "                                \n",
    "                            for defin in defins:\n",
    "                                if '(' and ')' in defin:\n",
    "                                    def_text_without_bracket_text = re.sub(r\" ?\\([^)]+\\)\", \"\", defin).strip()\n",
    "                                    def_text_with_bracket_text = defin.replace('(', '').replace(')', '').strip()\n",
    "         \n",
    "                                    if '||' not in def_text_with_bracket_text:\n",
    "                                        final_defs += [def_text_without_bracket_text, def_text_with_bracket_text]\n",
    "                                    else:\n",
    "                                        final_defs += [def_text_without_bracket_text]\n",
    "                                else:\n",
    "                                    defin = defin.replace('(', '').replace(')', '').strip().lower()\n",
    "                                    final_defs += [defin]\n",
    "\n",
    "                    to_dict = ', '.join(final_defs)\n",
    "                    \n",
    "                    if to_dict != '':\n",
    "                        if any(item in to_dict for item in ['Все права защищены.', 'II']) == False:\n",
    "                            to_dict = re.sub(r'[\\s]+', ' ', to_dict)\n",
    "                            to_dict = re.sub(r'[а-яА-Я]+', '', to_dict)\n",
    "                            to_dict = to_dict.replace('smb.', '')\n",
    "                            to_dict = re.sub(r'[\\s]+\\, ', ', ', to_dict)\n",
    "                            to_dict = to_dict.replace('(', '').replace(')', '').strip()\n",
    "\n",
    "                            cleaned_def = re.sub(r\" ?\\([^)]+\\)\", \"\", to_dict)\n",
    "                            cleaned_def = cleaned_def.replace(')', '')\n",
    "                            cleaned_def = cleaned_def.replace('the ', '')\n",
    "                            if cleaned_def.startswith('to '):\n",
    "                                cleaned_def = cleaned_def[2:]\n",
    "                            cleaned_def = cleaned_def.replace(', ', '|').replace('; ', '|').strip()\n",
    "                            cleaned_def_list = cleaned_def.split('|')\n",
    "                            \n",
    "                            [defin.strip() for defin in cleaned_def_list]\n",
    "                            \n",
    "                            cleaned_def_list = [''.join([char for char in defin if char.isalpha() or char.isspace()]).strip() for defin in cleaned_def_list]\n",
    "                            \n",
    "                            cleaned_def_list = list(set(cleaned_def_list))\n",
    "                            \n",
    "                            return_dict['English translations'].append([s, cleaned_def_list])\n",
    "                            \n",
    "                            eng_def_found = True\n",
    "                            \n",
    "                    if p_div.get_text() != '':\n",
    "                        if p_div.find('a') != None:\n",
    "                            reroute_link_found = True\n",
    "                            a_element = p_div.find('a')\n",
    "                            reroute_url = 'https://www.lingvolive.com' + a_element['href']\n",
    "                            break\n",
    "                        else:\n",
    "                            reroute_url = ''\n",
    "                            \n",
    "            except Exception as e:\n",
    "                reroute = False\n",
    "                reroute_url = ''\n",
    "                return return_dict, reroute, reroute_url\n",
    "            \n",
    "            if reroute_link_found == True and eng_def_found == False:\n",
    "                reroute = True\n",
    "            else:\n",
    "                reroute = False\n",
    "            \n",
    "            return return_dict, reroute, reroute_url\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print('-----------ABBYY LINGVO SCRAPE RAISED AN ERROR:')\n",
    "            print(e)\n",
    "            reroute = False\n",
    "            reroute_url = ''\n",
    "            return return_dict, reroute, reroute_url\n",
    "                \n",
    "    return_dict, reroute, reroute_url = web_scrape_this_url(url)\n",
    "    if reroute == True:\n",
    "        print('REROUTING...')\n",
    "        return_dict, reroute, reroute_url = web_scrape_this_url(reroute_url)\n",
    "        if reroute == True:\n",
    "            return_dict, reroute, reroute_url = web_scrape_this_url(reroute_url)\n",
    "    \n",
    "\n",
    "    pos_codes = {\n",
    "        'нареч.' : 'adverb',\n",
    "        'прил.' : 'adjective',\n",
    "        'ж.р.' : 'noun',\n",
    "        'м.р.' : 'noun',\n",
    "        'несовер.' : 'verb',\n",
    "        'предл.' : 'preposition',\n",
    "        'союз' : 'conjunction',\n",
    "        'частица' : 'particle'\n",
    "    }\n",
    "\n",
    "    term_pos = ''\n",
    "\n",
    "    return_dict['PoS'] = term_pos\n",
    "        \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbyy_scrape('чей-то')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def wiktionary_parser_eng_trans_scrape(word, pos):\n",
    "    language = 'russian'\n",
    "    parser = WiktionaryParser()\n",
    "    return_dict = {\n",
    "        'PoS' : '',\n",
    "        'Term with accent' : '',\n",
    "        'Grammatical info' : '',\n",
    "        'Distinguishing gram info' : '',\n",
    "        'English translations' : [],\n",
    "        'Idiomatic phrases' : []\n",
    "    }\n",
    "    try:\n",
    "        if parser.fetch(word, language) != None:\n",
    "            try:\n",
    "                data = parser.fetch(word, language)[0]\n",
    "                #print(data)\n",
    "                try:\n",
    "                    for definition in data['definitions']:\n",
    "                        if definition['partOfSpeech'] == pos:\n",
    "                            return_dict['PoS'] = definition['partOfSpeech']\n",
    "                            term_with_accent = definition['text'][0]\n",
    "                            return_dict['Idiomatic phrases'] = definition['examples']\n",
    "                            term_with_accent = ''\n",
    "                            eng_trans = []\n",
    "                            text_idx = 0\n",
    "                            defs_to_reroute = ['Alternative spelling of', 'superlative degree of', 'short neuter singular of', ]\n",
    "                            if any(item in definition['text'][1] for item in defs_to_reroute):\n",
    "                                alt_spelling = definition['text'][1].split(' of ')[1]\n",
    "                                alt_spelling = alt_spelling.split()[0]\n",
    "                                alt_spelling = alt_spelling.replace('о́', 'о').replace('е́', 'е').replace('и́', 'и').replace('у́', 'у')\n",
    "                                alt_spelling = alt_spelling.replace('ы́','ы').replace('а́','а')\n",
    "                                if parser.fetch(alt_spelling, language) != None:\n",
    "                                    try:\n",
    "                                        data = parser.fetch(alt_spelling, language)[0]\n",
    "                                        try:\n",
    "                                            for definition in data['definitions']: \n",
    "                                                if definition['partOfSpeech'] == pos:\n",
    "                                                    return_dict['PoS'] = definition['partOfSpeech']\n",
    "                                                    term_with_accent = definition['text'][0]\n",
    "                                                    return_dict['Idiomatic phrases'] = definition['examples']\n",
    "                                                    term_with_accent = ''\n",
    "                                                    eng_trans = []\n",
    "                                                    text_idx = 0\n",
    "                                                    while text_idx + 1 <= len(definition['text']):\n",
    "                                                        item = definition['text'][text_idx]\n",
    "                                                        if text_idx == 0:\n",
    "                                                            return_dict['Term with accent'] = item.split(' • ')[0]\n",
    "                                                            return_dict['Grammatical info'] = item.split(' • ')[1]\n",
    "                                                        else:\n",
    "                                                            if 'passive of ' not in item:\n",
    "                                                                eng_trans.append(item)\n",
    "                                                        text_idx += 1\n",
    "                                                    return_dict['English translations'] = eng_trans\n",
    "                                        except Exception:\n",
    "                                            print('Wiktionary Parser returned None.')\n",
    "                                            sys.exit()\n",
    "                                    except Exception:\n",
    "                                        print('Wiktionary Parser returned None.')\n",
    "                                        sys.exit()\n",
    "                                else:\n",
    "                                    print('Wiktionary Parser returned None.')\n",
    "                                    sys.exit()\n",
    "                            else:\n",
    "                                while text_idx + 1 <= len(definition['text']):\n",
    "                                    item = definition['text'][text_idx]\n",
    "                                    if text_idx == 0:\n",
    "                                        return_dict['Term with accent'] = item.split(' • ')[0]\n",
    "                                        return_dict['Grammatical info'] = item.split(' • ')[1]\n",
    "                                    else:\n",
    "                                        if 'passive of ' not in item:\n",
    "                                            eng_trans.append(item)\n",
    "                                    text_idx += 1\n",
    "                                return_dict['English translations'] = eng_trans\n",
    "                except Exception:\n",
    "                    print('Wiktionary Parser returned None.')\n",
    "            except Exception:\n",
    "                print('Wiktionary Parser returned None.')\n",
    "        else:\n",
    "            print('Wiktionary Parser returned None.')\n",
    "    except Exception as e:\n",
    "        print('Wiktionary Parser returned None.')\n",
    "    formatted_eng_trans = []\n",
    "    eng_tran_idx = 0\n",
    "    while eng_tran_idx + 1 <= len(return_dict['English translations']):\n",
    "        formatted_eng_trans.append(return_dict['English translations'][eng_tran_idx])\n",
    "        eng_tran_idx += 1\n",
    "    return_dict['English translations'] = formatted_eng_trans\n",
    "    if 'impf (perfective' in return_dict['Grammatical info']:\n",
    "        return_dict['Distinguishing gram info'] = 'imperfective'\n",
    "    elif 'pf (imperfective'in return_dict['Grammatical info']:\n",
    "        return_dict['Distinguishing gram info'] = 'perfective'\n",
    "    \n",
    "    return(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_parser_eng_trans_scrape('жаль', 'noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coll_eng_tran(coll):\n",
    "    \n",
    "#     translation = translate_client.translate(coll, source_language='ru', target_language='en')\n",
    "#     time.sleep(1)\n",
    "    \n",
    "#     return translation['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cooljugator_scrape(word, query_pos_full):\n",
    "    \n",
    "    conjugs_dict = {}\n",
    "    \n",
    "    pos_dict = {\n",
    "        'adjective' : 'a',\n",
    "        'noun' : 'n',\n",
    "        'verb' : ''\n",
    "    }\n",
    "    \n",
    "    base_url = 'https://cooljugator.com/ru'\n",
    "    if query_pos_full in pos_dict:\n",
    "        url = base_url + pos_dict[query_pos_full] + '/' + word\n",
    "\n",
    "        response = requests.get(url)\n",
    "        print('RESPONSE FROM COOLJUGATOR:', response)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        conjug_div = soup.find('section', {'id' : 'conjugations'})\n",
    "        \n",
    "        try:\n",
    "            for table_div in conjug_div.find_all('div', {'class' : 'conjugation-table collapsable'}):\n",
    "                for cell_div in table_div.find_all('div', {'class' : 'conjugation-cell conjugation-cell-four'}):\n",
    "                    if cell_div.has_attr('id'):\n",
    "                        cell_id = cell_div['id'].replace('_no_accent', '')\n",
    "                        if cell_div.has_attr('data-stressed'):\n",
    "                            conjugs_dict[cell_id] = cell_div['data-stressed']\n",
    "                        elif cell_div.has_attr('data-default'):\n",
    "                            conjugs_dict[cell_id] = cell_div['data-default']\n",
    "\n",
    "                for cell_div in conjug_div.find_all('div', {'class' : 'conjugation-cell conjugation-cell-four leftmost'}):\n",
    "                    if cell_div.has_attr('id'):\n",
    "                        cell_id = cell_div['id'].replace('_no_accent', '')\n",
    "                        if cell_div.has_attr('data-stressed'):\n",
    "                            conjugs_dict[cell_id] = cell_div['data-stressed']\n",
    "                        elif cell_div.has_attr('data-default'):\n",
    "                            conjugs_dict[cell_id] = cell_div['data-default']\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print('COOLJUGATOR SCRAPE RAISED AN ERROR:')\n",
    "            print(e)\n",
    "    \n",
    "    else:\n",
    "        print('COOLJUGATOR_SCRAPE: QUERY NOT A NOUN, VERB OR ADJECTIVE --> NOT SCRAPING.')\n",
    "                \n",
    "    return conjugs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooljugator_scrape('единый', 'adjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eng_trans_and_syns(query, query_pos_full):\n",
    "    wikt_results = wiktionary_parser_eng_trans_scrape(query, query_pos_full)\n",
    "    term_with_accent = wikt_results['Term with accent']\n",
    "    if term_with_accent == '':\n",
    "        term_with_accent = query\n",
    "    term_without_accent = term_with_accent.replace('о́', 'о').replace('ы́', 'ы').replace('у́', 'у').replace('а́', 'а').replace('е́', 'е').replace('и́', 'и').replace('я́', 'я').replace('э́', 'э')\n",
    "    term_with_accent = re.sub(r\"(.)\\1\", '<u>' + r\"\\1\\1\" + '</u>', term_with_accent)\n",
    "    gram_info = wikt_results['Grammatical info']\n",
    "    disting_gram_info = wikt_results['Distinguishing gram info']\n",
    "    \n",
    "    if '\\xa0n\\xa0' in gram_info:\n",
    "        term_gender_colour = 'grey'\n",
    "    elif '\\xa0f\\xa0' in gram_info:\n",
    "        term_gender_colour = 'red'\n",
    "    elif '\\xa0m\\xa0' in gram_info:\n",
    "        term_gender_colour = 'blue'\n",
    "    else:\n",
    "        term_gender_colour = 'black'\n",
    "            \n",
    "    inflected_forms =  []\n",
    "    inflected_forms_dict = cooljugator_scrape(term_without_accent, query_pos_full) \n",
    "    query_parse = morph.parse(query)[0]\n",
    "    if query_pos_full == 'noun':\n",
    "        for key, value in inflected_forms_dict.items():\n",
    "            value = value.replace('о́', 'о').replace('ы́', 'ы').replace('у́', 'у').replace('а́', 'а').replace('е́', 'е').replace('и́', 'и').replace('я́', 'я').replace('э́', 'э')\n",
    "            if key == 'nom_P':\n",
    "                if term_gender_colour == 'blue':\n",
    "                    if (value.endswith('ы') == False) and (term_without_accent.endswith(('ь', 'к', 'г', 'х', 'ш', 'щ', 'ч', 'ж')) == False):\n",
    "                        inflected_forms.append(value[:-2] + '<font size=\\\"+5\\\" color=\\\"blue\\\">' + value[-2:]  + '</font>')\n",
    "                    if (value.endswith('и') == False) and (term_without_accent.endswith(('ь')) == True):\n",
    "                        inflected_forms.append(value[:-2] + '<font size=\\\"+5\\\" color=\\\"blue\\\">' + value[-2:]  + '</font>')\n",
    "                    elif len(value) > len(term_without_accent) + 1:\n",
    "                        inflected_forms.append(value[:-3] + '<font size=\\\"+5\\\" color=\\\"blue\\\">' + value[-3:]  + '</font>')\n",
    "                    else:\n",
    "                        inflected_forms.append(value)\n",
    "                elif (term_gender_colour == 'grey') and (term_without_accent.endswith('е') == False) and (value.endswith('а') == False):\n",
    "                    inflected_forms.append(value[:-2] + '<font size=\\\"+5\\\" color=\\\"blue\\\">' + value[-2:]  + '</font>')\n",
    "                elif (term_gender_colour == 'grey') and (term_without_accent.endswith('е') == True) and (value.endswith('я') == False):\n",
    "                    inflected_forms.append(value[:-2] + '<font size=\\\"+5\\\" color=\\\"blue\\\">' + value[-2:]  + '</font>')\n",
    "                elif ('ё' in value) and ('ё' not in term_without_accent):\n",
    "                    inflected_forms.append(value.replace('ё', '<font size=\\\"+5\\\" color=\\\"blue\\\">ё</font>'))\n",
    "                elif len(value) > len(term_without_accent) + 1:\n",
    "                    inflected_forms.append(value[:-3] + '<font size=\\\"+5\\\" color=\\\"blue\\\">' + value[-3:]  + '</font>')\n",
    "                else:\n",
    "                    inflected_forms.append(value)\n",
    "            elif key == 'loc':\n",
    "                if value.endswith('у'):\n",
    "                    inflected_forms.append(value[:-1] + '<font size=\\\"+5\\\" color=\\\"purple\\\">у</font>')\n",
    "                elif value.endswith('у́'):\n",
    "                    inflected_forms.append(value[:-2] + '<font size=\\\"+5\\\" color=\\\"purple\\\">у́</font>')\n",
    "                else:\n",
    "                    inflected_forms.append(value)\n",
    "            else:\n",
    "                inflected_forms.append(value)\n",
    "                    \n",
    "    elif query_pos_full == 'verb':\n",
    "        for key, value in inflected_forms_dict.items():\n",
    "            second_sing_form = ''\n",
    "            if 'present2' in inflected_forms_dict:\n",
    "                second_sing_form = inflected_forms_dict['present2']\n",
    "            elif 'future2' in inflected_forms_dict:\n",
    "                second_sing_form = inflected_forms_dict['future2']\n",
    "            if second_sing_form != '':\n",
    "                if second_sing_form.endswith(('аешь', 'яешь', 'ешься', 'яешься', 'а́ешь', 'я́ешь', 'а́ешься', 'я́ешься')):\n",
    "                    color = 'orange'\n",
    "                    term_gender_colour = 'orange'\n",
    "                else:\n",
    "                    if second_sing_form.endswith(('ишь', 'ишься', 'и́шь', 'и́шься')):\n",
    "                        color = 'blue'\n",
    "                        term_gender_colour = 'blue'\n",
    "                    elif second_sing_form.endswith(('ешь', 'ешься')):\n",
    "                        color = 'purple'\n",
    "                        term_gender_colour = 'purple'\n",
    "                    elif second_sing_form.endswith(('ёшь', 'ёшься')):\n",
    "                        color = 'firebrick'\n",
    "                        term_gender_colour = 'firebrick'\n",
    "                    else:\n",
    "                        color = 'black'\n",
    "            else:\n",
    "                color = 'black'\n",
    "                        \n",
    "        for key, value in inflected_forms_dict.items():\n",
    "            inflected_forms.append('<font color=\\\"' + color + '\\\">' + value + '</font>')\n",
    "            \n",
    "    elif query_pos_full == 'adjective':\n",
    "        if query_parse.inflect({'COMP'}) != None:\n",
    "            comp_form = query_parse.inflect({'COMP'}).word\n",
    "            if comp_form.endswith('ее') == False:\n",
    "                inflected_forms.append(comp_form[:-2] + '<font size=\\\"+5\\\" color=\\\"orange\\\">' + comp_form[-2:] + '</font>')\n",
    "            else:\n",
    "                inflected_forms.append(comp_form)\n",
    "\n",
    "    abbyy_results = abbyy_scrape(query)\n",
    "    \n",
    "    case_taken = abbyy_results['Case taken']\n",
    "    \n",
    "    cleaned_defs = abbyy_results['English translations']\n",
    "\n",
    "    syn_pos_tags = {\n",
    "        'noun' : '.n.',\n",
    "        'verb' : '.v.',\n",
    "        'adjective' : '.a.',\n",
    "        'adverb' : '',\n",
    "        'preposition' : ''\n",
    "    }\n",
    "    \n",
    "    infl_pos_tags = {\n",
    "        'verb' : 'V',\n",
    "        'adjective' : 'A',\n",
    "        'noun' : 'N',\n",
    "    } \n",
    "    \n",
    "    \n",
    "    defs_for_check = []\n",
    "    for defin in cleaned_defs:\n",
    "        for word in defin[1]:\n",
    "            defs_for_check.append(word)\n",
    "    \n",
    "    # CREATE A DICTIONARY WITH EACH FULL DEF AS A KEY AND ALL THEIR SYNS AND INFLECTED FORMS AS ITS VALUE:\n",
    "    inflected_eng_defs = {}\n",
    "    basic_defs_added = []\n",
    "    syns_added = []\n",
    "    for basic_def in cleaned_defs:\n",
    "        full_def = basic_def[0]\n",
    "        basic_def = basic_def[1]\n",
    "        n_basic_def = []\n",
    "        all_forms = []\n",
    "        # Find the synonyms of each word\n",
    "        for word in basic_def:\n",
    "            word = word.strip()\n",
    "            if word not in basic_defs_added:\n",
    "                basic_defs_added.append(word)\n",
    "                n_basic_def.append(word)\n",
    "                if query_pos_full == 'verb':\n",
    "                    syn_results = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "                elif query_pos_full == 'noun':\n",
    "                    syn_results =  wordnet.synsets(word, pos=wordnet.NOUN)\n",
    "                elif query_pos_full == 'adjective':\n",
    "                    syn_results = wordnet.synsets(word, pos=wordnet.ADJ)\n",
    "                elif query_pos_full == 'adverb':\n",
    "                    syn_results = wordnet.synsets(word, pos=wordnet.ADV)\n",
    "                else:\n",
    "                    syn_results = wordnet.synsets(word)\n",
    "                syn_words = []\n",
    "                for syn in syn_results:\n",
    "                    for lm in syn.lemmas():\n",
    "                        syn_word = lm.name()\n",
    "                        if syn_word not in defs_for_check:\n",
    "                            syn_words.append(syn_word)\n",
    "                syn_words.append(word)\n",
    "                \n",
    "                # FIND THE INFLECTED FORMS OF EACH SYNONYM (SENSITIVE TO PoS) AND ADD TO WORD FORMS LIST:\n",
    "                word_forms = []\n",
    "                for syn_word in syn_words:\n",
    "                    syn_word = syn_word.replace('_', ' ').strip()\n",
    "                    if syn_word not in syns_added:\n",
    "                        syns_added.append(syn_word)\n",
    "                        # CHECK IF THE SYNONYM IS MORE THAN ONE WORD. IF IT IS, ONLY INFLECT THE FIRST WORD:\n",
    "                        syn_word_list = nltk.word_tokenize(syn_word)\n",
    "                        if query_pos_full in infl_pos_tags:\n",
    "                            infl_pos_tag = infl_pos_tags[query_pos_full]\n",
    "                            if syn_word_list != []:\n",
    "                                if len(syn_word_list) == 1:\n",
    "                                    if getAllInflections(syn_word_list[0], pos_type=infl_pos_tag) != {}:\n",
    "                                        [word_forms.append(value[0]) for key, value in getAllInflections(syn_word_list[0], pos_type=infl_pos_tag).items()]\n",
    "                                    else:\n",
    "                                        word_forms.append(syn_word)\n",
    "                                else:\n",
    "                                    if getAllInflections(syn_word_list[0], pos_type=infl_pos_tag) != {}:\n",
    "                                        word_forms = [infl_form[0]+' '+' '.join(syn_word_list[1:]) for infl_form in list(getAllInflections(syn_word_list[0], pos_type=infl_pos_tag).values())]\n",
    "                                    else:\n",
    "                                        word_forms.append(syn_word)\n",
    "                        else:\n",
    "                            if syn_word_list != []:\n",
    "                                if len(syn_word_list) == 1:\n",
    "                                    if getAllInflections(syn_word_list[0]) != {}:\n",
    "                                        [word_forms.append(value[0]) for key, value in getAllInflections(syn_word_list[0]).items()]\n",
    "                                    else:\n",
    "                                        word_forms.append(syn_word)\n",
    "                                else:\n",
    "                                    if getAllInflections(syn_word_list[0]) != {}:\n",
    "                                        word_forms = [infl_form[0]+' '+' '.join(syn_word_list[1:]) for infl_form in list(getAllInflections(syn_word_list[0]).values())]\n",
    "                                    else:\n",
    "                                        word_forms.append(syn_word)\n",
    "                    all_forms += word_forms\n",
    "                    \n",
    "        all_forms = list(set(all_forms))\n",
    "        inflected_eng_defs[full_def] = all_forms\n",
    "        \n",
    "    print('TERM\\'S INFLECTED FORMS:', ', '.join(inflected_forms))\n",
    "    \n",
    "    return {'inflected_eng_defs' : inflected_eng_defs,\n",
    "            'term_with_accent' : term_with_accent,\n",
    "            'case_taken' : case_taken,\n",
    "            'term_gender_colour' : term_gender_colour,\n",
    "            'disting_gram_info' : disting_gram_info,\n",
    "            'conjugation_declension_info' : inflected_forms}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_trans_and_syns('единый', 'adjective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find word's monolingual dictionary definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrevs_dict = {}\n",
    "\n",
    "with open(proj_dir + '\\\\wiktionary_abbrevs_trans.txt', 'r') as file:\n",
    "    file_read = file.read()\n",
    "    for line in file_read.split('\\n'):\n",
    "        \n",
    "        abbrev = line.split('£')[0]\n",
    "        expl = line.split('£')[1]\n",
    "        \n",
    "        \n",
    "        abbrevs_dict[abbrev] = expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monoling_wikt_scrape(word):\n",
    "    url = \"https://ru.wiktionary.org/w/index.php?title=\" + word + '&printable=yes'\n",
    "    response = requests.get(url)\n",
    "    wikt_definitions = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find('ol') != None:\n",
    "            entries = soup.find('ol')\n",
    "            if entries.findAll('li') != None:\n",
    "                number_of_defs = len(entries.findAll('li'))\n",
    "                index = 0\n",
    "                while index + 1 <= number_of_defs:\n",
    "    \n",
    "                    # definition:\n",
    "                    entry = entries.find_all('li')[index]\n",
    "                    entry_text = entry.get_text()\n",
    "                    \n",
    "                    if entry_text != '':\n",
    "                    \n",
    "                        definition_text = entry_text.split('◆')[0]\n",
    "                        definition_text_extra = ''\n",
    "\n",
    "                        #clean up definition text before translating:\n",
    "                        definition_text = definition_text.replace(' гл.', ' глаголя ')\n",
    "                        \n",
    "                        if 'по значению глаголя ' in definition_text:\n",
    "                            definition_text_extra = definition_text.split('по значению глаголя ')[1]\n",
    "                        \n",
    "                        if 'соотносящийся по значению с существительным ' in definition_text:\n",
    "                            definition_text_extra = definition_text.split('соотносящийся по значению с существительным ')[1]\n",
    "                        \n",
    "                        definition_text = re.sub(\"\\[.*?\\]\", \"\", definition_text) \n",
    "                        definition_text = definition_text.replace('по значению глаголя', 'of verb: ')\n",
    "                        definition_text = definition_text.replace('свойство по значению прилагательного', 'quality of being ')\n",
    "                        definition_text = definition_text.replace(' по значению с существительным', ' to noun: ')\n",
    "                        \n",
    "                        # register tag:\n",
    "                        register_tag = ''\n",
    "                        for key, value in abbrevs_dict.items():\n",
    "                            if key in definition_text:\n",
    "                                definition_text = definition_text.replace(key, '')\n",
    "                                register_tag += '<i>' + value + '</i>'\n",
    "                                \n",
    "                        # example sentence:\n",
    "                        if entry.find('span', {'class' : 'example-absent'}) == None:\n",
    "                            if entry.find('span', {'class' : 'example-block'}) != None:\n",
    "                                ex_sentence_1 = entry.find('span', {'class' : 'example-block'})\n",
    "                                \n",
    "                                ex_sentence_one_text = ex_sentence_1.get_text()\n",
    "                            \n",
    "                                if entry.find('span', {'class' : 'example-details'}) != None:\n",
    "                                    text_to_remove = entry.find('span', {'class' : 'example-details'}).get_text()\n",
    "                                    ex_sentence_one_text = ex_sentence_one_text.replace(text_to_remove, '')\n",
    "                                \n",
    "                                ex_sent_with_blank = ex_sentence_one_text\n",
    "                                ex_sent_full = ex_sentence_one_text\n",
    "                                \n",
    "                                query_parse = morph.parse(word)[0]\n",
    "\n",
    "                                forms = []\n",
    "\n",
    "                                for word_form in query_parse.lexeme:\n",
    "                                    query = word_form[0]\n",
    "                                    forms.append(query)\n",
    "\n",
    "                                forms = list(set(forms))\n",
    "                                \n",
    "                                for form in forms:\n",
    "                                    capitalised_form = form.replace(form[0], form[0].upper())\n",
    "                                    \n",
    "                                    ex_sent_with_blank = re.sub('\\s' + form + '[\\s.]', ' ____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub('\\s' + form + '[\\s,]', ' ____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub('\\s' + form + '[\\s!]', ' ____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub('\\s' + form + '[\\s?]', ' ____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub(capitalised_form + '[\\s.]', '____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub(capitalised_form + '[\\s,]', '____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub(capitalised_form + '[\\s!]', '____ ', ex_sent_with_blank)\n",
    "                                    ex_sent_with_blank = re.sub(capitalised_form + '[\\s?]', '____ ', ex_sent_with_blank)\n",
    "                                    \n",
    "                                    #ex_sent_with_blank = ex_sent_with_blank.replace(form, '_____')\n",
    "                                    #ex_sent_with_blank = ex_sent_with_blank.replace(capitalised_form, '_____')\n",
    "                                    ex_sent_full = ex_sent_full.replace(form, '<b>' + form + '</b>')\n",
    "                                    ex_sent_full = ex_sent_full.replace(capitalised_form, '<b>' + capitalised_form + '</b>')\n",
    "                                \n",
    "                            else:\n",
    "                                ex_sent_with_blank = ''\n",
    "                                ex_sent_full = ''\n",
    "                        else:\n",
    "                            ex_sent_with_blank = ''\n",
    "                            ex_sent_full = ''\n",
    "                        \n",
    "                        added_to_register_tag = ''\n",
    "                        #move eg. 'about water' from definition_text to register_tag:\n",
    "                        definition_text = definition_text.replace(' - ', ' — ')\n",
    "                        definition_text = definition_text.replace('· ', ' — ')\n",
    "                        if ' — ' in definition_text:\n",
    "                            split_def = re.split(' — ', definition_text)\n",
    "                            added_to_register_tag += split_def[0]\n",
    "                            definition_text = split_def[1]\n",
    "                        #add text in brackets in definition_text to register_tag:\n",
    "                        if '(' and ')' in definition_text:\n",
    "                            text_in_brackets = definition_text[definition_text.find(\"(\")+1:definition_text.find(\")\")]\n",
    "                            added_to_register_tag += text_in_brackets\n",
    "                        if added_to_register_tag != '':\n",
    "                            register_tag += ', ' + added_to_register_tag\n",
    "                            \n",
    "                        definition_eng = definition_text\n",
    "                        \n",
    "                        definition_eng = definition_eng.replace(' it', ' ')\n",
    "                        definition_eng = definition_eng.replace('the ', ' ')\n",
    "                        definition_eng = definition_eng.replace(' a ', ' ')\n",
    "                        \n",
    "                        if definition_text_extra != '':\n",
    "                            definition_text_extra = ' (' + definition_text_extra + ')'\n",
    "\n",
    "                        definition_package = {'register_tag_ru' : register_tag,\n",
    "                                              'definition_ru' : definition_eng + definition_text_extra,\n",
    "                                              'ex_sentence_blank' : ex_sent_with_blank,\n",
    "                                                'ex_sentence_full' : ex_sent_full\n",
    "                                             }\n",
    "\n",
    "                        wikt_definitions.append(definition_package)\n",
    "                        \n",
    "                    index += 1\n",
    "                \n",
    "                \n",
    "                #translate all defs in one API request:\n",
    "                reg_tags_to_translate = []\n",
    "                defs_to_translate = []\n",
    "                for definition in wikt_definitions:\n",
    "                    if definition['register_tag_ru']:\n",
    "                        reg_tags_to_translate.append(definition['register_tag_ru'])\n",
    "                    else:\n",
    "                        reg_tags_to_translate.append('_')\n",
    "                    if definition['definition_ru']:\n",
    "                        defs_to_translate.append(definition['definition_ru'])\n",
    "                    else:\n",
    "                        defs_to_translate.append('_')\n",
    "                    \n",
    "                if reg_tags_to_translate == ['']:\n",
    "                    reg_tags_to_translate = ['_']\n",
    "                \n",
    "                if defs_to_translate == ['']:\n",
    "                    defs_to_translate = ['_']\n",
    "                \n",
    "                text_for_google_translate = ' | '.join(reg_tags_to_translate) + ' || ' + ' | '.join(defs_to_translate)\n",
    "                text_translated = coll_eng_tran(text_for_google_translate)\n",
    "                \n",
    "                reg_tags_translated_text = text_translated.split(' || ')[0]\n",
    "                reg_tags_translated = reg_tags_translated_text.split(' | ')\n",
    "                \n",
    "                defs_translated_text = text_translated.split(' || ')[1]\n",
    "                defs_translated = defs_translated_text.split(' | ')\n",
    "                \n",
    "                idx = 0\n",
    "                while idx + 1 <= len(wikt_definitions):\n",
    "                    definition = wikt_definitions[idx]\n",
    "                    definition['definition_eng'] = defs_translated[idx]\n",
    "                    definition['register_tag_eng'] = reg_tags_translated[idx]\n",
    "                    \n",
    "                    idx += 1\n",
    "                    \n",
    "                \n",
    "                return(wikt_definitions)\n",
    "            else:\n",
    "                return('Error')\n",
    "        else:\n",
    "            return('Error')\n",
    "    except Exception as e:\n",
    "        return('Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monoling_wikt_scrape('панель')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\MdeCL\\\\Google Drive\\\\Work, productivity and interests_\\\\computer science\\\\coding skills (technical)\\\\VS Code project files\\\\NLP-powered Vocab Learning Strategy\\\\Russian Vocab Project\\\\ru_prepositions.txt\", 'r', encoding='utf8') as file:\n",
    "    file_read = file.read()\n",
    "    preps = file_read.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define Russian stop words\n",
    "stop_words_lems = []\n",
    "for index, row in rnc_freq_list.iloc[1:1000].iterrows():\n",
    "    poss_to_keep = ['pronoun', 'verb', 'conjunction', 'adverb']\n",
    "    if row['PoS tag'] in poss_to_keep:\n",
    "        if index <= 50 or row['PoS tag'] in ['pronoun', 'conjunction']:\n",
    "            stop_words_lems.append(row['Word'])\n",
    "\n",
    "ru_stop_words = stop_words_lems    \n",
    "\n",
    "particles = ['не']\n",
    "conjs = ['и', 'но', 'а', 'или', ]\n",
    "prons = ['он', 'который']\n",
    "\n",
    "ru_stop_words.remove('сказать')\n",
    "ru_stop_words.remove('всякий')\n",
    "ru_stop_words.remove('некий')\n",
    "ru_stop_words.remove('данный')\n",
    "\n",
    "ru_stop_words += ['нет', 'да', 'более', 'также', 'это', 'все', 'нее', 'очень'] + particles + conjs + prons\n",
    "\n",
    "ru_stop_words = list(set(ru_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import idiom_dict_df from csv file and convert back to Python dict format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idiom_dict_df = pd.read_csv('C:\\\\Users\\\\MdeCL\\\\Google Drive\\\\Work, productivity and interests_\\\\computer science\\\\coding skills (technical)\\\\VS Code project files\\\\NLP-powered Vocab Learning Strategy\\\\Russian Vocab Project\\\\idiom_dict_df.csv')\n",
    "\n",
    "idiom_dict_for_check = {}\n",
    "for index, row in idiom_dict_df.iterrows():\n",
    "    idiom_dict_for_check[row['Lemmed coll']] = {'Original ru phrase' : row['Original ru phrase'],\n",
    "                                      'Eng tran' : row['Eng tran']        \n",
    "                                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idiom_dict_for_check[' еще бы ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coll_type_filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def coll_type_filter(lemmed_coll, pos_tag_list, case_ahead):\n",
    "    \n",
    "    \n",
    "    punct = ['=', '.', ':', '!', '?', ';', '-', '—', '\\\"', '\\`\\`', '«', '»', '(', ')', ]\n",
    "    ignored_words = ru_stop_words\n",
    "    \n",
    "    coll_as_idiom_dict_key = ' ' + ' '.join(lemmed_coll) + ' '\n",
    "    if (coll_as_idiom_dict_key in idiom_dict_for_check) or (coll_as_idiom_dict_key.replace('ё', 'е') in idiom_dict_for_check):\n",
    "        coll_type = 'lex'\n",
    "    else:\n",
    "        if any([item in punct for item in lemmed_coll]):\n",
    "            coll_type = 'punct'\n",
    "        elif any([item.isdigit() for item in lemmed_coll]):\n",
    "            coll_type = 'ignore'\n",
    "        else:\n",
    "            if (str(lemmed_coll[0]) == ',') or (str(lemmed_coll[-1]) == ','):\n",
    "                coll_type = 'punct'\n",
    "            else:\n",
    "                if len(pos_tag_list) == 1:\n",
    "                    coll_type = 'ignore'\n",
    "                \n",
    "                elif len(pos_tag_list) == 2:\n",
    "                    \n",
    "                    if pos_tag_list[0] == 'PREP' and pos_tag_list[1] == 'NOUN':\n",
    "                        coll_type = 'gram without case ahead'\n",
    "                    elif pos_tag_list[0] == 'NOUN' and pos_tag_list[1] == 'PREP' and case_ahead != 'No case ahead':\n",
    "                        coll_type = 'gram with case ahead'\n",
    "                    elif pos_tag_list[0] in ['INFN', 'PRTF', 'VERB'] and pos_tag_list[1] == 'PREP' and case_ahead != 'No case ahead':\n",
    "                        coll_type = 'gram with case ahead'\n",
    "                    elif pos_tag_list[0] == 'ADJF' and pos_tag_list[1] == 'PREP' and case_ahead != 'No case ahead':\n",
    "                        coll_type = 'gram with case ahead'\n",
    "                    elif pos_tag_list[0] == 'ADVB' and pos_tag_list[1] == 'PREP' and case_ahead != 'No case ahead':\n",
    "                        coll_type = 'gram with case ahead'\n",
    "                    \n",
    "                    elif pos_tag_list[0] == 'ADVB' and pos_tag_list[1] == 'ADJF':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'ADVB' and pos_tag_list[1]  in ['INFN', 'PRTF', 'VERB']:\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'ADJF' and pos_tag_list[1] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'NOUN' and pos_tag_list[1] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0]  in ['INFN', 'PRTF', 'VERB'] and pos_tag_list[1] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'NOUN' and pos_tag_list[1]  in ['INFN', 'PRTF', 'VERB']:\n",
    "                        coll_type = 'lex'\n",
    "\n",
    "                    else:\n",
    "                        coll_type = 'ignore'\n",
    "                \n",
    "                elif len(pos_tag_list) == 3:\n",
    "                    \n",
    "                    if pos_tag_list[0] == 'PREP' and pos_tag_list[1] == 'NOUN' and pos_tag_list[2] == 'PREP' and case_ahead != 'No case ahead':\n",
    "                        coll_type = 'gram with case ahead'\n",
    "                        \n",
    "                    elif pos_tag_list[0] == 'NOUN' and pos_tag_list[2] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'NOUN' and pos_tag_list[1] != 'CONJ' and pos_tag_list[2] in ['INFN', 'PRTF', 'VERB']:\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] in ['INFN', 'PRTF', 'VERB'] and pos_tag_list[1] != 'CONJ' and pos_tag_list[2] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'ADJF' and pos_tag_list[1] == 'NOUN' and pos_tag_list[2] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'PREP' and pos_tag_list[1] == 'ADJF' and pos_tag_list[2] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'PREP' and pos_tag_list[1] == 'NOUN' and pos_tag_list[2] == 'NOUN':\n",
    "                        coll_type = 'lex'\n",
    "                    elif pos_tag_list[0] == 'ADJF' and pos_tag_list[1] == 'CONJ' and pos_tag_list[2] == 'ADJF':\n",
    "                        coll_type = 'lex'\n",
    "                    \n",
    "                    else:\n",
    "                        coll_type = 'ignore'\n",
    "                        \n",
    "                elif len(pos_tag_list) >= 4:\n",
    "                    \n",
    "                    if pos_tag_list[-1] != 'ADJF':\n",
    "                        coll_type = 'lex'\n",
    "                    else:\n",
    "                        coll_type = 'ignore'\n",
    "                                \n",
    "        stop_word_bool = False\n",
    "        if coll_type == 'lex':\n",
    "            if str(lemmed_coll[0]) not in ignored_words and str(lemmed_coll[-1]) not in ignored_words:\n",
    "                coll_type = 'lex'\n",
    "            else:\n",
    "                coll_type = 'ignore'\n",
    "    \n",
    "    if len(lemmed_coll) == 1:\n",
    "        coll_type = 'ignore'\n",
    "        \n",
    "    return coll_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_type_filter(('хотя', 'бы'), ['CONJ', 'PART'], 'No case ahead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = rnc_freq_list['Estimated frequency'].sum()\n",
    "word_freq_dict = {}\n",
    "for index, row in rnc_freq_list.iterrows():\n",
    "    word_freq_dict[row['Word']] = row['Estimated frequency']\n",
    "    \n",
    "def coll_score(n_grams_df_row):\n",
    "    lemmed_coll = n_grams_df_row['Lemmed collocation']\n",
    "    raw_freq = n_grams_df_row['Raw frequency']\n",
    "    freq = n_grams_df_row['Frequency']\n",
    "    \n",
    "    combined_freq = 1\n",
    "    for item in lemmed_coll:\n",
    "        combined_freq = combined_freq * word_freq_dict[item]\n",
    "    \n",
    "    mi_score = math.log2(freq * corpus_size / combined_freq)\n",
    "    \n",
    "    return mi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Lemmed collocation': [('как', 'бы', 'то', 'ни', 'быть')],\n",
    "                       'Raw frequency' : 110,\n",
    "                       'Frequency' : 2000})\n",
    "\n",
    "coll_score(test_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocational analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_coll(coll):\n",
    "    lem_coll = []\n",
    "    for word in coll:\n",
    "        p = morph.parse(word)[0]\n",
    "        lem_coll.append(p.normal_form)\n",
    "    return lem_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_coll(['для', ',', 'времен'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_lem_coll(coll):\n",
    "    lem_coll = []\n",
    "    lem = True\n",
    "    for word in coll:\n",
    "        p = morph.parse(word)[0]\n",
    "        pos = p.tag.POS\n",
    "        if lem == True:\n",
    "            lem_coll.append(p.normal_form)\n",
    "        else:\n",
    "            lem_coll.append(word)\n",
    "        if pos == 'PREP':\n",
    "            lem = False\n",
    "        elif pos in ['INFN', 'VERB', 'NOUN']:\n",
    "            lem = True\n",
    "        \n",
    "    return lem_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Из тумана вышла лагерная ограда – ряды проволоки , натянутые между железобетонными столбами'\n",
    "coll = sent.split(' ')\n",
    "complex_lem_coll(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_coll(['для', ',', 'времен'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def colls_from_sents_df_row(row, **kwargs):\n",
    "    \n",
    "    for arg in kwargs:\n",
    "        query_forms = kwargs[arg] \n",
    "    \n",
    "    source = row['Source']\n",
    "    src_sent = row['Standardised source sentence']\n",
    "    sent_words_raw = row['Standardised source sentence'].replace('  ', ' ').split(' ')\n",
    "    lemmed_sent_words_raw = row['Lemmed source sentence'].replace('  ', ' ').split(' ')\n",
    "#     lemmed_sent_words_raw = row['Coll lemmed source sentence'].replace('  ', ' ').split(' ')\n",
    "    pos_tag_list_raw = row['Source sentence PoS tags'].split(' ')\n",
    "    case_list_raw = row['Source sentence cases'].split(' ')\n",
    "    target_sent = row['Standardised target sentence']\n",
    "    \n",
    "    sent_words = []\n",
    "    lemmed_sent_words = []\n",
    "    pos_tag_list = []\n",
    "    case_list = []\n",
    "    token_idx = 0\n",
    "    for token in lemmed_sent_words_raw:\n",
    "        if token != '':\n",
    "            lemmed_sent_words.append(token)\n",
    "            sent_words.append(sent_words_raw[token_idx])\n",
    "            pos_tag_list.append(pos_tag_list_raw[token_idx])\n",
    "            if case_list_raw[token_idx] == 'nomn':\n",
    "                case_list.append('accs')\n",
    "            else:\n",
    "                case_list.append(case_list_raw[token_idx])\n",
    "        token_idx += 1\n",
    "\n",
    "    for query in query_forms:\n",
    "        try:\n",
    "            q_idx = lemmed_sent_words.index(query)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    start_b0 = []\n",
    "    start_b1 = []\n",
    "    start_b2 = []\n",
    "    start_b3 = []\n",
    "    start_b4 = []\n",
    "    \n",
    "    words_behind = 0\n",
    "    while words_behind <= 3 and words_behind <= q_idx:\n",
    "        words_ahead = 3\n",
    "        while words_ahead >= 1:\n",
    "            if q_idx + words_ahead <= len(sent_words):\n",
    "                n_gram = sent_words[q_idx - words_behind:q_idx + words_ahead]\n",
    "                n_gram = [word.lower() for word in n_gram]\n",
    "                lemmed_n_gram = lemmed_sent_words[q_idx - words_behind:q_idx + words_ahead]\n",
    "                n_gram_pos_list = pos_tag_list[q_idx - words_behind:q_idx + words_ahead]\n",
    "                n_gram_case_list = case_list[q_idx - words_behind:q_idx + words_ahead]\n",
    "                \n",
    "                if q_idx + words_ahead + 1 < len(sent_words):\n",
    "                    if n_gram_pos_list[-1] == 'VERB' or 'INFN' or 'PREP':\n",
    "                        case_of_next_token = str(case_list[q_idx + words_ahead + 1])\n",
    "                        if case_of_next_token != 'None':\n",
    "                            case_ahead = case_of_next_token\n",
    "                        else:\n",
    "                            case_ahead = 'No case ahead'\n",
    "                    else:\n",
    "                        case_ahead = 'No case ahead'\n",
    "                else:\n",
    "                    case_ahead = 'No case ahead'\n",
    "                if case_ahead == 'None':\n",
    "                    print('COLLS FROM SENTS DF ROW LABELLED CASE AHEAD AS NONE')\n",
    "                    case_ahead = 'No case ahead'\n",
    "\n",
    "                # PASS N-GRAM THROUGH COLL TYPE FILTER TO FIND COLL TYPE:\n",
    "                coll_type = coll_type_filter(lemmed_n_gram, n_gram_pos_list, case_ahead)\n",
    "                \n",
    "                src_sent_term_bold_list = []\n",
    "                src_sent_coll_bold_list = []\n",
    "                idx = 0\n",
    "                for token in sent_words_raw:\n",
    "                    if idx == q_idx:\n",
    "                        src_sent_term_bold_list.append('<b><i>' + token + '</i></b>')\n",
    "                        src_sent_coll_bold_list.append('<b><i>' + token + '</i></b>')\n",
    "                    elif idx in [i for i in range(q_idx - words_behind, q_idx + words_ahead) if i != q_idx]:\n",
    "                        src_sent_term_bold_list.append(token)\n",
    "                        src_sent_coll_bold_list.append('<b>' + token + '</b>')\n",
    "                    else:\n",
    "                        src_sent_term_bold_list.append(token)\n",
    "                        src_sent_coll_bold_list.append(token)\n",
    "                    idx += 1\n",
    "                src_sent_term_bold = ' '.join(src_sent_term_bold_list)\n",
    "                src_sent_coll_bold = ' '.join(src_sent_coll_bold_list)\n",
    "                \n",
    "                meaning_dependent_info = []\n",
    "                count_case = False\n",
    "                for token in n_gram:\n",
    "                    if count_case == True:\n",
    "                        meaning_dependent_info.append(n_gram_pos_list[n_gram.index(token)])\n",
    "                    else:\n",
    "                        meaning_dependent_info.append('')\n",
    "                    if n_gram_pos_list[n_gram.index(token)] == 'PREP':\n",
    "                        count_case = True\n",
    "                        \n",
    "                n_gram_package = {}\n",
    "                n_gram_package['Source'] = source\n",
    "                n_gram_package['Source sentence with term bold'] = src_sent_term_bold\n",
    "                n_gram_package['Source sentence with coll bold'] = src_sent_coll_bold\n",
    "                n_gram_package['Source'] = source\n",
    "                n_gram_package['Collocation type'] = coll_type\n",
    "                n_gram_package['Original N-gram'] = n_gram\n",
    "                n_gram_package['Lemmed N-gram'] = lemmed_n_gram\n",
    "                n_gram_package['Case ahead'] = case_ahead\n",
    "                n_gram_package['N-gram PoS tags'] = n_gram_pos_list\n",
    "                n_gram_package['N-gram cases'] = n_gram_case_list\n",
    "                n_gram_package['N-gram meaning dependent info'] = meaning_dependent_info\n",
    "                n_gram_package['N-gram term position'] = q_idx\n",
    "                n_gram_package['N-gram collocate positions'] = [i for i in range(q_idx - words_behind, q_idx + words_ahead)  if i != q_idx]\n",
    "                n_gram_package['Edited N-grams'] = []\n",
    "\n",
    "                #COUNT COLLOCATIONS AT A DISTANCE (i.e. colls separated by a word).\n",
    "                for word in n_gram[1:-1]:\n",
    "                    word_idx = n_gram.index(word)\n",
    "                    if sent_words.index(word) != q_idx:\n",
    "                        word_before_pos = pos_tag_list[sent_words.index(word)-1] \n",
    "                        word_pos = pos_tag_list[sent_words.index(word)] \n",
    "                        word_ahead_pos = pos_tag_list[sent_words.index(word)+1] \n",
    "                        removable_pos_tags = ['ADVB', 'ADJF']\n",
    "                        if (word_pos in removable_pos_tags) or (word_before_pos in ['INFN', 'VERB'] and word_pos in ['NOUN', 'NPRO'] and word_ahead_pos == 'PREP'):\n",
    "\n",
    "                            mut_n_gram = n_gram[:]\n",
    "                            del mut_n_gram[word_idx]\n",
    "                            edited_n_gram = mut_n_gram\n",
    "                            edited_n_gram = [word.lower() for word in edited_n_gram]\n",
    "\n",
    "                            mut_lemmed_n_gram = lemmed_n_gram[:]\n",
    "                            del mut_lemmed_n_gram[word_idx]\n",
    "                            lemmed_edited_n_gram = [word.lower() for word in mut_lemmed_n_gram]\n",
    "\n",
    "                            mut_pos_list = n_gram_pos_list[:]\n",
    "                            del mut_pos_list[word_idx]\n",
    "                            edited_pos_tags = mut_pos_list\n",
    "\n",
    "                            mut_case_list = n_gram_case_list[:]\n",
    "                            del mut_case_list[word_idx]\n",
    "                            edited_case_list = mut_case_list\n",
    "                            \n",
    "                            # PASS N-GRAM THROUGH COLL TYPE FILTER TO FIND COLL TYPE:\n",
    "                            edited_coll_type = coll_type_filter(lemmed_edited_n_gram, edited_pos_tags, case_ahead)\n",
    "\n",
    "                            src_sent_term_bold_list = []\n",
    "                            src_sent_coll_bold_list = []\n",
    "                            idx = 0\n",
    "                            for token in sent_words_raw:\n",
    "                                if idx == q_idx:\n",
    "                                    src_sent_term_bold_list.append('<b><i>' + token + '</i></b>')\n",
    "                                    src_sent_coll_bold_list.append('<b><i>' + token + '</i></b>')\n",
    "                                elif idx in [i for i in range(q_idx - words_behind, q_idx + words_ahead)  if i != q_idx and i != sent_words.index(word)]:\n",
    "                                    src_sent_term_bold_list.append(token)\n",
    "                                    src_sent_coll_bold_list.append('<b>' + token + '</b>')\n",
    "                                else:\n",
    "                                    src_sent_term_bold_list.append(token)\n",
    "                                    src_sent_coll_bold_list.append(token)\n",
    "                                idx += 1\n",
    "                            src_sent_term_bold = ' '.join(src_sent_term_bold_list)\n",
    "                            src_sent_coll_bold = ' '.join(src_sent_coll_bold_list)\n",
    "                            \n",
    "                            meaning_dependent_info = []\n",
    "                            count_case = False\n",
    "                            for token in n_gram:\n",
    "                                if count_case == True:\n",
    "                                    meaning_dependent_info.append(n_gram_pos_list[n_gram.index(token)])\n",
    "                                else:\n",
    "                                    meaning_dependent_info.append('')\n",
    "                                if n_gram_pos_list[n_gram.index(token)] == 'PREP':\n",
    "                                    count_case = True\n",
    "                            \n",
    "                            n_gram_package['Edited N-grams'].append({'Source' : source,\n",
    "                                                                     'N-gram collocate positions' : [i for i in range(q_idx - words_behind, q_idx + words_ahead) if i != q_idx and i != sent_words.index(word)]\n",
    "                                                                    })\n",
    "                if words_behind == 0:\n",
    "                    if n_gram_package not in start_b0:\n",
    "                        start_b0.append(n_gram_package)\n",
    "                elif words_behind == 1:\n",
    "                    if n_gram_package not in start_b1:\n",
    "                        start_b1.append(n_gram_package)\n",
    "                elif words_behind == 2:\n",
    "                    if n_gram_package not in start_b2:\n",
    "                        start_b2.append(n_gram_package)\n",
    "                elif words_behind == 3:\n",
    "                    if n_gram_package not in start_b3:\n",
    "                        start_b3.append(n_gram_package)\n",
    "                elif words_behind == 4:\n",
    "                    if n_gram_package not in start_b4:\n",
    "                        start_b4.append(n_gram_package)\n",
    "\n",
    "            words_ahead = words_ahead - 1\n",
    "        words_behind += 1\n",
    "\n",
    "    sent_package = {'Source' : source,\n",
    "                    'Source sentence' : src_sent,\n",
    "                    'Target sentence' : target_sent,\n",
    "                    'PoS tag list' : pos_tag_list,\n",
    "                    'Case list' : case_list,\n",
    "                    'N-grams' : {'start_b0' : start_b0,\n",
    "                                  'start_b1' : start_b1,\n",
    "                                  'start_b2' : start_b2,\n",
    "                                  'start_b3' : start_b3,\n",
    "                                  'start_b4' : start_b4\n",
    "                                 }\n",
    "                    }\n",
    "\n",
    "    return sent_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "idx = 1\n",
    "while idx <= 1:\n",
    "    pp.pprint(colls_from_sents_df_row(results_df.iloc[6], query_forms = query_forms))\n",
    "    idx += 1\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     157,
     182
    ]
   },
   "outputs": [],
   "source": [
    "def count_colls(sent_packages, query_forms, raw_min_coll_freq, est_min_coll_freq, est_eng_def_freq, raw_eng_def_freq, est_term_freq, raw_term_freq):\n",
    "\n",
    "    n_grams_df = pd.DataFrame()\n",
    "    colls_added = []\n",
    "    \n",
    "    for package in sent_packages:\n",
    "        src_sent = package['Source sentence']\n",
    "        target_sent = package['Target sentence']\n",
    "        pos_tag_list = package['PoS tag list']\n",
    "        case_list = package['Case list']\n",
    "        n_grams_package = package['N-grams']\n",
    "\n",
    "        for group, package in n_grams_package.items():\n",
    "\n",
    "            coll_matched = False\n",
    "            if package != []:\n",
    "                for n_gram in package:\n",
    "\n",
    "                        src = n_gram['Source']\n",
    "                        src_sent_term_bold = n_gram['Source sentence with term bold']\n",
    "                        src_sent_coll_bold = n_gram['Source sentence with coll bold']\n",
    "                        coll_type = n_gram['Collocation type']\n",
    "                        lemmed_coll_only = n_gram['Lemmed N-gram']\n",
    "                        original_coll_only = n_gram['Original N-gram']\n",
    "                        case_ahead = n_gram['Case ahead']\n",
    "                        coll_pos_tags = tuple(n_gram['N-gram PoS tags'])\n",
    "                        coll_cases = tuple(n_gram['N-gram cases'])\n",
    "                        n_gram_idx = len(original_coll_only)\n",
    "\n",
    "                        if coll_type == 'gram with case ahead':\n",
    "                            lemmed_coll = tuple(lemmed_coll_only + [case_ahead])\n",
    "                            original_coll = tuple(original_coll_only + [case_ahead])\n",
    "                        else:\n",
    "                            lemmed_coll = tuple(lemmed_coll_only)\n",
    "                            original_coll = tuple(original_coll_only)\n",
    "\n",
    "                        reorder_coll = False\n",
    "                        if n_gram_idx == 2:\n",
    "                            if ('ADVB' in coll_pos_tags) and (any(['INFN', 'PRTF', 'VERB']) in coll_pos_tags):\n",
    "                                reordered_lemmed_coll = [lemmed_coll[1], lemmed_coll[0]]\n",
    "                                reorder_coll = True\n",
    "                        elif n_gram_idx == 3:\n",
    "                            if (coll_pos_tags[0] == 'PREP') and (coll_pos_tags[1] == 'NOUN') and (coll_pos_tags[2] in ['INFN', 'PRTF', 'VERB']):\n",
    "                                reordered_lemmed_coll = [lemmed_coll[2], lemmed_coll[0], lemmed_coll[1]]\n",
    "                                reorder_coll = True\n",
    "\n",
    "                        if lemmed_coll in colls_added:\n",
    "                            n_grams_df = n_grams_df.append({'Source' : src,\n",
    "                                                            'Source sentence with term bold' : src_sent_term_bold,\n",
    "                                                            'Source sentence with coll bold' : src_sent_coll_bold,\n",
    "                                                            'Target sentence' : target_sent,\n",
    "                                                            'Original collocation' : original_coll,\n",
    "                                                            'Lemmed collocation' : lemmed_coll,\n",
    "                                                            'Collocation type' : coll_type\n",
    "                                                            },\n",
    "                                                              ignore_index = True)\n",
    "                            coll_matched = True\n",
    "                            colls_added.append(lemmed_coll)\n",
    "\n",
    "                        elif (lemmed_coll not in colls_added) and (reorder_coll == True):\n",
    "                            if reordered_lemmed_coll in colls_added:\n",
    "                                n_grams_df = n_grams_df.append({'Source' : src,\n",
    "                                                                'Source sentence with term bold' : src_sent_term_bold,\n",
    "                                                                'Source sentence with coll bold' : src_sent_coll_bold,\n",
    "                                                                'Target sentence' : target_sent,\n",
    "                                                                'Original collocation' : original_coll,\n",
    "                                                                'Lemmed collocation' : reordered_lemmed_coll,\n",
    "                                                                'Collocation type' : coll_type\n",
    "                                                                },\n",
    "                                                                  ignore_index = True)\n",
    "                                coll_matched = True\n",
    "                                colls_added.append(reordered_lemmed_coll)\n",
    "\n",
    "                        elif (lemmed_coll not in colls_added) and (reorder_coll == False):\n",
    "                                n_grams_df = n_grams_df.append({'Source' : src,\n",
    "                                                                'Source sentence with term bold' : src_sent_term_bold,\n",
    "                                                                'Source sentence with coll bold' : src_sent_coll_bold,\n",
    "                                                                'Target sentence' : target_sent,\n",
    "                                                                'Original collocation' : original_coll,\n",
    "                                                                'Lemmed collocation' : lemmed_coll,\n",
    "                                                                'Collocation type' : coll_type\n",
    "                                                                },\n",
    "                                                                  ignore_index = True)\n",
    "                                colls_added.append(lemmed_coll)\n",
    "\n",
    "                        if coll_matched == False:\n",
    "                            edited_n_grams = n_gram['Edited N-grams']\n",
    "                            for edited_n_gram in edited_n_grams:\n",
    "\n",
    "                                src = edited_n_gram['Source']\n",
    "                                src_sent_term_bold = edited_n_gram['Source sentence with term bold']\n",
    "                                src_sent_coll_bold = edited_n_gram['Source sentence with coll bold']\n",
    "                                coll_type = edited_n_gram['Collocation type']\n",
    "                                lemmed_coll_only = edited_n_gram['Lemmed Edited N-gram']\n",
    "                                original_coll_only = edited_n_gram['Original Edited N-gram']\n",
    "                                case_ahead = edited_n_gram['Case ahead']\n",
    "                                coll_pos_tags = tuple(edited_n_gram['Edited N-gram PoS tags'])\n",
    "                                coll_cases = tuple(edited_n_gram['Edited N-gram cases'])\n",
    "                                n_gram_idx = len(original_coll_only)\n",
    "\n",
    "                                if coll_type == 'gram with case ahead':\n",
    "                                    lemmed_coll = tuple(lemmed_coll_only + [case_ahead])\n",
    "                                    original_coll = tuple(original_coll_only + [case_ahead])\n",
    "                                else:\n",
    "                                    lemmed_coll = tuple(lemmed_coll_only)\n",
    "                                    original_coll = tuple(original_coll_only)\n",
    "\n",
    "                                reorder_coll = False\n",
    "                                if n_gram_idx == 2:\n",
    "                                    if ('ADVB' in coll_pos_tags) and (any(['INFN', 'PRTF', 'VERB']) in coll_pos_tags):\n",
    "                                        reordered_lemmed_coll = [lemmed_coll[1], lemmed_coll[0]]\n",
    "                                        reorder_coll = True\n",
    "                                elif n_gram_idx == 3:\n",
    "                                    if (coll_pos_tags[0] == 'PREP') and (coll_pos_tags[1] == 'NOUN') and (coll_pos_tags[2] in ['INFN', 'PRTF', 'VERB']):\n",
    "                                        reordered_lemmed_coll = [lemmed_coll[2], lemmed_coll[0], lemmed_coll[1]]\n",
    "                                        reorder_coll = True\n",
    "\n",
    "                                if lemmed_coll in colls_added:\n",
    "                                    n_grams_df = n_grams_df.append({'Source' : src,\n",
    "                                                                    'Source sentence with term bold' : src_sent_term_bold,\n",
    "                                                                    'Source sentence with coll bold' : src_sent_coll_bold,\n",
    "                                                                    'Target sentence' : target_sent,\n",
    "                                                                    'Original collocation' : original_coll,\n",
    "                                                                    'Lemmed collocation' : lemmed_coll,\n",
    "                                                                    'Collocation type' : coll_type\n",
    "                                                                    },\n",
    "                                                                      ignore_index = True)\n",
    "                                    coll_matched = True\n",
    "                                    colls_added.append(lemmed_coll)\n",
    "\n",
    "                                elif (lemmed_coll not in colls_added) and (reorder_coll == True):\n",
    "                                    if reordered_lemmed_coll in colls_added:\n",
    "                                        n_grams_df = n_grams_df.append({'Source' : src,\n",
    "                                                                        'Source sentence with term bold' : src_sent_term_bold,\n",
    "                                                                        'Source sentence with coll bold' : src_sent_coll_bold,\n",
    "                                                                        'Target sentence' : target_sent,\n",
    "                                                                        'Original collocation' : original_coll,\n",
    "                                                                        'Lemmed collocation' : reordered_lemmed_coll,\n",
    "                                                                        'Collocation type' : coll_type\n",
    "                                                                        },\n",
    "                                                                          ignore_index = True)\n",
    "                                        coll_matched = True\n",
    "                                        colls_added.append(reordered_lemmed_coll)\n",
    "\n",
    "                                elif (lemmed_coll not in colls_added) and (reorder_coll == False):\n",
    "                                    n_grams_df = n_grams_df.append({'Source' : src,\n",
    "                                                                    'Source sentence with term bold' : src_sent_term_bold,\n",
    "                                                                    'Source sentence with coll bold' : src_sent_coll_bold,\n",
    "                                                                    'Target sentence' : target_sent,\n",
    "                                                                    'Original collocation' : original_coll,\n",
    "                                                                    'Lemmed collocation' : lemmed_coll,\n",
    "                                                                    'Collocation type' : coll_type\n",
    "                                                                    },\n",
    "                                                                      ignore_index = True)\n",
    "                                    colls_added.append(lemmed_coll)\n",
    "\n",
    "\n",
    "    def extract_eng_n_grams(df_row):\n",
    "        target_sent = df_row['Target sentence']\n",
    "        sent_tokens1 = target_sent.split(' ')\n",
    "        sent_tokens = []\n",
    "        for token in sent_tokens1:\n",
    "            if token != '':\n",
    "                sent_tokens.append(token)\n",
    "        n_grams_package = []\n",
    "        start_token_idx = 0\n",
    "        while start_token_idx < sent_tokens.index(sent_tokens[-1]):\n",
    "            start_token_list = []\n",
    "            end_token_idx = sent_tokens.index(sent_tokens[-1])\n",
    "            while start_token_idx < end_token_idx:\n",
    "                n_gram_list = sent_tokens[start_token_idx:end_token_idx]\n",
    "                lemmed_n_gram_list = n_gram_list\n",
    "                #lemmed_n_gram_list = [english_lemmatizer.lemmatize(token) for token in n_gram_list]\n",
    "                start_token_list.append({'Original n-gram' : n_gram_list,\n",
    "                                         'Lemmed n-gram' : lemmed_n_gram_list,\n",
    "                                         'Target sentence' : target_sent\n",
    "                                        })\n",
    "                end_token_idx = end_token_idx - 1\n",
    "            n_grams_package.append(start_token_list)\n",
    "            start_token_idx += 1\n",
    "        return n_grams_package\n",
    "\n",
    "    def top_eng_n_gram(series):\n",
    "        n_grams_df = pd.DataFrame()\n",
    "        colls_added = []\n",
    "        for n_grams_package in series:\n",
    "            for start_token_list in n_grams_package:\n",
    "                coll_matched = False\n",
    "                for n_gram_package in start_token_list:\n",
    "                    n_gram_tuple = tuple(n_gram_package['Original n-gram'])\n",
    "                    lemmed_n_gram_tuple = n_gram_package['Lemmed n-gram']\n",
    "                    target_sent = n_gram_package['Target sentence']\n",
    "                    if coll_matched == False:\n",
    "                        # FILTER THE ENG N-GRAMS HERE:\n",
    "                        if any(token in string.punctuation for token in lemmed_n_gram_tuple) == False:\n",
    "                            contains_num = False\n",
    "                            for token in lemmed_n_gram_tuple:\n",
    "                                if any(char.isnumeric() for char in token):\n",
    "                                    contains_num = True\n",
    "                            if contains_num == False:\n",
    "                                if lemmed_n_gram_tuple[0] not in eng_stop_words:\n",
    "                                    if lemmed_n_gram_tuple[-1] not in eng_stop_words:\n",
    "                                        ('--> PASSED THROUGH ENG COLL FILTER.')\n",
    "                                        if lemmed_n_gram_tuple in colls_added:\n",
    "                                            n_grams_df = n_grams_df.append({'Lemmed n-gram' : lemmed_n_gram_tuple,\n",
    "                                                                            'Original n-gram' : n_gram_tuple,\n",
    "                                                                            'Target sentence' : target_sent\n",
    "                                                                           }, ignore_index = True)\n",
    "                                            coll_matched = True\n",
    "                                            colls_added.append(lemmed_n_gram_tuple)\n",
    "                                        else:\n",
    "                                            n_grams_df = n_grams_df.append({'Lemmed n-gram' : lemmed_n_gram_tuple,\n",
    "                                                                            'Original n-gram' : n_gram_tuple,\n",
    "                                                                            'Target sentence' : target_sent\n",
    "                                                                           }, ignore_index = True)\n",
    "                                            colls_added.append(lemmed_n_gram_tuple)\n",
    "#                                     else:\n",
    "#                                         print('COLL CONTAINS STOP WORDS AT END --> DIDN\\'T PASS THROUGH ENG COLL FILTER.')\n",
    "#                                 else:\n",
    "#                                     print('COLL CONTAINS STOP WORDS AT START --> DIDN\\'T PASS THROUGH ENG COLL FILTER.')\n",
    "#                             else:\n",
    "#                                 print('COLL CONTAINS NUMERAL --> DIDN\\'T PASS THROUGH ENG COLL FILTER.')\n",
    "#                         else:\n",
    "#                             print('COLL CONTAINS PUNCT --> DIDN\\'T PASS THROUGH ENG COLL FILTER.')\n",
    "\n",
    "        if n_grams_df.empty == True:\n",
    "            top_eng_coll_df_row = pd.DataFrame()\n",
    "        else:\n",
    "            group_idx = 1\n",
    "            for name, group_df in n_grams_df.groupby(['Lemmed n-gram']):\n",
    "                if group_idx == 1:\n",
    "                    top_eng_coll_df_row = group_df.head(1)\n",
    "                group_idx += 1\n",
    "\n",
    "        return top_eng_coll_df_row\n",
    "\n",
    "    colls_df = pd.DataFrame()\n",
    "    if n_grams_df.empty == False:\n",
    "        for name, group_df in n_grams_df.groupby(['Lemmed collocation']):\n",
    "            top_n_gram_row = group_df.head(1)\n",
    "            top_n_gram_row['Target sentence n-gram'] = top_n_gram_row['Target sentence']\n",
    "            top_n_gram_row['Raw frequency'] = len(group_df.index)\n",
    "            top_n_gram_row['Frequency'] = round(est_term_freq * len(group_df.index)/raw_term_freq)\n",
    "            #top_n_gram_row['Source sentence with term bold'] = group_df.iloc[0]['Source sentence with term bold']\n",
    "            #top_n_gram_row['Source sentence with coll bold'] = group_df.iloc[0]['Source sentence with coll bold']\n",
    "            #top_n_gram_row['Target sentence'] = group_df.iloc[0]['Target sentence']\n",
    "            \n",
    "            other_sent_pairs_en = ''\n",
    "            other_sent_pairs_ru_term_in_bold = ''\n",
    "            other_sent_pairs_ru_coll_in_bold = ''\n",
    "            other_sent_pairs_en_ru_term_in_bold = ''\n",
    "            other_sent_pairs_ru_term_in_bold_en = ''\n",
    "            other_sent_pairs_en_ru_coll_in_bold = ''\n",
    "            other_sent_pairs_ru_coll_in_bold_en = ''\n",
    "            idx = 1\n",
    "            for index, row in group_df.iloc[1:].iterrows():\n",
    "                other_sent_pairs_en += row['Target sentence'] + '<br><br>'\n",
    "                other_sent_pairs_ru_term_in_bold += row['Source sentence with term bold'] + '<br><br>'\n",
    "                other_sent_pairs_ru_coll_in_bold += row['Source sentence with coll bold'] + '<br><br>'\n",
    "                other_sent_pairs_en_ru_term_in_bold += str(row['Target sentence']) + '<br>' + '<font color=\\\"green\\\">' + row['Source sentence with term bold'] + '</font>' + '<br><br>'\n",
    "                other_sent_pairs_ru_term_in_bold_en += row['Source sentence with term bold'] + '<br>' + '<font color=\\\"green\\\">' + row['Target sentence'] + '</font>' + '<br><br>'\n",
    "                other_sent_pairs_en_ru_coll_in_bold += str(row['Target sentence']) + '<br>' + '<font color=\\\"green\\\">' + row['Source sentence with coll bold'] + '</font>' + '<br><br>'\n",
    "                other_sent_pairs_ru_coll_in_bold_en += row['Source sentence with coll bold'] + '<br>' + '<font color=\\\"green\\\">' + row['Target sentence'] + '</font>' + '<br><br>'\n",
    "                idx += 1\n",
    "                if idx == 7:\n",
    "                    break\n",
    "\n",
    "            top_n_gram_row['Other sentence pairs en'] = other_sent_pairs_en\n",
    "            top_n_gram_row['Other sentence pairs ru term in bold'] = other_sent_pairs_ru_term_in_bold\n",
    "            top_n_gram_row['Other sentence pairs ru coll in bold'] = other_sent_pairs_ru_coll_in_bold\n",
    "            top_n_gram_row['Other sentence pairs en ru term in bold'] = other_sent_pairs_en_ru_term_in_bold\n",
    "            top_n_gram_row['Other sentence pairs ru term in bold en'] = other_sent_pairs_ru_term_in_bold_en\n",
    "            top_n_gram_row['Other sentence pairs en ru coll in bold'] = other_sent_pairs_en_ru_coll_in_bold\n",
    "            top_n_gram_row['Other sentence pairs ru coll in bold en'] = other_sent_pairs_ru_coll_in_bold_en\n",
    "\n",
    "            colls_df = colls_df.append(top_n_gram_row, ignore_index = True)\n",
    "\n",
    "    colls_df = colls_df.sort_values('Frequency', ascending=False)\n",
    "    \n",
    "    gram_colls_df = colls_df[colls_df['Collocation type'].isin(['gram with case ahead', 'gram without case ahead'])]\n",
    "    lex_colls_df = colls_df[colls_df['Collocation type']=='lex']\n",
    "        \n",
    "    print('\\n\\tCOUNT COLLS: EST MIN COLL FREQ:', est_min_coll_freq)\n",
    "    print('\\tCOUNT COLLS: RAW MIN COLL FREQ:', raw_min_coll_freq)\n",
    "    \n",
    "    print('\\tCOUNT COLLS: FILTERING FOR LEX COLLS ABOVE MIN COLL FREQ')\n",
    "    if lex_colls_df[(lex_colls_df['Frequency']>=est_min_coll_freq) & (lex_colls_df['Raw frequency']>= raw_min_coll_freq)].empty == False:\n",
    "        return_df = lex_colls_df[(lex_colls_df['Frequency']>=est_min_coll_freq) & (lex_colls_df['Raw frequency']>= raw_min_coll_freq)]\n",
    "        default_to_top_coll = False\n",
    "    else:\n",
    "        default_to_top_coll = True\n",
    "        print('\\tCOUNT COLLS: FILTERING FOR GRAM COLLS ABOVE MIN COLL FREQ')\n",
    "        if gram_colls_df[(gram_colls_df['Frequency']>=est_min_coll_freq) & (gram_colls_df['Raw frequency']>= raw_min_coll_freq)].empty == False:\n",
    "            return_df = gram_colls_df[(gram_colls_df['Frequency']>=est_min_coll_freq) & (gram_colls_df['Raw frequency']>= raw_min_coll_freq)].head(1)\n",
    "        else:\n",
    "            print('\\tCOUNT COLLS: FILTERING FOR DEFAULT LEX COLL BELOW MIN COLL FREQ')\n",
    "            if lex_colls_df.empty == False:\n",
    "                return_df = lex_colls_df.head(1)\n",
    "            else:\n",
    "                print('\\tCOUNT COLLS: FILTERING FOR DEFAULT GRAM COLLS BELOW MIN COLL FREQ')\n",
    "                if gram_colls_df.empty == False:\n",
    "                    return_df = gram_colls_df.head(1)\n",
    "                else:\n",
    "                    print('\\tCOUNT COLLS: FILTERING FOR NO PUNCT COLLS')\n",
    "                    no_punct_df = colls_df[colls_df['Collocation type']!='punct']\n",
    "                    if no_punct_df.empty == False:\n",
    "                        return_df = no_punct_df.head(1)\n",
    "                    else:\n",
    "                        print('\\tCOUNT COLLS: DEFAULTING TO FIRST ROW IN COLLS DF')\n",
    "                        if colls_df.empty == False:\n",
    "                            return_df = colls_df.head(1)\n",
    "                        else:\n",
    "                            print('\\tCOUNT_COLLS: NO COLLOCATIONS FOUND.')\n",
    "\n",
    "    return lex_colls_df, return_df, gram_colls_df.head(3), default_to_top_coll\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create idiom dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Scrape wordreference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def wordref_phrases_scrape(word):\n",
    "    \n",
    "    base_url = \"https://www.wordreference.com/ruen\"\n",
    "    full_url = base_url + '/' + word\n",
    "    full_url = full_url.replace('\\n','')\n",
    "    response = requests.get(full_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for a in soup.select('div #article'):\n",
    "            # to extract the phrases in the wordref definition\n",
    "            if a.find('span', 'phrase') != None:\n",
    "                extracted_phrases_dict = {}\n",
    "                \n",
    "                quick_dict = {}\n",
    "                \n",
    "                for phrase_ru_tag in a.findAll('span', 'phrase'):\n",
    "                    \n",
    "                    phrase_ru_text = phrase_ru_tag.get_text()\n",
    "                    phrase_ru_text = phrase_ru_text\n",
    "                    \n",
    "                    #print(phrase_ru_text, phrase_en_text)\n",
    "                    \n",
    "                    #Divide phrase syntax into multiple phrases where applicable:\n",
    "                    phrase_variants = ['', '', '']\n",
    "                    \n",
    "                    phrase_ru_text_list = []\n",
    "                    \n",
    "                    idx = 0\n",
    "                    for word in phrase_ru_text.split(' '):\n",
    "                        \n",
    "                        #print(word)\n",
    "                        \n",
    "                        if '/' in phrase_ru_text:\n",
    "                            #print(phrase_ru_text.split('/'))\n",
    "                            phrase_ru_text_list.append(word.split('/'))\n",
    "                        else:\n",
    "                            phrase_ru_text_list.append([word])\n",
    "                        \n",
    "                        idx += 1\n",
    "                            \n",
    "                    #print(phrase_ru_text_list)\n",
    "                    \n",
    "                    \n",
    "                    for word_group in phrase_ru_text_list:\n",
    "                        #print(word_group)\n",
    "                        for word in word_group:\n",
    "                            for item in phrase_variants:\n",
    "                                item += word + ' '\n",
    "                    \n",
    "                    #print(phrase_variants)\n",
    "                    \n",
    "                    next_el = phrase_ru_tag.findNext()\n",
    "                    if next_el.has_attr('class'):\n",
    "                        \n",
    "                        if next_el[\"class\"][0] == 'IN':\n",
    "                            \n",
    "                            phrase_ru_text += ' (' + next_el.get_text() + ')'\n",
    "                        \n",
    "                        elif next_el.get(\"class\")[0] == 'ital' and next_el.get_text() == 'или':\n",
    "                            \n",
    "                            words = phrase_ru_text.split(' ')\n",
    "            \n",
    "                            idx = 0\n",
    "                            phrase_start = ''\n",
    "                            phrase_one_end = ''\n",
    "                            phrase_two_end = ''\n",
    "                            for word in words:\n",
    "                                \n",
    "                                if word == words[words.index('или') - 1]:\n",
    "                                    phrase_one_end = word\n",
    "                                    \n",
    "                                elif word == words[words.index('или') + 1]:\n",
    "                                    phrase_two_end = word\n",
    "                                \n",
    "                                else:\n",
    "                                    if word != words[words.index('или')]:\n",
    "                                        phrase_start += word + ' '\n",
    "                                    \n",
    "                                idx += 1\n",
    "                            \n",
    "                            phrase_ru_text = phrase_start + phrase_one_end\n",
    "                            extracted_phrases_dict[phrase_start + phrase_two_end] = phrase_ru_tag.findNext('a').get_text()\n",
    "                    \n",
    "                    phrase_en_tag = phrase_ru_tag.findNext('a')\n",
    "                    phrase_en_text = phrase_en_tag.get_text()\n",
    "                    \n",
    "                    next_en_tag = phrase_en_tag.findNext()\n",
    "                    \n",
    "                    if next_en_tag.has_attr('class'):\n",
    "                        if next_en_tag.get(\"class\")[0] == 'ital' and next_en_tag.get_text() == 'или':\n",
    "                            phrase_en_text += ' или'\n",
    "                            nn_en_tag = next_en_tag.findNext()\n",
    "                            if nn_en_tag.has_attr('href'):\n",
    "                                phrase_en_text += ' ' + nn_en_tag.get_text()\n",
    "\n",
    "                    \n",
    "                    extracted_phrases_dict[phrase_ru_text] = phrase_en_text\n",
    "                    \n",
    "                return(extracted_phrases_dict)\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        print(response)\n",
    "        print('\\tError connecting to wordreference.com.\\nProgram stopped.')\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wordref_phrases_scrape('от')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "terms = []\n",
    "\n",
    "# import freq list\n",
    "with open('ru_corpus_freq_list.tsv', 'r') as file:\n",
    "    \n",
    "    output_file = open(proj_dir + '/ru_phrases_file10.txt', 'w+')\n",
    "    \n",
    "    row_count = 1\n",
    "    \n",
    "    start_now = False\n",
    "    \n",
    "    for index,row in rnc_freq_list.iterrows():\n",
    "            \n",
    "        term = row['words']\n",
    "        term = term.replace('\\n','')\n",
    "        \n",
    "        if term == 'рупор':\n",
    "            start_now = True\n",
    "\n",
    "        if start_now == True:\n",
    "            print(term.upper())\n",
    "\n",
    "            if wordref_phrases_scrape(word = term) != None:\n",
    "                phrases = wordref_phrases_scrape(word = term)\n",
    "                # clean up phrase: remove brackets, obj pronouns and accents\n",
    "\n",
    "                if phrases:\n",
    "                    for ru,eng in phrases.items():\n",
    "\n",
    "                        print(ru + ' ' + '£' + eng + '\\n')\n",
    "\n",
    "                        output_file.write(ru + ' ' + '£' + eng + '\\n')\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        row_count += 1\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Scrape phrases from dic.academic.ru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def academic_ru_scrape(word):\n",
    "    \n",
    "    phrase_list = []\n",
    "    full_url = 'https://translate.academic.ru/{}/ru/en/'.format(word)\n",
    "    full_url = full_url.replace('\\n','')\n",
    "    response = requests.get(full_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for div in soup.find_all('div'):\n",
    "            div_text = div.get_text()\n",
    "            if bool(re.search('[а-яА-Я]', div_text)) == True and len(div_text) <= 130:\n",
    "                phrase_list.append(div_text.replace('\\n', ' ').replace('\\t', ' '))\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "            \n",
    "    return phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_file = open(proj_dir + '\\\\dic_academic_ru_phrases.txt', 'w+', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for index, row in rnc_freq_list.iterrows():\n",
    "    print(index, row['Word'])\n",
    "    while True:\n",
    "        try:\n",
    "            [output_file.write(phrase + '\\n') for phrase in academic_ru_scrape(row['Word'])]\n",
    "            time.sleep(5)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Filter and format the phrases from dic.academic.ru and export filtered phrases to txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(proj_dir + '\\\\dic_academic_ru_phrases.txt', 'r', encoding='utf-8') as input_file:\n",
    "    with open(proj_dir + '\\\\dic_academic_ru_phrases_filtered.txt', 'w+', encoding='utf-8') as output_file:\n",
    "        \n",
    "        phrase_dict = {}\n",
    "        \n",
    "        for line in input_file.readlines():\n",
    "            if any(string in line for string in ['Перевод:',\n",
    "                                         'Толкование перевода',\n",
    "                                        'Толкование Перевод',\n",
    "                                         'Экспорт словарей на сайты',\n",
    "                                         'Пометить текст и поделиться']\n",
    "                  ) == False:\n",
    "                if bool(re.search(r'\\d[\\.\\)]', line)) == False:\n",
    "                    if bool(re.search(r'[а-яА-Я]', line)) == True:\n",
    "                        if len(line.strip().split(' ')) > 1:\n",
    "                            \n",
    "                            line = line.lower()\n",
    "                                    \n",
    "                            phrases = []\n",
    "                            for phrase in line.split(';'):\n",
    "                                \n",
    "                                phrase = re.sub(r'\\([^)]*\\)', '', phrase)\n",
    "                                phrase = re.sub(r'\\w+\\.\\s', '', phrase)\n",
    "                                phrase = re.sub(r'\\w+\\.\\;', ';', phrase)\n",
    "                                if phrase.startswith(('- ',' - ', '— ', ' — ', ' —  ')):\n",
    "                                    phrase = phrase[2:]\n",
    "                                    \n",
    "                                if bool(re.search(r'[а-яА-Я]+\\s[\\—\\-]\\s[a-zA-Z]+', phrase)) == True:\n",
    "                                    ru = re.split(r'\\s[\\—\\-]\\s', phrase)[0]\n",
    "                                    en = re.split(r'\\s[\\—\\-]\\s', phrase)[1]\n",
    "                                else:\n",
    "                                    if bool(re.search(r'[a-zA-Z]+\\s*[a-zA-Z]', phrase)) == True:\n",
    "                                        ru = re.sub(r'[a-zA-Z]+\\s*[a-zA-Z]', '', phrase)\n",
    "                                        en = re.findall(r'[a-zA-Z]+\\s*[a-zA-Z]', phrase)[0]\n",
    "                                    else:\n",
    "                                        ru = phrase\n",
    "                                        en = ''\n",
    "                                \n",
    "                                ru = ''.join([char for char in ru if char.isalpha() or \\\n",
    "                                                                    char.isspace() or \\\n",
    "                                                                    char == '-'])\n",
    "                                for key, value in {'о́' : 'о', 'я́' : 'я', 'и́' : 'и',\n",
    "                                                   'э́' : 'э', 'а́' : 'а', 'е' : 'е',\n",
    "                                                   'ы́' : 'ы', 'у́' : 'у'}.items():\n",
    "                                    if key in ru:\n",
    "                                        phrase = phrase.replace(key, value)\n",
    "                                phrase = re.sub(r'\\s+', ' ', ru)\n",
    "                                ru = ' ' + ru.strip() + ' '\n",
    "                                \n",
    "                                en = en.replace('\\n', '')\n",
    "                    \n",
    "                                if len(ru.strip().split(' ')) > 1:\n",
    "                            \n",
    "                                    if (ru not in phrase_dict) or (phrase_dict[ru] == ''):\n",
    "                                        phrase_dict[ru] = en\n",
    "                                        \n",
    "        for ru, en in phrase_dict.items():\n",
    "            output_file.write((ru + '||' + en).replace('\\n', '') + '\\n')\n",
    "            idiom_dict[ru] = {'Original ru phrase' : ru,\n",
    "                             'Eng tran' : en}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Import dic.academic.ru filtered phrases by adding to idiom_dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(proj_dir + '\\\\dic_academic_ru_phrases_filtered.txt', 'r', encoding='utf-8') as input_file:\n",
    "    for line in input_file:\n",
    "        ru = line.split('||')[0]\n",
    "        en = line.split('||')[1]\n",
    "        \n",
    "        idiom_dict[ru] = {'Original ru phrase' : ru,\n",
    "                     'Eng tran' : en}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Standardise and lemmatise idiom dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# REDUNDANT\n",
    "\n",
    "def standardise_idiom(ru_idiom, eng_tran):\n",
    "\n",
    "    return_dict = {}\n",
    "    \n",
    "    ru_phrase_for_lemming = ru_idiom.replace('    ', ' ')\n",
    "    ru_phrase_for_lemming = ru_phrase_for_lemming.replace('  ', ' ')\n",
    "    \n",
    "    ru_phrase_for_lemming = ru_phrase_for_lemming.replace('-', '/')\n",
    "    \n",
    "    ru_phrase_for_lemming = ru_phrase_for_lemming.replace('кого/н', 'кого-н').replace('кому/н', 'кому-н')\n",
    "    ru_phrase_for_lemming = ru_phrase_for_lemming.replace('что/н', 'что-н').replace('куда/нибудь', 'куда-нибудь')\n",
    "    ru_phrase_for_lemming = ru_phrase_for_lemming.replace(' чём/н', ' чём-н')\n",
    "    ru_phrase_for_lemming = ru_phrase_for_lemming.replace('из/за', 'из-за')\n",
    "    \n",
    "    print(ru_phrase_for_lemming)\n",
    "    \n",
    "    variants = []\n",
    "    \n",
    "    #Divide phrase syntax into multiple phrases where applicable:\n",
    "    \n",
    "    if '/' in ru_phrase_for_lemming:\n",
    "\n",
    "        phrase_ru_text_list = ru_phrase_for_lemming.split(' ')\n",
    "\n",
    "        print(phrase_ru_text_list)\n",
    "\n",
    "        idx = 0\n",
    "        for word in phrase_ru_text_list:\n",
    "\n",
    "            if '/' in word:\n",
    "                variant_words = word.split('/')\n",
    "                slash_word_index = idx\n",
    "                \n",
    "            idx += 1\n",
    "        \n",
    "        for variant_word in variant_words:\n",
    "            phrase_ru_text_list[slash_word_index] = variant_word\n",
    "            print(phrase_ru_text_list)\n",
    "            variants.append(' '.join(phrase_ru_text_list))\n",
    "\n",
    "    else:\n",
    "        variants.append(ru_phrase_for_lemming)\n",
    "        \n",
    "        \n",
    "    print(variants)\n",
    "        \n",
    "        \n",
    "    for variant in variants:\n",
    "    \n",
    "        ru_phrase_for_lemming = variant.replace('(perf)', '').replace('(impf)', '')\n",
    "\n",
    "        ru_phrase_for_lemming = ru_phrase_for_lemming.replace('о́','о').replace('ы́','ы').replace('а́','а').replace('у́','у').replace('и́','и').replace('я́','я').replace('е́','е').replace('ю́','ю').replace('э́','э')\n",
    "        ru_phrase_for_lemming = ru_phrase_for_lemming.replace('кого-н', '').replace('кому-н', '')\n",
    "        ru_phrase_for_lemming = ru_phrase_for_lemming.replace('что-н', '').replace('кого-н', '').replace('куда-нибудь', '')\n",
    "        ru_phrase_for_lemming = ru_phrase_for_lemming.replace('чём-н', '')\n",
    "        ru_phrase_for_lemming = ru_phrase_for_lemming.replace('  +instr ', '').replace('+gen', '').replace('+dat', '').replace('+infin', '')\n",
    "\n",
    "\n",
    "        lemmed_ru_phrase = lem_coll(ru_phrase_for_lemming)\n",
    "        \n",
    "        lemmed_ru_phrase = lemmed_ru_phrase.replace('  ', ' ')\n",
    "\n",
    "\n",
    "        return_dict[lemmed_ru_phrase] = {'Original ru phrase' : variant, 'Eng tran' : eng_tran}\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "standardise_idiom('приносить кого-н-что-н в жертву ', 'to sacrifice sb/sth ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idiom_dict[' в перспективе ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# RE-LEMMATISE THE IDIOM DICT SO THAT IT LEMMATISES EVERY WORD IN THE PHRASE:\n",
    "\n",
    "original_ru_phrase_list = []\n",
    "eng_tran_list = []\n",
    "\n",
    "for key, value in idiom_dict.items():\n",
    "    original_ru_phrase_list.append(value['Original ru phrase'])\n",
    "    eng_tran_list.append(value['Eng tran'])\n",
    "    \n",
    "idiom_dict_df = pd.DataFrame({'Original ru phrase' : original_ru_phrase_list,\n",
    "                              'Eng tran' : eng_tran_list                           \n",
    "                             }\n",
    "                            )\n",
    "\n",
    "def lem_coll_from_string(coll_string):\n",
    "    coll_string = ''.join([char for char in coll_string if char.isalpha() or \\\n",
    "                                                    char.isspace() or \\\n",
    "                                                    char == '-'])\n",
    "    coll_string = coll_string.strip()\n",
    "    coll_list = coll_string.split(' ')\n",
    "    lem_coll = ' '\n",
    "    for word in coll_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lem_coll += p.normal_form + ' '\n",
    "    # replace ё with е (to account for variations in spelling practices in para_texts_df)\n",
    "    lem_coll = lem_coll.replace('ё', 'е')\n",
    "    return lem_coll\n",
    "\n",
    "idiom_dict_df['Lemmed coll'] = idiom_dict_df['Original ru phrase'].apply(lem_coll_from_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idiom_dict_df.iloc[90020:90030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idiom_dict_df[idiom_dict_df['Lemmed coll']==' еще бы ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Export idiom_dict_df to csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idiom_dict_df.to_csv('C:\\\\Users\\\\MdeCL\\\\Google Drive\\\\Work, productivity and interests_\\\\computer science\\\\coding skills (technical)\\\\VS Code project files\\\\NLP-powered Vocab Learning Strategy\\\\Russian Vocab Project\\\\idiom_dict_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Export idiom dictionary to txt file (redundant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(proj_dir + '/ru_phrases_file.txt', 'r') as file:\n",
    "    \n",
    "    output_file = open(proj_dir + '/lemmed_ru_phrases_file.txt', 'w+')\n",
    "    \n",
    "    file_read = file.read()\n",
    "    lines = file_read.split('\\n')\n",
    "    idx = 1\n",
    "    for line in lines:\n",
    "        \n",
    "        if line != '':\n",
    "        \n",
    "            ru_phrase = line.split('£')[0]\n",
    "\n",
    "            eng_phrase = line.split('£')[1]\n",
    "            \n",
    "            lemmed_ru_phrase_dict = standardise_idiom(ru_phrase, eng_phrase)\n",
    "            \n",
    "            for key,value in lemmed_ru_phrase_dict.items():\n",
    "                \n",
    "                lemmed_ru_phrase = key\n",
    "                ru_phrase = value['Original ru phrase']\n",
    "                eng_phrase = value['Eng tran']\n",
    "            \n",
    "                output_file.write(lemmed_ru_phrase + '£' + ru_phrase + '£' + eng_phrase + '\\n')\n",
    "\n",
    "        idx += 1\n",
    "    \n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "para_texts_df[para_texts_df['Lemmed source sentence'].str.contains(' выйти за рамка ', regex=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rnc_freq_list[rnc_freq_list['Estimated frequency']>=1780]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform analysis of each term in frequency list and write flashcards:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the info about a word in the corpus and otherwise online:\n",
    "- Semantic analysis\n",
    "- Collocational analysis\n",
    "- Grammatical analysis\n",
    "- Register / genre analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_en_note_type = genanki.Model(\n",
    "  1607392319,\n",
    "  'TL-EN TYPE TEST v3',\n",
    "  fields=[\n",
    "    {'name': 'Scaled frequency'},\n",
    "    {'name' : 'Frequency rank'},\n",
    "    {'name' : 'Term/collocation test'},\n",
    "    {'name': 'Question'},\n",
    "    {'name': 'Answer'},\n",
    "    {'name': 'Term with accent'},\n",
    "    {'name': 'Distinguishing grammatical info'},\n",
    "    {'name' : 'Conjugation and declension info'},\n",
    "    {'name' : 'Top three grammatical collocations'},\n",
    "    {'name': 'Definition being tested'},\n",
    "    {'name': 'Other definitions'},\n",
    "    {'name': 'Source sentence'},\n",
    "    {'name': 'Target sentence'},\n",
    "    {'name': 'Sentence source'},\n",
    "    {'name' : 'Other sentence pairs ru'},\n",
    "    {'name' : 'Other sentence pairs both'}\n",
    "  ],\n",
    "  templates=[\n",
    "    {\n",
    "      'name': 'TL-EN CARD TYPE',\n",
    "      'qfmt': '<font>{{Term/collocation test}}<br><br><font size=\"+6\">{{Term with accent}}</font><br><font size=\"-1\">{{Conjugation and declension info}}<br>{{Distinguishing grammatical info}}<br>{{Top three grammatical collocations}}<br><hr><font size=\"+1\"><font color=\"navy\"><b>____</b><br>{{Other definitions}}</font><br><hr>{{Source sentence}}{{type:Target sentence}}<br>{{Other sentence pairs ru}}</font>',\n",
    "      'afmt': '<font>{{Term/collocation test}}<br><br><font size=\"+6\">{{Term with accent}}</font><br><font size=\"-1\">{{Conjugation and declension info}}<br>{{Distinguishing grammatical info}}<br>{{Top three grammatical collocations}}<br><hr><font size=\"+1\"><font color=\"navy\"><b>{{Definition being tested}}</b><br>{{Other definitions}}</font><br><hr>{{Source sentence}}<br><font color=\"green\">{{Target sentence}}</font>{{type:Target sentence}}<br>{{Other sentence pairs both}}</font>',\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "en_tl_note_type = genanki.Model(\n",
    "  1607392320,\n",
    "  'EN-TL TYPE TEST v3',\n",
    "  fields=[\n",
    "    {'name': 'Scaled frequency'},\n",
    "    {'name' : 'Frequency rank'},\n",
    "    {'name' : 'Term/collocation test'},\n",
    "    {'name': 'Question'},\n",
    "    {'name': 'Answer'},\n",
    "    {'name': 'Term with accent'},\n",
    "    {'name': 'Distinguishing grammatical info'},\n",
    "    {'name' : 'Conjugation and declension info'},\n",
    "    {'name' : 'Top three grammatical collocations'},\n",
    "    {'name': 'Definition being tested'},\n",
    "    {'name': 'Other definitions'},\n",
    "    {'name': 'Source sentence'},\n",
    "    {'name': 'Target sentence'},\n",
    "    {'name': 'Sentence source'},\n",
    "    {'name' : 'Other sentence pairs en'},\n",
    "    {'name' : 'Other sentence pairs both'}\n",
    "  ],\n",
    "  templates=[\n",
    "    {\n",
    "      'name': 'EN-TL CARD TYPE',\n",
    "      'qfmt': '<font>{{Term/collocation test}}<br><br><font size=\"+6\">______</font><br><br><font size=\"-1\">{{Distinguishing grammatical info}}<br><hr><font size=\"+1\"><font color=\"navy\"><b>{{Definition being tested}}</b><br>{{Other definitions}}</font><br><hr>{{Target sentence}}{{type:Source sentence}}<br>{{Other sentence pairs en}}</font>',\n",
    "      'afmt': '<font>{{Term/collocation test}}<br><br><font size=\"+6\">{{Term with accent}}</font><br><font size=\"-1\">{{Conjugation and declension info}}<br>{{Distinguishing grammatical info}}<br>{{Top three grammatical collocations}}<br><hr><font size=\"+1\"><font color=\"navy\"><b>{{Definition being tested}}</b><br>{{Other definitions}}</font><br><hr>{{Target sentence}}<br><font color=\"green\">{{Source sentence}}</font>{{type:Source sentence}}<br>{{Other sentence pairs both}}</font>',\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnc_freq_list[rnc_freq_list['Word']=='иней']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colls_written_to_cards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_freq_rank = 4500\n",
    "end_freq_rank = 5000\n",
    "limited_freq_list = rnc_freq_list.iloc[start_freq_rank:end_freq_rank]\n",
    "reduced_df_size = 8000000\n",
    "max_concordance_results = 400\n",
    "est_min_coll_freq = 370\n",
    "raw_min_coll_freq = 2\n",
    "\n",
    "deck_name = 'Russian Vocab ' + str(start_freq_rank) + '-' + str(end_freq_rank)\n",
    "\n",
    "my_deck = genanki.Deck(2059400110, deck_name)\n",
    "\n",
    "reduced_df = para_texts_df.iloc[0:reduced_df_size]\n",
    "\n",
    "for index, row in limited_freq_list.iterrows():\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(index)\n",
    "    query = row['Word']\n",
    "    print(query)\n",
    "    freq_rank = row['Frequency rank']\n",
    "    estimated_freq = row['Estimated frequency']\n",
    "    print('TERM ESTIMATED FREQUENCY:', str(estimated_freq))\n",
    "    query_pos_full = row['PoS tag']\n",
    "\n",
    "    return_dict = eng_trans_and_syns(query, query_pos_full)\n",
    "    inflected_eng_defs = return_dict['inflected_eng_defs']\n",
    "    term_gender_colour = return_dict['term_gender_colour']\n",
    "    term_with_accent = return_dict['term_with_accent']\n",
    "    case_taken = return_dict['case_taken']\n",
    "    disting_gram_info = return_dict['disting_gram_info']\n",
    "    conjugation_declension_info = ', '.join(return_dict['conjugation_declension_info'])\n",
    "\n",
    "    query_forms = []\n",
    "    query_parse = morph.parse(query)[0]\n",
    "    for word_form in query_parse.lexeme:\n",
    "        form = word_form[0]\n",
    "        query_forms.append(form)\n",
    "        if 'ё' in form:\n",
    "            query_forms.append(form.replace('ё', 'е'))\n",
    "    query_forms = list(set(query_forms))\n",
    "\n",
    "    p = morph.parse(query)[0]\n",
    "    lemmed_query = p.normal_form\n",
    "    results_df = reduced_df[reduced_df['Lemmed source sentence'].str.contains(' ' + lemmed_query + ' ', regex = False)]\n",
    "\n",
    "    print('\\n\\nNUMBER OF CONCORDANCE RESULTS BEFORE DELETING DUPLICATES:', len(results_df.index))\n",
    "    results_df = results_df.drop_duplicates('Source sentence')\n",
    "    print('NUMBER OF CONCORDANCE RESULTS AFTER DELETING DUPLICATES:', len(results_df.index))\n",
    "    concordance_freq = len(results_df.iloc[:])\n",
    "    if len(results_df.index) > max_concordance_results:\n",
    "        print('RESULTS DF REDUCED TO ' + str(max_concordance_results) + ' PAIRED SENTENCES.')\n",
    "        results_df = results_df.iloc[0:max_concordance_results]\n",
    "    reduced_concordance_freq = len(results_df.iloc[:])\n",
    "\n",
    "    results_df['English definition'] = 'Unmatched'\n",
    "    eng_def_idx = 1\n",
    "    for basic_def, all_forms in inflected_eng_defs.items():\n",
    "        for form in all_forms:\n",
    "            form = form.strip().replace('.', ' ')\n",
    "            form_wds = form.split(' ')\n",
    "            wd_count = len(form_wds)\n",
    "            if wd_count == 1:\n",
    "                results_df[results_df['Standardised target sentence'].str.contains(' ' +  form + ' ', regex=False)] = results_df[results_df['Standardised target sentence'].str.contains(' ' +  form + ' ', regex=False)].replace('Unmatched', basic_def)\n",
    "            elif wd_count == 2:\n",
    "                results_df[results_df['Standardised target sentence'].str.contains(' ' +  form + ' ', regex=False)]['English definition'] = results_df[results_df['Standardised target sentence'].str.contains(' ' +  form + ' ', regex=False)]['English definition'].replace('Unmatched', basic_def)\n",
    "                results_df[results_df['Standardised target sentence'].str.contains(' '+form_wds[0]+'.*'+form_wds[1]+' ', regex=True)]['English definition'] = results_df[results_df['Standardised target sentence'].str.contains(' '+form_wds[0]+'.*'+form_wds[1]+' ', regex=True)]['English definition'].replace('Unmatched', basic_def)\n",
    "            elif wd_count == 3:\n",
    "                results_df[results_df['Standardised target sentence'].str.contains(' ' +  form + ' ', regex=False)]['English definition'] = results_df[results_df['Standardised target sentence'].str.contains(' ' +  form + ' ', regex=False)]['English definition'].replace('Unmatched', basic_def)\n",
    "                results_df[results_df['Standardised target sentence'].str.contains(' '+form_wds[0]+'.*'+form_wds[1]+' '+'.*'+form_wds[2]+' ', regex=True)]['English definition'] = results_df[results_df['Standardised target sentence'].str.contains(' '+form_wds[0]+'.*'+form_wds[1]+' '+'.*'+form_wds[2]+' ', regex=True)]['English definition'].replace('Unmatched', basic_def)\n",
    "\n",
    "    print('\\nRESULTS_DF.UNIQUE()', results_df['English definition'].unique())\n",
    "    eng_def_idx = 1\n",
    "    print('\\nENG DEF VALUE COUNTS:')\n",
    "    print(results_df[results_df['English definition']!='Unmatched']['English definition'].value_counts())\n",
    "\n",
    "    basic_defs = results_df[results_df['English definition']!='Unmatched']['English definition'].value_counts().index.tolist()\n",
    "    for eng_def in basic_defs:\n",
    "        print('\\nENG DEF INDEX:', eng_def_idx)\n",
    "        print('ENG DEF:', eng_def)\n",
    "        eng_def_df = results_df[results_df['English definition']==eng_def]\n",
    "        if eng_def_df.empty == True:\n",
    "            pass\n",
    "        else:\n",
    "            eng_def_df = eng_def_df.drop_duplicates('Source sentence')\n",
    "            print('NUMBER OF SENTENCES FOR THIS ENGLISH DEFINITION:', len(eng_def_df.index))\n",
    "            if eng_def_idx == 1:\n",
    "                eng_def_freq = estimated_freq\n",
    "            else:\n",
    "                eng_def_freq = round(estimated_freq * (len(eng_def_df.index)/reduced_concordance_freq))\n",
    "            print('ESTIMATED ENG DEF FREQUENCY:', eng_def_freq)\n",
    "\n",
    "            if eng_def_freq < est_min_coll_freq:\n",
    "                print('ESTIMATED ENGLISH DEFINITION FREQUENCY IS LOWER THAN MINIMUM COLL FREQUENCY --> NOT WRITING ANY FLASHCARDS FOR IT.')\n",
    "            else:\n",
    "                q_forms = query_forms\n",
    "                return_series = eng_def_df.apply(colls_from_sents_df_row, query_forms=q_forms, axis=1)\n",
    "                lex_colls_df, eng_def_colls_df, top_three_gram_colls_df, default_to_top_coll = count_colls(return_series.tolist(), q_forms, raw_min_coll_freq, est_min_coll_freq, eng_def_freq, len(eng_def_df.index), estimated_freq, reduced_concordance_freq)\n",
    "                \n",
    "                case_colour_dict = {'gent' : 'green',\n",
    "                                    'datv' : 'purple',\n",
    "                                   'loct' : 'blue',\n",
    "                                   'ablt' : 'aqua'}\n",
    "                gram_coll_list = []\n",
    "                for index, row in top_three_gram_colls_df.iterrows():\n",
    "                    gram_coll = ' '.join(row['Original collocation'])\n",
    "                    for key, value in case_colour_dict.items():\n",
    "                        if key in gram_coll:\n",
    "                            gram_coll = gram_coll.replace(key, '<font color=\"' + value + '\">' + key + '</font>')\n",
    "                    gram_coll_list.append(gram_coll + ' ' + str(row['Raw frequency']))\n",
    "                top_three_gram_colls = ' | '.join(gram_coll_list)\n",
    "                print('TOP THREE GRAM COLLS:', top_three_gram_colls)\n",
    "                print('\\n')\n",
    "\n",
    "                if eng_def_colls_df.empty == True:\n",
    "                    print('ENG DEF COLLS DF RETURNED EMPTY.')\n",
    "                else:\n",
    "                    # Create cards for the English definition only\n",
    "                    print('CARDS FOR ENG DEF ONLY:')\n",
    "                    row = eng_def_colls_df.iloc[0]\n",
    "                    card_freq = eng_def_freq\n",
    "                    term_or_collocation = 'Term'\n",
    "                    tl_test = query\n",
    "                    en_test = eng_def\n",
    "                    other_defs = '<br>'.join([item for item in basic_defs if item != eng_def])\n",
    "\n",
    "                    source_sent_one = row['Source sentence with term bold']\n",
    "                    target_sent_one = row['Target sentence']\n",
    "                    \n",
    "                    other_sent_pairs_en = ''\n",
    "                    other_sent_pairs_ru_term_in_bold = ''\n",
    "                    other_sent_pairs_ru_coll_in_bold = ''\n",
    "                    other_sent_pairs_en_ru_term_in_bold = ''\n",
    "                    other_sent_pairs_ru_term_in_bold_en = ''\n",
    "                    other_sent_pairs_en_ru_coll_in_bold = ''\n",
    "                    other_sent_pairs_ru_coll_in_bold_en = ''\n",
    "                    idx = 1\n",
    "                    for index, lex_coll_row in lex_colls_df.iloc[1:].iterrows():\n",
    "                        other_sent_pairs_en += lex_coll_row['Target sentence'] + '<br><br>'\n",
    "                        other_sent_pairs_ru_term_in_bold += lex_coll_row['Source sentence with term bold'] + '<br><br>'\n",
    "                        other_sent_pairs_ru_coll_in_bold += lex_coll_row['Source sentence with coll bold'] + '<br><br>'\n",
    "                        other_sent_pairs_en_ru_term_in_bold += str(lex_coll_row['Target sentence']) + '<br>' + '<font color=\\\"green\\\">' + lex_coll_row['Source sentence with term bold'] + '</font>' + '<br><br>'\n",
    "                        other_sent_pairs_ru_term_in_bold_en += lex_coll_row['Source sentence with term bold'] + '<br>' + '<font color=\\\"green\\\">' + lex_coll_row['Target sentence'] + '</font>' + '<br><br>'\n",
    "                        other_sent_pairs_en_ru_coll_in_bold += str(lex_coll_row['Target sentence']) + '<br>' + '<font color=\\\"green\\\">' + lex_coll_row['Source sentence with coll bold'] + '</font>' + '<br><br>'\n",
    "                        other_sent_pairs_ru_coll_in_bold_en += lex_coll_row['Source sentence with coll bold'] + '<br>' + '<font color=\\\"green\\\">' + lex_coll_row['Target sentence'] + '</font>' + '<br><br>'\n",
    "                        idx += 1\n",
    "                        if idx == 7:\n",
    "                            break\n",
    "\n",
    "                    print('\\n\\tCOLLOCATION TYPE:', row['Collocation type'])\n",
    "                    print('\\tRAW FREQUENCY:', row['Raw frequency'])\n",
    "                    print('\\tSCALED FREQUENCY:', row['Frequency'])\n",
    "                    print('\\tLEMMED COLLOCATION:', row['Lemmed collocation'])\n",
    "                    print('\\tORIGINAL COLLOCATION:', row['Original collocation'])\n",
    "                    print('\\tCARD FREQUENCY:', card_freq)\n",
    "                    print('\\n')\n",
    "\n",
    "                    tl_en_note = genanki.Note(\n",
    "                    model = tl_en_note_type,\n",
    "                    fields = [str(card_freq),\n",
    "                             str(freq_rank) + ' d' + str(eng_def_idx),\n",
    "                             term_or_collocation,\n",
    "                              tl_test,\n",
    "                              en_test,\n",
    "                              '<font color=\\\"' + term_gender_colour + '\\\">' + term_with_accent + '</font>' + case_taken,\n",
    "                              disting_gram_info,\n",
    "                              conjugation_declension_info,\n",
    "                              top_three_gram_colls,\n",
    "                              eng_def,\n",
    "                              other_defs,\n",
    "                              source_sent_one,\n",
    "                              target_sent_one,\n",
    "                              str(row['Source']),\n",
    "                              other_sent_pairs_ru_term_in_bold,\n",
    "                              other_sent_pairs_ru_term_in_bold_en\n",
    "                                ]\n",
    "                    )\n",
    "\n",
    "                    my_deck.add_note(tl_en_note)\n",
    "\n",
    "                    en_tl_note = genanki.Note(\n",
    "                    model = en_tl_note_type,\n",
    "                    fields = [str(round(card_freq * 1.05)),\n",
    "                            str(freq_rank) + ' d' + str(eng_def_idx),\n",
    "                             term_or_collocation,\n",
    "                              en_test,\n",
    "                              tl_test,\n",
    "                              '<font color=\\\"' + term_gender_colour + '\\\">' + term_with_accent + '</font>' + case_taken,\n",
    "                              disting_gram_info,\n",
    "                              conjugation_declension_info,\n",
    "                              top_three_gram_colls,\n",
    "                              eng_def,\n",
    "                              other_defs,\n",
    "                              source_sent_one,\n",
    "                              target_sent_one,\n",
    "                              str(row['Source']),\n",
    "                              other_sent_pairs_en,\n",
    "                              other_sent_pairs_en_ru_term_in_bold\n",
    "                                ]\n",
    "                    )\n",
    "\n",
    "                    my_deck.add_note(en_tl_note)\n",
    "\n",
    "                    # Create cards for each collocation of the English definition\n",
    "                    if default_to_top_coll == False:\n",
    "                        coll_idx = 1\n",
    "                        for index, row in eng_def_colls_df.iterrows():\n",
    "\n",
    "                            print('COLL INDEX:', coll_idx)\n",
    "                            card_freq = row['Frequency']\n",
    "                            term_or_collocation = 'Collocation'\n",
    "                            tl_test = ' '.join(row['Original collocation'])\n",
    "                            en_test = row['Target sentence']\n",
    "                            other_defs = '<br>'.join([item for item in basic_defs if item != eng_def])\n",
    "\n",
    "                            source_sent_one = row['Source sentence with coll bold']\n",
    "                            target_sent_one = row['Target sentence']\n",
    "\n",
    "                            other_sent_pairs_en = row['Other sentence pairs en']\n",
    "                            other_sent_pairs_ru_term_in_bold = row['Other sentence pairs ru term in bold']\n",
    "                            other_sent_pairs_ru_coll_in_bold = row['Other sentence pairs ru coll in bold']\n",
    "                            other_sent_pairs_en_ru_term_in_bold = row['Other sentence pairs en ru term in bold']\n",
    "                            other_sent_pairs_ru_term_in_bold_en = row['Other sentence pairs ru term in bold en']\n",
    "                            other_sent_pairs_en_ru_coll_in_bold = row['Other sentence pairs en ru coll in bold']\n",
    "                            other_sent_pairs_ru_coll_in_bold_en = row['Other sentence pairs ru coll in bold en']\n",
    "\n",
    "                            print('\\n\\tCOLLOCATION TYPE:', row['Collocation type'])\n",
    "                            print('\\tRAW FREQUENCY:', row['Raw frequency'])\n",
    "                            print('\\tSCALED FREQUENCY:', row['Frequency'])\n",
    "                            print('\\tLEMMED COLLOCATION:', row['Lemmed collocation'])\n",
    "                            print('\\tORIGINAL COLLOCATION:', row['Original collocation'])\n",
    "                            print('\\tCARD FREQUENCY:', card_freq)\n",
    "                            print('\\n')\n",
    "                            \n",
    "                            # Check if a card for the collocation has already been written\n",
    "                            if row['Lemmed collocation'] not in colls_written_to_cards:\n",
    "                    \n",
    "                                colls_written_to_cards.append(row['Lemmed collocation'])\n",
    "\n",
    "                                tl_en_note = genanki.Note(\n",
    "                                model = tl_en_note_type,\n",
    "                                fields = [str(card_freq),\n",
    "                                         str(freq_rank) + ' d' + str(eng_def_idx) + ' c' + str(coll_idx),\n",
    "                                         term_or_collocation,\n",
    "                                          tl_test,\n",
    "                                          en_test,\n",
    "                                          '<font color=\\\"' + term_gender_colour + '\\\">' + term_with_accent + '</font>' + case_taken,\n",
    "                                          disting_gram_info,\n",
    "                                          conjugation_declension_info,\n",
    "                                          top_three_gram_colls,\n",
    "                                          eng_def,\n",
    "                                          other_defs,\n",
    "                                          source_sent_one,\n",
    "                                          target_sent_one,\n",
    "                                          str(row['Source']),\n",
    "                                          other_sent_pairs_ru_coll_in_bold,\n",
    "                                          other_sent_pairs_ru_coll_in_bold_en\n",
    "                                            ]\n",
    "                                )\n",
    "\n",
    "                                my_deck.add_note(tl_en_note)\n",
    "\n",
    "                                en_tl_note = genanki.Note(\n",
    "                                model = en_tl_note_type,\n",
    "                                fields = [str(round(card_freq * 1.05)),\n",
    "                                        str(freq_rank) + ' d' + str(eng_def_idx) + ' c' + str(coll_idx),\n",
    "                                         term_or_collocation,\n",
    "                                          en_test,\n",
    "                                          tl_test,\n",
    "                                          '<font color=\\\"' + term_gender_colour + '\\\">' + term_with_accent + '</font>' + case_taken,\n",
    "                                          disting_gram_info,\n",
    "                                          conjugation_declension_info,\n",
    "                                          top_three_gram_colls,\n",
    "                                          eng_def,\n",
    "                                          other_defs,\n",
    "                                        source_sent_one,\n",
    "                                          target_sent_one,\n",
    "                                          str(row['Source']),\n",
    "                                          other_sent_pairs_en,\n",
    "                                          other_sent_pairs_en_ru_coll_in_bold\n",
    "                                            ]\n",
    "                                )\n",
    "\n",
    "                                my_deck.add_note(en_tl_note)\n",
    "\n",
    "                            coll_idx += 1                  \n",
    "\n",
    "        eng_def_idx += 1\n",
    "\n",
    "    print('\\n\\nNON MATCHED SENTS:')\n",
    "    print(len(results_df[results_df['English definition']=='Unmatched'].index))\n",
    "    # Find the top collocations in the unmatched sentences\n",
    "    if len(results_df[results_df['English definition']=='Unmatched'].index) != 0:\n",
    "        est_non_matched_sents_freq = round(estimated_freq * len(results_df.index)/concordance_freq)\n",
    "        q_forms = query_forms\n",
    "        return_series = results_df[results_df['English definition']=='Unmatched'].apply(colls_from_sents_df_row, query_forms=q_forms, axis=1)\n",
    "        lex_colls_df, non_matched_sents_colls_df, top_three_gram_colls_df, default_to_top_coll = count_colls(return_series.tolist(), q_forms, raw_min_coll_freq, est_min_coll_freq, est_non_matched_sents_freq, len(results_df[results_df['English definition']=='Unmatched'].index), estimated_freq, reduced_concordance_freq)\n",
    "\n",
    "        top_three_gram_colls = ''\n",
    "\n",
    "        if default_to_top_coll == True:\n",
    "            print('NO COLLS FOUND ABOVE MINIMUM FREQUENCIES --> NOT WRITING ANY FLASHCARDS.')\n",
    "        else:\n",
    "            if non_matched_sents_colls_df.empty == True:\n",
    "                print('NON MATCHED SENTS COLLS DF RETURNED EMPTY.')\n",
    "            else:\n",
    "                coll_idx = 1\n",
    "                for index, row in non_matched_sents_colls_df.iterrows():\n",
    "\n",
    "                    term_or_collocation = 'Collocation'\n",
    "                    eng_def = ''\n",
    "                    other_defs = '<br>'.join(basic_defs)\n",
    "                    card_freq = row['Frequency']\n",
    "                    tl_test = ' '.join(row['Original collocation'])\n",
    "                    en_test = row['Target sentence']\n",
    "\n",
    "                    source_sent_one = row['Source sentence with coll bold']\n",
    "                    target_sent_one = row['Target sentence']\n",
    "\n",
    "                    other_sent_pairs_en = row['Other sentence pairs en']\n",
    "                    other_sent_pairs_ru_term_in_bold = row['Other sentence pairs ru term in bold']\n",
    "                    other_sent_pairs_ru_coll_in_bold = row['Other sentence pairs ru coll in bold']\n",
    "                    other_sent_pairs_en_ru_term_in_bold = row['Other sentence pairs en ru term in bold']\n",
    "                    other_sent_pairs_ru_term_in_bold_en = row['Other sentence pairs ru term in bold en']\n",
    "                    other_sent_pairs_en_ru_coll_in_bold = row['Other sentence pairs en ru coll in bold']\n",
    "                    other_sent_pairs_ru_coll_in_bold_en = row['Other sentence pairs ru coll in bold en']\n",
    "\n",
    "                    print('\\n\\tCOLLOCATION TYPE:', row['Collocation type'])\n",
    "                    print('\\tRAW FREQUENCY:', row['Raw frequency'])\n",
    "                    print('\\tSCALED FREQUENCY:', row['Frequency'])\n",
    "                    print('\\tLEMMED COLLOCATION:', row['Lemmed collocation'])\n",
    "                    print('\\tORIGINAL COLLOCATION:', row['Original collocation'])\n",
    "                    print('\\tCARD FREQUENCY:', card_freq)\n",
    "                    print('\\n')\n",
    "                    \n",
    "                    # Check if a card for the collocation has already been written\n",
    "                    if row['Lemmed collocation'] not in colls_written_to_cards:\n",
    "                                \n",
    "                        colls_written_to_cards.append(row['Lemmed collocation'])\n",
    "                        \n",
    "                        tl_en_note = genanki.Note(\n",
    "                            model = tl_en_note_type,\n",
    "                            fields = [str(card_freq),\n",
    "                                    str(freq_rank) + ' c' + str(coll_idx),\n",
    "                                     term_or_collocation,\n",
    "                                      tl_test,\n",
    "                                      en_test,\n",
    "                                      '<font color=\\\"' + term_gender_colour + '\\\">' + term_with_accent + '</font>' + case_taken,\n",
    "                                      disting_gram_info,\n",
    "                                      conjugation_declension_info,\n",
    "                                      top_three_gram_colls,\n",
    "                                      eng_def,\n",
    "                                      other_defs,\n",
    "                                      source_sent_one,\n",
    "                                      target_sent_one,\n",
    "                                      str(row['Source']),\n",
    "                                      other_sent_pairs_ru_coll_in_bold,\n",
    "                                        other_sent_pairs_ru_coll_in_bold_en\n",
    "                                        ]\n",
    "                        )\n",
    "\n",
    "                        my_deck.add_note(tl_en_note)\n",
    "\n",
    "                        en_tl_note = genanki.Note(\n",
    "                        model = en_tl_note_type,\n",
    "                        fields = [str(round(card_freq * 1.05)),\n",
    "                                  str(freq_rank) + ' c' + str(coll_idx),\n",
    "                                 term_or_collocation,\n",
    "                                  en_test,\n",
    "                                  tl_test,\n",
    "                                  '<font color=\\\"' + term_gender_colour + '\\\">' + term_with_accent + '</font>' + case_taken,\n",
    "                                  disting_gram_info,\n",
    "                                  conjugation_declension_info,\n",
    "                                  top_three_gram_colls,\n",
    "                                  eng_def,\n",
    "                                  other_defs,\n",
    "                                  source_sent_one,\n",
    "                                  target_sent_one,\n",
    "                                  str(row['Source']),\n",
    "                                   other_sent_pairs_en,\n",
    "                                    other_sent_pairs_en_ru_coll_in_bold\n",
    "                                    ]\n",
    "                        )\n",
    "\n",
    "                        my_deck.add_note(en_tl_note)\n",
    "\n",
    "                    coll_idx += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "    if time_elapsed <= 18:\n",
    "        time.sleep(18 - time_elapsed)\n",
    "\n",
    "    print('\\n\\n')\n",
    "\n",
    "genanki.Package(my_deck).write_to_file('C:\\\\Users\\\\MdeCL\\\\Desktop\\\\Vocab Project Desktop Files\\\\russian_vocab.apkg')\n",
    "\n",
    "print('\\n\\nFlashcard writing complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Export Anki deck file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "genanki.Package(my_deck).write_to_file('C:\\\\Users\\\\MdeCL\\\\Desktop\\\\Vocab Project Desktop Files\\\\russian_vocab.apkg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
